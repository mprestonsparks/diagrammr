2024-01-23 14:42:36,830 - INFO - [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on http://127.0.0.1:5000
2024-01-23 14:42:36,830 - INFO - [33mPress CTRL+C to quit[0m
2024-01-23 14:42:36,830 - INFO -  * Restarting with stat
2024-01-23 14:42:37,115 - WARNING -  * Debugger is active!
2024-01-23 14:42:37,122 - INFO -  * Debugger PIN: 139-904-016
2024-01-23 14:43:02,145 - INFO - Received JSON data: {'gitRepoUrl': 'https://github.com/mprestonsparks/diagrammr.git', 'local_dir': '/Users/preston/Documents/gitRepos/diagrammr/src/scripts/../../output', 'gitHubAccessToken': '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', 'configFile': '/Users/preston/Documents/gitRepos/diagrammr/src/scripts/config.json'}
2024-01-23 14:43:02,146 - INFO - Received configFile: /Users/preston/Documents/gitRepos/diagrammr/src/scripts/config.json
2024-01-23 14:43:02,146 - INFO - Received gitHubAccessToken: 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
2024-01-23 14:43:02,365 - DEBUG - Failed checking if running in CYGWIN due to: FileNotFoundError(2, 'No such file or directory')
2024-01-23 14:43:02,366 - DEBUG - Popen(['git', 'clone', '-v', '--', 'https://github.com/mprestonsparks/diagrammr.git', '../../output'], cwd=/Users/preston/Documents/gitRepos/diagrammr, stdin=None, shell=False, universal_newlines=True)
2024-01-23 14:43:08,460 - DEBUG - Cmd(['git', 'clone', '-v', '--', 'https://github.com/mprestonsparks/diagrammr.git', '../../output'])'s unused stdout: 
2024-01-23 14:43:08,621 - DEBUG - Popen(['git', 'clone', '-v', '--', 'https://github.com/mprestonsparks/diagrammr.git', '../../output'], cwd=/Users/preston/Documents/gitRepos/diagrammr, stdin=None, shell=False, universal_newlines=True)
2024-01-23 14:43:13,674 - DEBUG - Cmd(['git', 'clone', '-v', '--', 'https://github.com/mprestonsparks/diagrammr.git', '../../output'])'s unused stdout: 
2024-01-23 14:43:13,676 - DEBUG - Popen(['git', 'cat-file', '--batch-check'], cwd=/Users/preston/Documents/output, stdin=<valid stream>, shell=False, universal_newlines=False)
2024-01-23 14:43:13,682 - DEBUG - Popen(['git', 'cat-file', '--batch'], cwd=/Users/preston/Documents/output, stdin=<valid stream>, shell=False, universal_newlines=False)
2024-01-23 14:43:13,704 - DEBUG - Type of included_files: <class 'dict'>
2024-01-23 14:43:13,704 - DEBUG - Value of included_files: {'src/models/__init__.py': '', 'src/models/git_repo.py': "# src/models/git_repo.py\nimport git\nimport json\nimport shutil\nimport os\n\nclass GitRepo:\n    def __init__(self, config_file):\n        with open(config_file) as json_file:\n            data = json.load(json_file)\n        self.repo_url = data['gitRepoUrl']\n        self.local_dir = data['local_dir']\n\n    def clone(self):\n        # Delete the directory if it exists and is not empty\n        if os.path.exists(self.local_dir) and os.listdir(self.local_dir):\n            shutil.rmtree(self.local_dir)\n        # Clone the repository and return the local path\n        # This is a simple example and doesn't handle errors\n        git.Repo.clone_from(self.repo_url, self.local_dir)\n\n    def retrieve_code(self, file_path):\n        # Retrieve the code from a file in the repository\n        # This is a simple example and doesn't handle errors\n        with open(f'{self.local_dir}/{file_path}') as file:\n            return file.read()", 'src/models/uml_diagram.py': "# src/models/uml_diagram.py\nimport os\nimport logging\nfrom logging import handlers\n\n# Configure logging to write to a file\nlog_directory = 'logs'\n# Create the directory if it doesn't exist\nos.makedirs(log_directory, exist_ok=True)  \nlog_filename = os.path.join(log_directory, 'models_uml_diagram.log')\nlog_handler = handlers.RotatingFileHandler(log_filename, maxBytes=1024*1024, backupCount=5)  # Log file with rotation\nlog_formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\nlog_handler.setFormatter(log_formatter)\nlogger = logging.getLogger()\nlogger.addHandler(log_handler)\nlogger.setLevel(logging.DEBUG)\n\nclass UMLDiagram:\n    def __init__(self, code, title):\n        self.code = code\n        self.title = title\n        self.uml_code = None\n\n    def generate(self, openai_api):\n        self.uml_code = openai_api.generate_from_code(self.code, self.title)\n\n    def save(self, openai_api, file_path):\n        if self.uml_code is not None:\n            openai_api.save_generated_output(self.uml_code, file_path)", 'src/routes/__init__.py': '', 'src/routes/uml_from_repo.py': '# uml_from_repo.py\nimport os\nimport fnmatch\nimport subprocess\nimport json\nimport git\nimport tempfile\nimport shutil\nimport logging\nfrom logging import handlers  \nfrom services.retrieve_code import clone_repo, retrieve_code\nfrom utils.code_to_uml import generate_content  # Import the function from code_to_uml.py\n\n \n# Configure logging to write to a file\nlog_directory = \'logs\'\n# Create the directory if it doesn\'t exist\nos.makedirs(log_directory, exist_ok=True)  \nlog_filename = os.path.join(log_directory, \'uml_from_repo.log\')\nlog_handler = handlers.RotatingFileHandler(log_filename, maxBytes=1024*1024, backupCount=5)  # Log file with rotation\nlog_formatter = logging.Formatter(\'%(asctime)s - %(levelname)s - %(message)s\')\nlog_handler.setFormatter(log_formatter)\nlogger = logging.getLogger()\nlogger.addHandler(log_handler)\nlogger.setLevel(logging.DEBUG)\n\ndef process_request(git_repo, github_access_token):\n    output_directory = git_repo.local_dir  # Get the output directory from the GitRepo object\n    branch_name = \'master\'  # \'master\' is the default branch name\n    temp_dir = None  # Initialize temp_dir outside the try block\n\n    try:\n        if not git_repo.repo_url or not output_directory or not github_access_token or not branch_name:\n            logger.error("Missing required parameters")  \n            return {"error": "Missing required parameters"}, 400\n\n        try:\n            temp_dir = tempfile.mkdtemp()\n        except Exception as e:\n            logger.error(f"Error during UML generation: {str(e)}", exc_info=True)\n            return {"error": str(e)}, 500\n\n        # Check if the directory exists and remove it\n        if os.path.exists(output_directory):\n            shutil.rmtree(output_directory)\n\n        # Clone the repository\n        repo = git.Repo.clone_from(git_repo.repo_url, output_directory)\n\n        # Load the config\n        with open(\'src/routes/config.json\', \'r\') as f:\n            config = json.load(f)\n        \n        def traverse_directories(repo, git_repo, config):\n            included_files = {}\n            for item in repo.tree().traverse():\n                if item.type == \'blob\':  # This means it\'s a file, not a directory\n                    for pattern in config[\'include\']:\n                        if fnmatch.fnmatch(item.path, pattern):\n                            with open(os.path.join(git_repo.local_dir, item.path), \'r\') as f:\n                                included_files[item.path] = f.read()\n                            break  # No need to check the remaining patterns\n            return included_files\n\n        # Retrieve the code from the repository\n        included_files = traverse_directories(repo, git_repo, config)\n\n        logger.debug(f"Type of included_files: {type(included_files)}")\n        logger.debug(f"Value of included_files: {included_files}")\n\n        # Save the UML diagram to a file\n        logger.info(f"Saving UML diagram to {output_directory}")\n        final_output_paths = generate_content(included_files, output_directory)  # This is now a list of file paths\n        logger.info(f"Final output paths: {final_output_paths}")\n\n        for path in final_output_paths:\n            logger.info(f"UML diagram saved at: {path}")  # Log the path where each UML diagram was saved\n\n        return {\n            "message": "UML diagrams generated successfully",\n            "details": {\n                "Repository": git_repo.repo_url,\n                "Output Paths": final_output_paths  # This is now a list of file paths\n            }\n        }, 200\n\n    except Exception as e:\n        logger.error(f"Error during UML generation: {str(e)}")\n        return {"error": str(e)}, 500\n\n    finally:\n        # Clean up the temporary directory\n        if temp_dir is not None:\n            logger.info("Cleaning up temporary directory")\n            shutil.rmtree(temp_dir)\n', 'src/scripts/__init__.py': '', 'src/scripts/execute_generate_uml.py': "import json\nimport os\nimport requests\nimport logging\nfrom logging import handlers\nfrom dotenv import load_dotenv\n\n# Load environment variables from .env file\nload_dotenv()\n\n# Configure logging to write to a file\nlog_directory = '../../logs'\n# Create the directory if it doesn't exist\nos.makedirs(log_directory, exist_ok=True)  \nlog_filename = os.path.join(log_directory, 'execute_generate_uml.log')\nlog_handler = handlers.RotatingFileHandler(log_filename, maxBytes=1024*1024, backupCount=5)  # Log file with rotation\nlog_formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\nlog_handler.setFormatter(log_formatter)\nlogger = logging.getLogger()\nlogger.addHandler(log_handler)\nlogger.setLevel(logging.DEBUG)\n\n\n# Configure logging\nlogging.basicConfig(filename='execute_generate_uml.log', level=logging.DEBUG, format='%(asctime)s - %(levelname)s - %(message)s')\n\n# Current file's directory\ncurrent_dir = os.path.dirname(os.path.abspath(__file__))\n# Configuration file path\nconfig_file_path = os.path.join(current_dir, 'config.json')\n\n# Read configuration from the JSON file\nwith open(config_file_path, 'r') as config_file:\n    config_data = json.load(config_file)\n\n# Retrieve the GitHub access token from environment variables\ngithub_token = os.getenv('GITHUB_PAT')\n\n# Endpoint URL\nurl = 'http://127.0.0.1:5000/generate-uml'\n\n# Headers\nheaders = {\n    'Content-Type': 'application/json'\n}\n\n# Update the GitHub access token, local directory, and configFile in the config data\nconfig_data['gitHubAccessToken'] = github_token\nconfig_data['local_dir'] = os.path.join(current_dir, '../..', 'output')\nconfig_data['configFile'] = config_file_path  \n\n# Log the JSON data being sent in the request\nlogging.info(f'Sending JSON data to {url}:')\nlogging.info(json.dumps(config_data, indent=2))\n\ntry:\n    # Make the POST request\n    response = requests.post(url, headers=headers, json=config_data)\n\n    # Log the response from the server\n    logging.info(f'Response from server ({url}):')\n    logging.info(f'Status Code: {response.status_code}')\n    logging.info(f'Response Text: {response.text}')\n\n    # Print the response from the server\n    print(response.text)\n\n    # Save the response to a file in the output directory\n    output_dir = os.path.join(current_dir, '../..', 'output')\n    os.makedirs(output_dir, exist_ok=True)\n    with open(os.path.join(output_dir, 'response.json'), 'w') as output_file:\n        json.dump(response.json(), output_file, indent=2)\n\nexcept Exception as e:\n    # Log any exceptions that occur\n    logging.error(f'Error occurred: {str(e)}')", 'src/services/UMLGenerator.py': 'import json\nimport os\nimport logging\nfrom services.openai_api import OpenAIAPI\n\nclass UMLGenerator:\n    def __init__(self, config_file):\n        self.api = OpenAIAPI()\n        self.OUTPUT_DIRECTORY = "src/output" \n        # Load the configuration\n        with open(config_file) as file:\n            self.config = json.load(file)\n\n    def generate_from_codes(self, file_list):\n        uml_contents = []\n        for file in file_list:\n            if self._should_skip_file(file):\n                continue\n            logging.info(f"Generating UML for file: {file}")\n            with open(file, \'r\') as f:\n                file_content = f.read()\n            uml_content = self.api.generate_from_code(file_content, os.path.basename(file))\n            if uml_content != "UML generation failed":\n                uml_contents.append(uml_content)\n            else:\n                logging.error(f"UML generation failed for file: {file}")\n        return uml_contents\n \n    def save_generated_outputs(self, uml_contents, file_names):\n        for uml_content, file_name in zip(uml_contents, file_names):\n            logging.info(f"Saving UML diagram for file: {file_name}")\n            self.api.save_generated_output(uml_content, file_name)\n\n    def _should_skip_file(self, file):\n        python_files_only = self.config.get(\'python_files_only\', False)\n        ignore_files = self.config.get(\'ignore_files\', [])\n        ignore_extensions = self.config.get(\'ignore_extensions\', [])\n        return (file in ignore_files or \n                (python_files_only and not file.endswith(\'.py\')) or \n                any(file.endswith(ext) for ext in ignore_extensions))\n\n    def _generate_from_code(self, file):\n        logging.info(f"Generating UML diagram for file: {file}")\n        return self.api.generate_from_code(file)', 'src/services/__init__.py': '', 'src/services/openai_api.py': 'import json\nimport os\nimport logging\nfrom logging import handlers\nfrom openai import OpenAI, OpenAIError\nfrom dotenv import load_dotenv\nfrom models.uml_diagram import UMLDiagram\nfrom models.git_repo import GitRepo  # Import GitRepo\n\n\n# Load environment variables from .env file\nload_dotenv()\n\n# Configure logging to write to a file\nlog_directory = \'logs\'\n# Create the directory if it doesn\'t exist\nos.makedirs(log_directory, exist_ok=True)  \nlog_filename = os.path.join(log_directory, \'openai_api.log\')\nlog_handler = handlers.RotatingFileHandler(log_filename, maxBytes=1024*1024, backupCount=5)  # Log file with rotation\nlog_formatter = logging.Formatter(\'%(asctime)s - %(levelname)s - %(message)s\')\nlog_handler.setFormatter(log_formatter)\nlogger = logging.getLogger()\nlogger.addHandler(log_handler)\nlogger.setLevel(logging.DEBUG)\n\n\nclass OpenAIAPI:\n    def __init__(self):\n        # Load environment variables from .env file\n        load_dotenv()\n\n        # Retrieve the OpenAI API key from the environment\n        openai_api_key = os.getenv("OPENAI_API_KEY")\n\n        self.client = OpenAI(api_key=openai_api_key)\n\n        # Constants\n        self.MODEL_NAME = "gpt-3.5-turbo-instruct"\n        self.MAX_TOKENS = 1024\n        self.OUTPUT_DIRECTORY = "src/output"\n\n    def generate_uml_diagram(self, repo_url, file_path, title):\n        # Clone the repository and retrieve the code\n        git_repo = GitRepo(repo_url)\n        git_repo.clone()\n        code = git_repo.retrieve_code(file_path)\n\n        uml_diagram = UMLDiagram(code, title)\n        uml_diagram.generate(self)\n        return uml_diagram\n\n    def save_uml_diagram(self, uml_diagram, file_path):\n        uml_diagram.save(self, file_path)\n\n    def write_response_to_file(self, response, filename):\n        output_dir = "output/openai"\n        os.makedirs(output_dir, exist_ok=True)  # Create the directory if it doesn\'t exist\n        with open(os.path.join(output_dir, f"{filename}.txt"), \'w\') as f:\n            f.write(response)\n\n    def generate_from_code(self, code, title):\n        # Split the code into chunks of MAX_TOKENS\n        code_chunks = [code[i:i+self.MAX_TOKENS] for i in range(0, len(code), self.MAX_TOKENS)]\n\n        puml_chunks = []\n        for chunk in code_chunks:\n            prompt_text = f"Create UML diagrams in .puml format for the following code:\\n\\n{chunk}"\n            logging.info(f"Sending prompt to OpenAI: {prompt_text}")  # Log the prompt text\n            try:\n                response = self.client.completions.create(\n                    model=self.MODEL_NAME,\n                    prompt=prompt_text,\n                    max_tokens=self.MAX_TOKENS)\n                \n                # Log the response from OpenAI\n                logging.info(f"Received response from OpenAI: {response.choices[0].text}")  # Log the response from OpenAI\n\n                # Write the response to a .txt file\n                self.write_response_to_file(response.choices[0].text, title)\n\n                # Append the generated text to the UML code\n                puml_chunks.append(response.choices[0].text.strip())\n            except OpenAIError as e:\n                logging.error(f"An error occurred while sending the prompt: {e}")  # Log the error message\n                return f"An error occurred: {e}"  # Return a message if an OpenAI API error occurs\n\n        # Combine the .puml chunks into a single string\n        combined_puml = "\\n".join(puml_chunks)\n\n        # Send a final request to OpenAI to integrate the .puml chunks\n        prompt_text = f"Create a .puml file that integrates the following .puml files:\\n\\n{combined_puml}"\n        response = self.client.completions.create(\n            model=self.MODEL_NAME,\n            prompt=prompt_text,\n            max_tokens=self.MAX_TOKENS)\n\n        # Extract the integrated .puml code from the response\n        generated_code = response.choices[0].text.strip()\n\n        # Add the @startuml, title and @enduml tags only once for each UML diagram\n        generated_code = f"@startuml\\n" + f"title {title}\\n" + generated_code + "\\n@enduml\\n"\n        return generated_code\n    \n\n    def save_generated_output(self, generated_code, file_path):\n        # Create the directory if it does not exist\n        logging.info(f"Saving generated output to {file_path}")\n        os.makedirs(os.path.dirname(file_path), exist_ok=True)\n        \n        # Write the UML code to the file\n        with open(file_path, \'w\') as file:\n            file.write(generated_code)\n            logging.info(f"Generated output saved to {file_path}")  # Optional: print out the path where the file was saved\n\n        return file_path   ', 'src/services/retrieve_code.py': 'import git\nimport json\nimport os\nimport logging\nfrom logging import handlers\n\n\n# Configure logging to write to a file\nlog_directory = \'logs\'\n# Create the directory if it doesn\'t exist\nos.makedirs(log_directory, exist_ok=True)  \nlog_filename = os.path.join(log_directory, \'retrieve_code.log\')\nlog_handler = handlers.RotatingFileHandler(log_filename, maxBytes=1024*1024, backupCount=5)  # Log file with rotation\nlog_formatter = logging.Formatter(\'%(asctime)s - %(levelname)s - %(message)s\')\nlog_handler.setFormatter(log_formatter)\nlogger = logging.getLogger()\nlogger.addHandler(log_handler)\nlogger.setLevel(logging.DEBUG)\n\n\ndef clone_repo(repo_url, temp_dir, access_token):\n    try:\n        # Modify the URL to include the access token\n        if access_token:\n            repo_url = repo_url.replace(\'https://\', f\'https://{access_token}@\')\n        logger.info(f"Cloning repository: {repo_url}")\n        repo = git.Repo.clone_from(repo_url, temp_dir)\n        logger.info(f"Successfully cloned repository: {repo_url}")\n        return repo\n    except Exception as e:\n        logger.error(f"Failed to clone repository: {str(e)}")\n        raise ValueError(f"Failed to clone repository: {str(e)}")\n\ndef retrieve_code(repo, branch_name):\n    try:\n        print(f"Attempting to checkout branch: {branch_name}")  # Diagnostic print statement\n        logger.info(f"Attempting to checkout branch: {branch_name}")\n        repo.git.fetch()  # Fetch the latest updates from the remote\n        repo.git.checkout(branch_name)\n        logger.info(f"Successfully checked out branch: {branch_name}")\n        \n        # Load the config\n        config_file_path = os.path.join(os.path.dirname(os.path.abspath(__file__)), \'config.json\')\n        with open(config_file_path, \'r\') as f:\n            config = json.load(f)\n        ignore_list = config.get(\'ignore\', [])\n        include_list = config.get(\'include\', [])\n\n        # Create a new dictionary to store file paths and their corresponding code\n        included_files = {}\n        for file in repo.tree():\n            if any(file.path.endswith(ext) for ext in include_list) and not any(ignored_file in file.path for ignored_file in ignore_list):\n                try:\n                    with open(file.abspath, \'r\') as f:\n                        included_files[file.path] = f.read()\n                    logger.info(f"Included file: {file.path}")\n                except FileNotFoundError:\n                    print(f"Ignoring missing file: {file.path}")  # Diagnostic print statement\n                    logger.warning(f"Ignoring missing file: {file.path}")\n\n        return included_files\n    except Exception as e:\n        logger.error(f"Failed to retrieve code: {str(e)}")\n        raise ValueError(f"Failed to retrieve code: {str(e)}")', 'src/utils/__init__.py': '', 'src/utils/code_to_uml.py': '# code_to_uml.py\nimport os\nfrom services.openai_api import OpenAIAPI \nimport logging\nfrom logging import handlers  \n\n# Create an instance of the OpenAI API\napi = OpenAIAPI()\n\n# Configure logging to write to a file\nlog_directory = \'logs\'\n# Create the directory if it doesn\'t exist\nos.makedirs(log_directory, exist_ok=True)  \nlog_filename = os.path.join(log_directory, \'code_to_uml.log\')\nlog_handler = handlers.RotatingFileHandler(log_filename, maxBytes=1024*1024, backupCount=5)  # Log file with rotation\nlog_formatter = logging.Formatter(\'%(asctime)s - %(levelname)s - %(message)s\')\nlog_handler.setFormatter(log_formatter)\nlogger = logging.getLogger()\nlogger.addHandler(log_handler)\nlogger.setLevel(logging.DEBUG)\n\ndef generate_content(files, output_directory):\n    logging.info(f"Files to process: {files}")  # Log the files dictionary\n    file_paths = []  # Initialize a list to store the file paths\n    for file_path, code in files.items():\n        # Skip if the file is empty\n        if not code.strip():\n            logging.info(f"Skipping empty file: {file_path}")\n            continue\n        logging.info(f"Processing file: {file_path}")\n        response = api.generate_from_code(code, os.path.basename(file_path))\n        # Extract the UML code from the response\n        generated_code_for_file = extract_uml_code(response)\n        if generated_code_for_file is None:\n            logging.error(f"Failed to generate UML diagram for {file_path}")\n            continue\n        logging.info(f"UML code generated for {file_path}: {generated_code_for_file}")  # Log the generated UML code\n\n        # Save the UML code for each file to a separate .puml file\n        file_name = f"{os.path.basename(file_path)}.puml"\n        final_output_path = api.save_generated_output(generated_code_for_file, os.path.join(output_directory, file_name))\n        file_paths.append(final_output_path)  # Append the file path to the list\n\n    logging.info(f"Generated file paths: {file_paths}")\n    # Return the list of file paths\n    return file_paths\n\ndef extract_uml_code(response):\n    # Split the response into lines\n    lines = response.split(\'\\n\')\n    # Check if \'@startuml\' and \'@enduml\' are in the response\n    if \'@startuml\' not in response or \'@enduml\' not in response:\n        logging.error("Failed to extract UML code from the response. \'@startuml\' or \'@enduml\' not found.")\n        return None\n    # Find the start and end of the UML code\n    start_index = lines.index(\'@startuml\')\n    end_index = lines.index(\'@enduml\') if \'@enduml\' in lines else -1\n    # Extract the UML code\n    uml_code = \'\\n\'.join(lines[start_index:end_index+1])\n    return uml_code'}
2024-01-23 14:43:13,705 - INFO - Saving UML diagram to ../../output
2024-01-23 14:43:13,705 - INFO - Files to process: {'src/models/__init__.py': '', 'src/models/git_repo.py': "# src/models/git_repo.py\nimport git\nimport json\nimport shutil\nimport os\n\nclass GitRepo:\n    def __init__(self, config_file):\n        with open(config_file) as json_file:\n            data = json.load(json_file)\n        self.repo_url = data['gitRepoUrl']\n        self.local_dir = data['local_dir']\n\n    def clone(self):\n        # Delete the directory if it exists and is not empty\n        if os.path.exists(self.local_dir) and os.listdir(self.local_dir):\n            shutil.rmtree(self.local_dir)\n        # Clone the repository and return the local path\n        # This is a simple example and doesn't handle errors\n        git.Repo.clone_from(self.repo_url, self.local_dir)\n\n    def retrieve_code(self, file_path):\n        # Retrieve the code from a file in the repository\n        # This is a simple example and doesn't handle errors\n        with open(f'{self.local_dir}/{file_path}') as file:\n            return file.read()", 'src/models/uml_diagram.py': "# src/models/uml_diagram.py\nimport os\nimport logging\nfrom logging import handlers\n\n# Configure logging to write to a file\nlog_directory = 'logs'\n# Create the directory if it doesn't exist\nos.makedirs(log_directory, exist_ok=True)  \nlog_filename = os.path.join(log_directory, 'models_uml_diagram.log')\nlog_handler = handlers.RotatingFileHandler(log_filename, maxBytes=1024*1024, backupCount=5)  # Log file with rotation\nlog_formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\nlog_handler.setFormatter(log_formatter)\nlogger = logging.getLogger()\nlogger.addHandler(log_handler)\nlogger.setLevel(logging.DEBUG)\n\nclass UMLDiagram:\n    def __init__(self, code, title):\n        self.code = code\n        self.title = title\n        self.uml_code = None\n\n    def generate(self, openai_api):\n        self.uml_code = openai_api.generate_from_code(self.code, self.title)\n\n    def save(self, openai_api, file_path):\n        if self.uml_code is not None:\n            openai_api.save_generated_output(self.uml_code, file_path)", 'src/routes/__init__.py': '', 'src/routes/uml_from_repo.py': '# uml_from_repo.py\nimport os\nimport fnmatch\nimport subprocess\nimport json\nimport git\nimport tempfile\nimport shutil\nimport logging\nfrom logging import handlers  \nfrom services.retrieve_code import clone_repo, retrieve_code\nfrom utils.code_to_uml import generate_content  # Import the function from code_to_uml.py\n\n \n# Configure logging to write to a file\nlog_directory = \'logs\'\n# Create the directory if it doesn\'t exist\nos.makedirs(log_directory, exist_ok=True)  \nlog_filename = os.path.join(log_directory, \'uml_from_repo.log\')\nlog_handler = handlers.RotatingFileHandler(log_filename, maxBytes=1024*1024, backupCount=5)  # Log file with rotation\nlog_formatter = logging.Formatter(\'%(asctime)s - %(levelname)s - %(message)s\')\nlog_handler.setFormatter(log_formatter)\nlogger = logging.getLogger()\nlogger.addHandler(log_handler)\nlogger.setLevel(logging.DEBUG)\n\ndef process_request(git_repo, github_access_token):\n    output_directory = git_repo.local_dir  # Get the output directory from the GitRepo object\n    branch_name = \'master\'  # \'master\' is the default branch name\n    temp_dir = None  # Initialize temp_dir outside the try block\n\n    try:\n        if not git_repo.repo_url or not output_directory or not github_access_token or not branch_name:\n            logger.error("Missing required parameters")  \n            return {"error": "Missing required parameters"}, 400\n\n        try:\n            temp_dir = tempfile.mkdtemp()\n        except Exception as e:\n            logger.error(f"Error during UML generation: {str(e)}", exc_info=True)\n            return {"error": str(e)}, 500\n\n        # Check if the directory exists and remove it\n        if os.path.exists(output_directory):\n            shutil.rmtree(output_directory)\n\n        # Clone the repository\n        repo = git.Repo.clone_from(git_repo.repo_url, output_directory)\n\n        # Load the config\n        with open(\'src/routes/config.json\', \'r\') as f:\n            config = json.load(f)\n        \n        def traverse_directories(repo, git_repo, config):\n            included_files = {}\n            for item in repo.tree().traverse():\n                if item.type == \'blob\':  # This means it\'s a file, not a directory\n                    for pattern in config[\'include\']:\n                        if fnmatch.fnmatch(item.path, pattern):\n                            with open(os.path.join(git_repo.local_dir, item.path), \'r\') as f:\n                                included_files[item.path] = f.read()\n                            break  # No need to check the remaining patterns\n            return included_files\n\n        # Retrieve the code from the repository\n        included_files = traverse_directories(repo, git_repo, config)\n\n        logger.debug(f"Type of included_files: {type(included_files)}")\n        logger.debug(f"Value of included_files: {included_files}")\n\n        # Save the UML diagram to a file\n        logger.info(f"Saving UML diagram to {output_directory}")\n        final_output_paths = generate_content(included_files, output_directory)  # This is now a list of file paths\n        logger.info(f"Final output paths: {final_output_paths}")\n\n        for path in final_output_paths:\n            logger.info(f"UML diagram saved at: {path}")  # Log the path where each UML diagram was saved\n\n        return {\n            "message": "UML diagrams generated successfully",\n            "details": {\n                "Repository": git_repo.repo_url,\n                "Output Paths": final_output_paths  # This is now a list of file paths\n            }\n        }, 200\n\n    except Exception as e:\n        logger.error(f"Error during UML generation: {str(e)}")\n        return {"error": str(e)}, 500\n\n    finally:\n        # Clean up the temporary directory\n        if temp_dir is not None:\n            logger.info("Cleaning up temporary directory")\n            shutil.rmtree(temp_dir)\n', 'src/scripts/__init__.py': '', 'src/scripts/execute_generate_uml.py': "import json\nimport os\nimport requests\nimport logging\nfrom logging import handlers\nfrom dotenv import load_dotenv\n\n# Load environment variables from .env file\nload_dotenv()\n\n# Configure logging to write to a file\nlog_directory = '../../logs'\n# Create the directory if it doesn't exist\nos.makedirs(log_directory, exist_ok=True)  \nlog_filename = os.path.join(log_directory, 'execute_generate_uml.log')\nlog_handler = handlers.RotatingFileHandler(log_filename, maxBytes=1024*1024, backupCount=5)  # Log file with rotation\nlog_formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\nlog_handler.setFormatter(log_formatter)\nlogger = logging.getLogger()\nlogger.addHandler(log_handler)\nlogger.setLevel(logging.DEBUG)\n\n\n# Configure logging\nlogging.basicConfig(filename='execute_generate_uml.log', level=logging.DEBUG, format='%(asctime)s - %(levelname)s - %(message)s')\n\n# Current file's directory\ncurrent_dir = os.path.dirname(os.path.abspath(__file__))\n# Configuration file path\nconfig_file_path = os.path.join(current_dir, 'config.json')\n\n# Read configuration from the JSON file\nwith open(config_file_path, 'r') as config_file:\n    config_data = json.load(config_file)\n\n# Retrieve the GitHub access token from environment variables\ngithub_token = os.getenv('GITHUB_PAT')\n\n# Endpoint URL\nurl = 'http://127.0.0.1:5000/generate-uml'\n\n# Headers\nheaders = {\n    'Content-Type': 'application/json'\n}\n\n# Update the GitHub access token, local directory, and configFile in the config data\nconfig_data['gitHubAccessToken'] = github_token\nconfig_data['local_dir'] = os.path.join(current_dir, '../..', 'output')\nconfig_data['configFile'] = config_file_path  \n\n# Log the JSON data being sent in the request\nlogging.info(f'Sending JSON data to {url}:')\nlogging.info(json.dumps(config_data, indent=2))\n\ntry:\n    # Make the POST request\n    response = requests.post(url, headers=headers, json=config_data)\n\n    # Log the response from the server\n    logging.info(f'Response from server ({url}):')\n    logging.info(f'Status Code: {response.status_code}')\n    logging.info(f'Response Text: {response.text}')\n\n    # Print the response from the server\n    print(response.text)\n\n    # Save the response to a file in the output directory\n    output_dir = os.path.join(current_dir, '../..', 'output')\n    os.makedirs(output_dir, exist_ok=True)\n    with open(os.path.join(output_dir, 'response.json'), 'w') as output_file:\n        json.dump(response.json(), output_file, indent=2)\n\nexcept Exception as e:\n    # Log any exceptions that occur\n    logging.error(f'Error occurred: {str(e)}')", 'src/services/UMLGenerator.py': 'import json\nimport os\nimport logging\nfrom services.openai_api import OpenAIAPI\n\nclass UMLGenerator:\n    def __init__(self, config_file):\n        self.api = OpenAIAPI()\n        self.OUTPUT_DIRECTORY = "src/output" \n        # Load the configuration\n        with open(config_file) as file:\n            self.config = json.load(file)\n\n    def generate_from_codes(self, file_list):\n        uml_contents = []\n        for file in file_list:\n            if self._should_skip_file(file):\n                continue\n            logging.info(f"Generating UML for file: {file}")\n            with open(file, \'r\') as f:\n                file_content = f.read()\n            uml_content = self.api.generate_from_code(file_content, os.path.basename(file))\n            if uml_content != "UML generation failed":\n                uml_contents.append(uml_content)\n            else:\n                logging.error(f"UML generation failed for file: {file}")\n        return uml_contents\n \n    def save_generated_outputs(self, uml_contents, file_names):\n        for uml_content, file_name in zip(uml_contents, file_names):\n            logging.info(f"Saving UML diagram for file: {file_name}")\n            self.api.save_generated_output(uml_content, file_name)\n\n    def _should_skip_file(self, file):\n        python_files_only = self.config.get(\'python_files_only\', False)\n        ignore_files = self.config.get(\'ignore_files\', [])\n        ignore_extensions = self.config.get(\'ignore_extensions\', [])\n        return (file in ignore_files or \n                (python_files_only and not file.endswith(\'.py\')) or \n                any(file.endswith(ext) for ext in ignore_extensions))\n\n    def _generate_from_code(self, file):\n        logging.info(f"Generating UML diagram for file: {file}")\n        return self.api.generate_from_code(file)', 'src/services/__init__.py': '', 'src/services/openai_api.py': 'import json\nimport os\nimport logging\nfrom logging import handlers\nfrom openai import OpenAI, OpenAIError\nfrom dotenv import load_dotenv\nfrom models.uml_diagram import UMLDiagram\nfrom models.git_repo import GitRepo  # Import GitRepo\n\n\n# Load environment variables from .env file\nload_dotenv()\n\n# Configure logging to write to a file\nlog_directory = \'logs\'\n# Create the directory if it doesn\'t exist\nos.makedirs(log_directory, exist_ok=True)  \nlog_filename = os.path.join(log_directory, \'openai_api.log\')\nlog_handler = handlers.RotatingFileHandler(log_filename, maxBytes=1024*1024, backupCount=5)  # Log file with rotation\nlog_formatter = logging.Formatter(\'%(asctime)s - %(levelname)s - %(message)s\')\nlog_handler.setFormatter(log_formatter)\nlogger = logging.getLogger()\nlogger.addHandler(log_handler)\nlogger.setLevel(logging.DEBUG)\n\n\nclass OpenAIAPI:\n    def __init__(self):\n        # Load environment variables from .env file\n        load_dotenv()\n\n        # Retrieve the OpenAI API key from the environment\n        openai_api_key = os.getenv("OPENAI_API_KEY")\n\n        self.client = OpenAI(api_key=openai_api_key)\n\n        # Constants\n        self.MODEL_NAME = "gpt-3.5-turbo-instruct"\n        self.MAX_TOKENS = 1024\n        self.OUTPUT_DIRECTORY = "src/output"\n\n    def generate_uml_diagram(self, repo_url, file_path, title):\n        # Clone the repository and retrieve the code\n        git_repo = GitRepo(repo_url)\n        git_repo.clone()\n        code = git_repo.retrieve_code(file_path)\n\n        uml_diagram = UMLDiagram(code, title)\n        uml_diagram.generate(self)\n        return uml_diagram\n\n    def save_uml_diagram(self, uml_diagram, file_path):\n        uml_diagram.save(self, file_path)\n\n    def write_response_to_file(self, response, filename):\n        output_dir = "output/openai"\n        os.makedirs(output_dir, exist_ok=True)  # Create the directory if it doesn\'t exist\n        with open(os.path.join(output_dir, f"{filename}.txt"), \'w\') as f:\n            f.write(response)\n\n    def generate_from_code(self, code, title):\n        # Split the code into chunks of MAX_TOKENS\n        code_chunks = [code[i:i+self.MAX_TOKENS] for i in range(0, len(code), self.MAX_TOKENS)]\n\n        puml_chunks = []\n        for chunk in code_chunks:\n            prompt_text = f"Create UML diagrams in .puml format for the following code:\\n\\n{chunk}"\n            logging.info(f"Sending prompt to OpenAI: {prompt_text}")  # Log the prompt text\n            try:\n                response = self.client.completions.create(\n                    model=self.MODEL_NAME,\n                    prompt=prompt_text,\n                    max_tokens=self.MAX_TOKENS)\n                \n                # Log the response from OpenAI\n                logging.info(f"Received response from OpenAI: {response.choices[0].text}")  # Log the response from OpenAI\n\n                # Write the response to a .txt file\n                self.write_response_to_file(response.choices[0].text, title)\n\n                # Append the generated text to the UML code\n                puml_chunks.append(response.choices[0].text.strip())\n            except OpenAIError as e:\n                logging.error(f"An error occurred while sending the prompt: {e}")  # Log the error message\n                return f"An error occurred: {e}"  # Return a message if an OpenAI API error occurs\n\n        # Combine the .puml chunks into a single string\n        combined_puml = "\\n".join(puml_chunks)\n\n        # Send a final request to OpenAI to integrate the .puml chunks\n        prompt_text = f"Create a .puml file that integrates the following .puml files:\\n\\n{combined_puml}"\n        response = self.client.completions.create(\n            model=self.MODEL_NAME,\n            prompt=prompt_text,\n            max_tokens=self.MAX_TOKENS)\n\n        # Extract the integrated .puml code from the response\n        generated_code = response.choices[0].text.strip()\n\n        # Add the @startuml, title and @enduml tags only once for each UML diagram\n        generated_code = f"@startuml\\n" + f"title {title}\\n" + generated_code + "\\n@enduml\\n"\n        return generated_code\n    \n\n    def save_generated_output(self, generated_code, file_path):\n        # Create the directory if it does not exist\n        logging.info(f"Saving generated output to {file_path}")\n        os.makedirs(os.path.dirname(file_path), exist_ok=True)\n        \n        # Write the UML code to the file\n        with open(file_path, \'w\') as file:\n            file.write(generated_code)\n            logging.info(f"Generated output saved to {file_path}")  # Optional: print out the path where the file was saved\n\n        return file_path   ', 'src/services/retrieve_code.py': 'import git\nimport json\nimport os\nimport logging\nfrom logging import handlers\n\n\n# Configure logging to write to a file\nlog_directory = \'logs\'\n# Create the directory if it doesn\'t exist\nos.makedirs(log_directory, exist_ok=True)  \nlog_filename = os.path.join(log_directory, \'retrieve_code.log\')\nlog_handler = handlers.RotatingFileHandler(log_filename, maxBytes=1024*1024, backupCount=5)  # Log file with rotation\nlog_formatter = logging.Formatter(\'%(asctime)s - %(levelname)s - %(message)s\')\nlog_handler.setFormatter(log_formatter)\nlogger = logging.getLogger()\nlogger.addHandler(log_handler)\nlogger.setLevel(logging.DEBUG)\n\n\ndef clone_repo(repo_url, temp_dir, access_token):\n    try:\n        # Modify the URL to include the access token\n        if access_token:\n            repo_url = repo_url.replace(\'https://\', f\'https://{access_token}@\')\n        logger.info(f"Cloning repository: {repo_url}")\n        repo = git.Repo.clone_from(repo_url, temp_dir)\n        logger.info(f"Successfully cloned repository: {repo_url}")\n        return repo\n    except Exception as e:\n        logger.error(f"Failed to clone repository: {str(e)}")\n        raise ValueError(f"Failed to clone repository: {str(e)}")\n\ndef retrieve_code(repo, branch_name):\n    try:\n        print(f"Attempting to checkout branch: {branch_name}")  # Diagnostic print statement\n        logger.info(f"Attempting to checkout branch: {branch_name}")\n        repo.git.fetch()  # Fetch the latest updates from the remote\n        repo.git.checkout(branch_name)\n        logger.info(f"Successfully checked out branch: {branch_name}")\n        \n        # Load the config\n        config_file_path = os.path.join(os.path.dirname(os.path.abspath(__file__)), \'config.json\')\n        with open(config_file_path, \'r\') as f:\n            config = json.load(f)\n        ignore_list = config.get(\'ignore\', [])\n        include_list = config.get(\'include\', [])\n\n        # Create a new dictionary to store file paths and their corresponding code\n        included_files = {}\n        for file in repo.tree():\n            if any(file.path.endswith(ext) for ext in include_list) and not any(ignored_file in file.path for ignored_file in ignore_list):\n                try:\n                    with open(file.abspath, \'r\') as f:\n                        included_files[file.path] = f.read()\n                    logger.info(f"Included file: {file.path}")\n                except FileNotFoundError:\n                    print(f"Ignoring missing file: {file.path}")  # Diagnostic print statement\n                    logger.warning(f"Ignoring missing file: {file.path}")\n\n        return included_files\n    except Exception as e:\n        logger.error(f"Failed to retrieve code: {str(e)}")\n        raise ValueError(f"Failed to retrieve code: {str(e)}")', 'src/utils/__init__.py': '', 'src/utils/code_to_uml.py': '# code_to_uml.py\nimport os\nfrom services.openai_api import OpenAIAPI \nimport logging\nfrom logging import handlers  \n\n# Create an instance of the OpenAI API\napi = OpenAIAPI()\n\n# Configure logging to write to a file\nlog_directory = \'logs\'\n# Create the directory if it doesn\'t exist\nos.makedirs(log_directory, exist_ok=True)  \nlog_filename = os.path.join(log_directory, \'code_to_uml.log\')\nlog_handler = handlers.RotatingFileHandler(log_filename, maxBytes=1024*1024, backupCount=5)  # Log file with rotation\nlog_formatter = logging.Formatter(\'%(asctime)s - %(levelname)s - %(message)s\')\nlog_handler.setFormatter(log_formatter)\nlogger = logging.getLogger()\nlogger.addHandler(log_handler)\nlogger.setLevel(logging.DEBUG)\n\ndef generate_content(files, output_directory):\n    logging.info(f"Files to process: {files}")  # Log the files dictionary\n    file_paths = []  # Initialize a list to store the file paths\n    for file_path, code in files.items():\n        # Skip if the file is empty\n        if not code.strip():\n            logging.info(f"Skipping empty file: {file_path}")\n            continue\n        logging.info(f"Processing file: {file_path}")\n        response = api.generate_from_code(code, os.path.basename(file_path))\n        # Extract the UML code from the response\n        generated_code_for_file = extract_uml_code(response)\n        if generated_code_for_file is None:\n            logging.error(f"Failed to generate UML diagram for {file_path}")\n            continue\n        logging.info(f"UML code generated for {file_path}: {generated_code_for_file}")  # Log the generated UML code\n\n        # Save the UML code for each file to a separate .puml file\n        file_name = f"{os.path.basename(file_path)}.puml"\n        final_output_path = api.save_generated_output(generated_code_for_file, os.path.join(output_directory, file_name))\n        file_paths.append(final_output_path)  # Append the file path to the list\n\n    logging.info(f"Generated file paths: {file_paths}")\n    # Return the list of file paths\n    return file_paths\n\ndef extract_uml_code(response):\n    # Split the response into lines\n    lines = response.split(\'\\n\')\n    # Check if \'@startuml\' and \'@enduml\' are in the response\n    if \'@startuml\' not in response or \'@enduml\' not in response:\n        logging.error("Failed to extract UML code from the response. \'@startuml\' or \'@enduml\' not found.")\n        return None\n    # Find the start and end of the UML code\n    start_index = lines.index(\'@startuml\')\n    end_index = lines.index(\'@enduml\') if \'@enduml\' in lines else -1\n    # Extract the UML code\n    uml_code = \'\\n\'.join(lines[start_index:end_index+1])\n    return uml_code'}
2024-01-23 14:43:13,705 - INFO - Skipping empty file: src/models/__init__.py
2024-01-23 14:43:13,706 - INFO - Processing file: src/models/git_repo.py
2024-01-23 14:43:13,706 - INFO - Sending prompt to OpenAI: Create UML diagrams in .puml format for the following code:

# src/models/git_repo.py
import git
import json
import shutil
import os

class GitRepo:
    def __init__(self, config_file):
        with open(config_file) as json_file:
            data = json.load(json_file)
        self.repo_url = data['gitRepoUrl']
        self.local_dir = data['local_dir']

    def clone(self):
        # Delete the directory if it exists and is not empty
        if os.path.exists(self.local_dir) and os.listdir(self.local_dir):
            shutil.rmtree(self.local_dir)
        # Clone the repository and return the local path
        # This is a simple example and doesn't handle errors
        git.Repo.clone_from(self.repo_url, self.local_dir)

    def retrieve_code(self, file_path):
        # Retrieve the code from a file in the repository
        # This is a simple example and doesn't handle errors
        with open(f'{self.local_dir}/{file_path}') as file:
            return file.read()
2024-01-23 14:43:13,707 - DEBUG - Request options: {'method': 'post', 'url': '/completions', 'files': None, 'json_data': {'model': 'gpt-3.5-turbo-instruct', 'prompt': "Create UML diagrams in .puml format for the following code:\n\n# src/models/git_repo.py\nimport git\nimport json\nimport shutil\nimport os\n\nclass GitRepo:\n    def __init__(self, config_file):\n        with open(config_file) as json_file:\n            data = json.load(json_file)\n        self.repo_url = data['gitRepoUrl']\n        self.local_dir = data['local_dir']\n\n    def clone(self):\n        # Delete the directory if it exists and is not empty\n        if os.path.exists(self.local_dir) and os.listdir(self.local_dir):\n            shutil.rmtree(self.local_dir)\n        # Clone the repository and return the local path\n        # This is a simple example and doesn't handle errors\n        git.Repo.clone_from(self.repo_url, self.local_dir)\n\n    def retrieve_code(self, file_path):\n        # Retrieve the code from a file in the repository\n        # This is a simple example and doesn't handle errors\n        with open(f'{self.local_dir}/{file_path}') as file:\n            return file.read()", 'max_tokens': 1024}}
2024-01-23 14:43:13,737 - DEBUG - connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=5.0 socket_options=None
2024-01-23 14:43:13,841 - DEBUG - connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x1063934d0>
2024-01-23 14:43:13,841 - DEBUG - start_tls.started ssl_context=<ssl.SSLContext object at 0x10465f6e0> server_hostname='api.openai.com' timeout=5.0
2024-01-23 14:43:13,859 - DEBUG - start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x106398590>
2024-01-23 14:43:13,860 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2024-01-23 14:43:13,860 - DEBUG - send_request_headers.complete
2024-01-23 14:43:13,860 - DEBUG - send_request_body.started request=<Request [b'POST']>
2024-01-23 14:43:13,861 - DEBUG - send_request_body.complete
2024-01-23 14:43:13,861 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2024-01-23 14:43:14,612 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Tue, 23 Jan 2024 20:43:14 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'Cache-Control', b'no-cache, must-revalidate'), (b'openai-model', b'gpt-3.5-turbo-instruct'), (b'openai-organization', b'user-lqvxa4hv7a8o8eyohtzmcoc6'), (b'openai-processing-ms', b'631'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=15724800; includeSubDomains'), (b'x-ratelimit-limit-requests', b'3000'), (b'x-ratelimit-limit-tokens', b'250000'), (b'x-ratelimit-remaining-requests', b'2999'), (b'x-ratelimit-remaining-tokens', b'248975'), (b'x-ratelimit-reset-requests', b'20ms'), (b'x-ratelimit-reset-tokens', b'245ms'), (b'x-request-id', b'ddea61471b448bcc8e8e4470927edfab'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Set-Cookie', b'__cf_bm=hddYSslCI_L29pXatdtkMYCaxUOqTgPUcSa94PP5ao4-1706042594-1-AXgm4L7JwSSqkcsGUrzoio6pnNZtQsuu3fNfg2j28f7Ea+dia4+WHxgfnPflswoS/1E5biG4A8C9aGv5Emoe21E=; path=/; expires=Tue, 23-Jan-24 21:13:14 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'Set-Cookie', b'_cfuvid=XStaaVe2Bzw2VSaqd9ZP.7QG7g4qTHjT5VkqbECMuSM-1706042594586-0-604800000; path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'Server', b'cloudflare'), (b'CF-RAY', b'84a2de23bd2a4535-ATL'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=":443"; ma=86400')])
2024-01-23 14:43:14,615 - INFO - HTTP Request: POST https://api.openai.com/v1/completions "HTTP/1.1 200 OK"
2024-01-23 14:43:14,616 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2024-01-23 14:43:14,616 - DEBUG - receive_response_body.complete
2024-01-23 14:43:14,617 - DEBUG - response_closed.started
2024-01-23 14:43:14,617 - DEBUG - response_closed.complete
2024-01-23 14:43:14,617 - DEBUG - HTTP Request: POST https://api.openai.com/v1/completions "200 OK"
2024-01-23 14:43:14,621 - INFO - Received response from OpenAI: 
@startuml

class GitRepo {
    -repo_url: string
    -local_dir: string
    +clone()
    +retrieve_code(file_path: string): string
}

GitRepo "1" *-- "1..*" GitRepo

@enduml
2024-01-23 14:43:14,624 - DEBUG - Request options: {'method': 'post', 'url': '/completions', 'files': None, 'json_data': {'model': 'gpt-3.5-turbo-instruct', 'prompt': 'Create a .puml file that integrates the following .puml files:\n\n@startuml\n\nclass GitRepo {\n    -repo_url: string\n    -local_dir: string\n    +clone()\n    +retrieve_code(file_path: string): string\n}\n\nGitRepo "1" *-- "1..*" GitRepo\n\n@enduml', 'max_tokens': 1024}}
2024-01-23 14:43:14,625 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2024-01-23 14:43:14,625 - DEBUG - send_request_headers.complete
2024-01-23 14:43:14,626 - DEBUG - send_request_body.started request=<Request [b'POST']>
2024-01-23 14:43:14,626 - DEBUG - send_request_body.complete
2024-01-23 14:43:14,626 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2024-01-23 14:43:17,952 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Tue, 23 Jan 2024 20:43:17 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'Cache-Control', b'no-cache, must-revalidate'), (b'openai-model', b'gpt-3.5-turbo-instruct'), (b'openai-organization', b'user-lqvxa4hv7a8o8eyohtzmcoc6'), (b'openai-processing-ms', b'3263'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=15724800; includeSubDomains'), (b'x-ratelimit-limit-requests', b'3000'), (b'x-ratelimit-limit-tokens', b'250000'), (b'x-ratelimit-remaining-requests', b'2999'), (b'x-ratelimit-remaining-tokens', b'248975'), (b'x-ratelimit-reset-requests', b'20ms'), (b'x-ratelimit-reset-tokens', b'245ms'), (b'x-request-id', b'b43d6cf3a2e1921ddf24af5c458a4583'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'84a2de288bac4535-ATL'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=":443"; ma=86400')])
2024-01-23 14:43:17,954 - INFO - HTTP Request: POST https://api.openai.com/v1/completions "HTTP/1.1 200 OK"
2024-01-23 14:43:17,954 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2024-01-23 14:43:17,955 - DEBUG - receive_response_body.complete
2024-01-23 14:43:17,955 - DEBUG - response_closed.started
2024-01-23 14:43:17,956 - DEBUG - response_closed.complete
2024-01-23 14:43:17,956 - DEBUG - HTTP Request: POST https://api.openai.com/v1/completions "200 OK"
2024-01-23 14:43:17,958 - INFO - UML code generated for src/models/git_repo.py: @startuml
title git_repo.py
@startuml

class User {
    -name: string
    -email: string
    +create_repo(repo_name: string): GitRepo
    +commit_changes(repo: GitRepo)
}

User *-- "1" GitRepo

@enduml
2024-01-23 14:43:17,959 - INFO - Saving generated output to ../../output/git_repo.py.puml
2024-01-23 14:43:17,960 - INFO - Generated output saved to ../../output/git_repo.py.puml
2024-01-23 14:43:17,960 - INFO - Processing file: src/models/uml_diagram.py
2024-01-23 14:43:17,961 - INFO - Sending prompt to OpenAI: Create UML diagrams in .puml format for the following code:

# src/models/uml_diagram.py
import os
import logging
from logging import handlers

# Configure logging to write to a file
log_directory = 'logs'
# Create the directory if it doesn't exist
os.makedirs(log_directory, exist_ok=True)  
log_filename = os.path.join(log_directory, 'models_uml_diagram.log')
log_handler = handlers.RotatingFileHandler(log_filename, maxBytes=1024*1024, backupCount=5)  # Log file with rotation
log_formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')
log_handler.setFormatter(log_formatter)
logger = logging.getLogger()
logger.addHandler(log_handler)
logger.setLevel(logging.DEBUG)

class UMLDiagram:
    def __init__(self, code, title):
        self.code = code
        self.title = title
        self.uml_code = None

    def generate(self, openai_api):
        self.uml_code = openai_api.generate_from_code(self.code, self.title)

    def save(self, openai_api, file_path):
        if self.uml_code is not None:
            openai_api.save_generated_output(self.uml_code, fi
2024-01-23 14:43:17,962 - DEBUG - Request options: {'method': 'post', 'url': '/completions', 'files': None, 'json_data': {'model': 'gpt-3.5-turbo-instruct', 'prompt': "Create UML diagrams in .puml format for the following code:\n\n# src/models/uml_diagram.py\nimport os\nimport logging\nfrom logging import handlers\n\n# Configure logging to write to a file\nlog_directory = 'logs'\n# Create the directory if it doesn't exist\nos.makedirs(log_directory, exist_ok=True)  \nlog_filename = os.path.join(log_directory, 'models_uml_diagram.log')\nlog_handler = handlers.RotatingFileHandler(log_filename, maxBytes=1024*1024, backupCount=5)  # Log file with rotation\nlog_formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\nlog_handler.setFormatter(log_formatter)\nlogger = logging.getLogger()\nlogger.addHandler(log_handler)\nlogger.setLevel(logging.DEBUG)\n\nclass UMLDiagram:\n    def __init__(self, code, title):\n        self.code = code\n        self.title = title\n        self.uml_code = None\n\n    def generate(self, openai_api):\n        self.uml_code = openai_api.generate_from_code(self.code, self.title)\n\n    def save(self, openai_api, file_path):\n        if self.uml_code is not None:\n            openai_api.save_generated_output(self.uml_code, fi", 'max_tokens': 1024}}
2024-01-23 14:43:17,964 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2024-01-23 14:43:17,965 - DEBUG - send_request_headers.complete
2024-01-23 14:43:17,965 - DEBUG - send_request_body.started request=<Request [b'POST']>
2024-01-23 14:43:17,965 - DEBUG - send_request_body.complete
2024-01-23 14:43:17,965 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2024-01-23 14:43:22,112 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Tue, 23 Jan 2024 20:43:21 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'Cache-Control', b'no-cache, must-revalidate'), (b'openai-model', b'gpt-3.5-turbo-instruct'), (b'openai-organization', b'user-lqvxa4hv7a8o8eyohtzmcoc6'), (b'openai-processing-ms', b'3489'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=15724800; includeSubDomains'), (b'x-ratelimit-limit-requests', b'3000'), (b'x-ratelimit-limit-tokens', b'250000'), (b'x-ratelimit-remaining-requests', b'2999'), (b'x-ratelimit-remaining-tokens', b'248975'), (b'x-ratelimit-reset-requests', b'20ms'), (b'x-ratelimit-reset-tokens', b'245ms'), (b'x-request-id', b'76c079323144114e7318b6c560d7a422'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'84a2de3d5ffe4535-ATL'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=":443"; ma=86400')])
2024-01-23 14:43:22,114 - INFO - HTTP Request: POST https://api.openai.com/v1/completions "HTTP/1.1 200 OK"
2024-01-23 14:43:22,114 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2024-01-23 14:43:22,115 - DEBUG - receive_response_body.complete
2024-01-23 14:43:22,116 - DEBUG - response_closed.started
2024-01-23 14:43:22,116 - DEBUG - response_closed.complete
2024-01-23 14:43:22,116 - DEBUG - HTTP Request: POST https://api.openai.com/v1/completions "200 OK"
2024-01-23 14:43:22,119 - INFO - Received response from OpenAI: le_path)

@startuml
class models.uml_diagram.UMLDiagram {
    - code: str
    - title: str
    - uml_code: str
    + generate(openai_api): void
    + save(openai_api, file_path): void
}

note right of models.uml_diagram.UMLDiagram::generate
    Generates UML code using
    OpenAI API from given code and title.
end note

note right of models.uml_diagram.UMLDiagram::save
    Saves generated UML code to file
    using OpenAI API.
end note

class logging.handlers.RotatingFileHandler {
    - maxBytes: int
    - backupCount: int
    - log_filename: str
}

note bottom of models.uml_diagram.UMLDiagram::init
    Initializes UMLDiagram object
    with given code and title.
end note

class logging.Logger {
    - logger_name: str
    - logger_level: int
    - handlers: logging.Handler[]
}

note bottom of models.uml_diagram.UMLDiagram
    Uses Logger to handle
    logging functionality.
end note

note right of models.uml_diagram.UMLDiagram::init
    Creates the logs directory if
    it doesn't exist.
end note

class models.uml_diagram.UMLDiagram ..> logging.handlers.RotatingFileHandler
class models.uml_diagram.UMLDiagram ..> logging.Logger

models.uml_diagram.UMLDiagram --> openai_api

openai_api ..> openai.api

models.uml_diagram.UMLDiagram --> file_path
@enduml
2024-01-23 14:43:22,120 - INFO - Sending prompt to OpenAI: Create UML diagrams in .puml format for the following code:

le_path)
2024-01-23 14:43:22,122 - DEBUG - Request options: {'method': 'post', 'url': '/completions', 'files': None, 'json_data': {'model': 'gpt-3.5-turbo-instruct', 'prompt': 'Create UML diagrams in .puml format for the following code:\n\nle_path)', 'max_tokens': 1024}}
2024-01-23 14:43:22,124 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2024-01-23 14:43:22,124 - DEBUG - send_request_headers.complete
2024-01-23 14:43:22,125 - DEBUG - send_request_body.started request=<Request [b'POST']>
2024-01-23 14:43:22,125 - DEBUG - send_request_body.complete
2024-01-23 14:43:22,125 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2024-01-23 14:43:29,882 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Tue, 23 Jan 2024 20:43:29 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'Cache-Control', b'no-cache, must-revalidate'), (b'openai-model', b'gpt-3.5-turbo-instruct'), (b'openai-organization', b'user-lqvxa4hv7a8o8eyohtzmcoc6'), (b'openai-processing-ms', b'7226'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=15724800; includeSubDomains'), (b'x-ratelimit-limit-requests', b'3000'), (b'x-ratelimit-limit-tokens', b'250000'), (b'x-ratelimit-remaining-requests', b'2999'), (b'x-ratelimit-remaining-tokens', b'248975'), (b'x-ratelimit-reset-requests', b'20ms'), (b'x-ratelimit-reset-tokens', b'245ms'), (b'x-request-id', b'a2c185bbf4241db46c7e67a5ea9f2d15'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'84a2de575adb4535-ATL'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=":443"; ma=86400')])
2024-01-23 14:43:29,883 - INFO - HTTP Request: POST https://api.openai.com/v1/completions "HTTP/1.1 200 OK"
2024-01-23 14:43:29,883 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2024-01-23 14:43:29,884 - DEBUG - receive_response_body.complete
2024-01-23 14:43:29,884 - DEBUG - response_closed.started
2024-01-23 14:43:29,884 - DEBUG - response_closed.complete
2024-01-23 14:43:29,884 - DEBUG - HTTP Request: POST https://api.openai.com/v1/completions "200 OK"
2024-01-23 14:43:29,885 - INFO - Received response from OpenAI:  { //Creating an abstract class called Vehicle with attributes and methods
abstract class Vehicle {
    //Private attributes
    -String brand
    -int year
    -String color
    -int mileage

    //Protected attribute
    #int price

    //Constructor with brand, year, and color parameters
    +Vehicle(String brand, int year, String color) 

    //Abstract method calculatePrice
    +abstract int calculatePrice()

    //Getter and Setter methods for mileage attribute
    +int getMileage()
    +void setMileage(int mileage)

    //Getter and Setter methods for price attribute
    +int getPrice()
    +void setPrice(int price)

    //Method to print vehicle information
    +void printInfo()
}

//Creating a subclass of Vehicle called Car
class Car extends Vehicle {
    //Private attribute
    -boolean hasSpoiler

    //Constructor with brand, year, color, and hasSpoiler parameters
    +Car(String brand, int year, String color, boolean hasSpoiler)

    //Overriding calculatePrice method from Vehicle class
    +int calculatePrice() 

    //Overriding printInfo method from Vehicle class
    +void printInfo()
}

//Creating a subclass of Vehicle called Motorcycle
class Motorcycle extends Vehicle {
    //Private attribute
    -boolean hasSidecar

    //Constructor with brand, year, color, and hasSidecar parameters
    +Motorcycle(String brand, int year, String color, boolean hasSidecar)

    //Overriding calculatePrice method from Vehicle class
    +int calculatePrice() 

    //Overriding printInfo method from Vehicle class
    +void printInfo()
}

Vehicle <|-- Car
Vehicle <|-- Motorcycle

Vehicle "1" o-- "1" Vehicle  : has-a
Car "1" o-- "1" Vehicle : is-a
Motorcycle "1" o-- "1" Vehicle : is-a

class Vehicle {
    -String brand
    -int year
    -String color
    -int mileage
    #int price
    +Vehicle(String brand, int year, String color)
    +abstract int calculatePrice()
    +int getMileage()
    +void setMileage(int mileage)
    +int getPrice()
    +void setPrice(int price)
    +void printInfo()
}

class Car {
    -boolean hasSpoiler
    +Car(String brand, int year, String color, boolean hasSpoiler)
    +int calculatePrice()
    +void printInfo()
}

class Motorcycle {
    -boolean hasSidecar
    +Motorcycle(String brand, int year, String color, boolean hasSidecar)
    +int calculatePrice()
    +void printInfo()
}

Vehicle <|-- Car
Vehicle <|-- Motorcycle
2024-01-23 14:43:29,888 - DEBUG - Request options: {'method': 'post', 'url': '/completions', 'files': None, 'json_data': {'model': 'gpt-3.5-turbo-instruct', 'prompt': 'Create a .puml file that integrates the following .puml files:\n\nle_path)\n\n@startuml\nclass models.uml_diagram.UMLDiagram {\n    - code: str\n    - title: str\n    - uml_code: str\n    + generate(openai_api): void\n    + save(openai_api, file_path): void\n}\n\nnote right of models.uml_diagram.UMLDiagram::generate\n    Generates UML code using\n    OpenAI API from given code and title.\nend note\n\nnote right of models.uml_diagram.UMLDiagram::save\n    Saves generated UML code to file\n    using OpenAI API.\nend note\n\nclass logging.handlers.RotatingFileHandler {\n    - maxBytes: int\n    - backupCount: int\n    - log_filename: str\n}\n\nnote bottom of models.uml_diagram.UMLDiagram::init\n    Initializes UMLDiagram object\n    with given code and title.\nend note\n\nclass logging.Logger {\n    - logger_name: str\n    - logger_level: int\n    - handlers: logging.Handler[]\n}\n\nnote bottom of models.uml_diagram.UMLDiagram\n    Uses Logger to handle\n    logging functionality.\nend note\n\nnote right of models.uml_diagram.UMLDiagram::init\n    Creates the logs directory if\n    it doesn\'t exist.\nend note\n\nclass models.uml_diagram.UMLDiagram ..> logging.handlers.RotatingFileHandler\nclass models.uml_diagram.UMLDiagram ..> logging.Logger\n\nmodels.uml_diagram.UMLDiagram --> openai_api\n\nopenai_api ..> openai.api\n\nmodels.uml_diagram.UMLDiagram --> file_path\n@enduml\n{ //Creating an abstract class called Vehicle with attributes and methods\nabstract class Vehicle {\n    //Private attributes\n    -String brand\n    -int year\n    -String color\n    -int mileage\n\n    //Protected attribute\n    #int price\n\n    //Constructor with brand, year, and color parameters\n    +Vehicle(String brand, int year, String color) \n\n    //Abstract method calculatePrice\n    +abstract int calculatePrice()\n\n    //Getter and Setter methods for mileage attribute\n    +int getMileage()\n    +void setMileage(int mileage)\n\n    //Getter and Setter methods for price attribute\n    +int getPrice()\n    +void setPrice(int price)\n\n    //Method to print vehicle information\n    +void printInfo()\n}\n\n//Creating a subclass of Vehicle called Car\nclass Car extends Vehicle {\n    //Private attribute\n    -boolean hasSpoiler\n\n    //Constructor with brand, year, color, and hasSpoiler parameters\n    +Car(String brand, int year, String color, boolean hasSpoiler)\n\n    //Overriding calculatePrice method from Vehicle class\n    +int calculatePrice() \n\n    //Overriding printInfo method from Vehicle class\n    +void printInfo()\n}\n\n//Creating a subclass of Vehicle called Motorcycle\nclass Motorcycle extends Vehicle {\n    //Private attribute\n    -boolean hasSidecar\n\n    //Constructor with brand, year, color, and hasSidecar parameters\n    +Motorcycle(String brand, int year, String color, boolean hasSidecar)\n\n    //Overriding calculatePrice method from Vehicle class\n    +int calculatePrice() \n\n    //Overriding printInfo method from Vehicle class\n    +void printInfo()\n}\n\nVehicle <|-- Car\nVehicle <|-- Motorcycle\n\nVehicle "1" o-- "1" Vehicle  : has-a\nCar "1" o-- "1" Vehicle : is-a\nMotorcycle "1" o-- "1" Vehicle : is-a\n\nclass Vehicle {\n    -String brand\n    -int year\n    -String color\n    -int mileage\n    #int price\n    +Vehicle(String brand, int year, String color)\n    +abstract int calculatePrice()\n    +int getMileage()\n    +void setMileage(int mileage)\n    +int getPrice()\n    +void setPrice(int price)\n    +void printInfo()\n}\n\nclass Car {\n    -boolean hasSpoiler\n    +Car(String brand, int year, String color, boolean hasSpoiler)\n    +int calculatePrice()\n    +void printInfo()\n}\n\nclass Motorcycle {\n    -boolean hasSidecar\n    +Motorcycle(String brand, int year, String color, boolean hasSidecar)\n    +int calculatePrice()\n    +void printInfo()\n}\n\nVehicle <|-- Car\nVehicle <|-- Motorcycle', 'max_tokens': 1024}}
2024-01-23 14:43:29,889 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2024-01-23 14:43:29,889 - DEBUG - send_request_headers.complete
2024-01-23 14:43:29,889 - DEBUG - send_request_body.started request=<Request [b'POST']>
2024-01-23 14:43:29,889 - DEBUG - send_request_body.complete
2024-01-23 14:43:29,890 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2024-01-23 14:43:30,892 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Tue, 23 Jan 2024 20:43:30 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'Cache-Control', b'no-cache, must-revalidate'), (b'openai-model', b'gpt-3.5-turbo-instruct'), (b'openai-organization', b'user-lqvxa4hv7a8o8eyohtzmcoc6'), (b'openai-processing-ms', b'875'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=15724800; includeSubDomains'), (b'x-ratelimit-limit-requests', b'3000'), (b'x-ratelimit-limit-tokens', b'250000'), (b'x-ratelimit-remaining-requests', b'2999'), (b'x-ratelimit-remaining-tokens', b'248975'), (b'x-ratelimit-reset-requests', b'20ms'), (b'x-ratelimit-reset-tokens', b'245ms'), (b'x-request-id', b'9f62c3e2cfc632623e5ce9b6e1870a9c'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'84a2de87ee8f4535-ATL'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=":443"; ma=86400')])
2024-01-23 14:43:30,893 - INFO - HTTP Request: POST https://api.openai.com/v1/completions "HTTP/1.1 200 OK"
2024-01-23 14:43:30,894 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2024-01-23 14:43:30,895 - DEBUG - receive_response_body.complete
2024-01-23 14:43:30,895 - DEBUG - response_closed.started
2024-01-23 14:43:30,896 - DEBUG - response_closed.complete
2024-01-23 14:43:30,896 - DEBUG - HTTP Request: POST https://api.openai.com/v1/completions "200 OK"
2024-01-23 14:43:30,898 - INFO - UML code generated for src/models/uml_diagram.py: @startuml
title uml_diagram.py
Vehicle "1" o-- "1" Vehicle : has-a
Car "1" o-- "1" Vehicle : is-a
Motorcycle "1" o-- "1" Vehicle : is-a
@enduml
2024-01-23 14:43:30,898 - INFO - Saving generated output to ../../output/uml_diagram.py.puml
2024-01-23 14:43:30,899 - INFO - Generated output saved to ../../output/uml_diagram.py.puml
2024-01-23 14:43:30,899 - INFO - Skipping empty file: src/routes/__init__.py
2024-01-23 14:43:30,899 - INFO - Processing file: src/routes/uml_from_repo.py
2024-01-23 14:43:30,899 - INFO - Sending prompt to OpenAI: Create UML diagrams in .puml format for the following code:

# uml_from_repo.py
import os
import fnmatch
import subprocess
import json
import git
import tempfile
import shutil
import logging
from logging import handlers  
from services.retrieve_code import clone_repo, retrieve_code
from utils.code_to_uml import generate_content  # Import the function from code_to_uml.py

 
# Configure logging to write to a file
log_directory = 'logs'
# Create the directory if it doesn't exist
os.makedirs(log_directory, exist_ok=True)  
log_filename = os.path.join(log_directory, 'uml_from_repo.log')
log_handler = handlers.RotatingFileHandler(log_filename, maxBytes=1024*1024, backupCount=5)  # Log file with rotation
log_formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')
log_handler.setFormatter(log_formatter)
logger = logging.getLogger()
logger.addHandler(log_handler)
logger.setLevel(logging.DEBUG)

def process_request(git_repo, github_access_token):
    output_directory = git_repo.local_dir  # Get the output directory from the GitRepo object
    branch_name = 'ma
2024-01-23 14:43:30,901 - DEBUG - Request options: {'method': 'post', 'url': '/completions', 'files': None, 'json_data': {'model': 'gpt-3.5-turbo-instruct', 'prompt': "Create UML diagrams in .puml format for the following code:\n\n# uml_from_repo.py\nimport os\nimport fnmatch\nimport subprocess\nimport json\nimport git\nimport tempfile\nimport shutil\nimport logging\nfrom logging import handlers  \nfrom services.retrieve_code import clone_repo, retrieve_code\nfrom utils.code_to_uml import generate_content  # Import the function from code_to_uml.py\n\n \n# Configure logging to write to a file\nlog_directory = 'logs'\n# Create the directory if it doesn't exist\nos.makedirs(log_directory, exist_ok=True)  \nlog_filename = os.path.join(log_directory, 'uml_from_repo.log')\nlog_handler = handlers.RotatingFileHandler(log_filename, maxBytes=1024*1024, backupCount=5)  # Log file with rotation\nlog_formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\nlog_handler.setFormatter(log_formatter)\nlogger = logging.getLogger()\nlogger.addHandler(log_handler)\nlogger.setLevel(logging.DEBUG)\n\ndef process_request(git_repo, github_access_token):\n    output_directory = git_repo.local_dir  # Get the output directory from the GitRepo object\n    branch_name = 'ma", 'max_tokens': 1024}}
2024-01-23 14:43:30,903 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2024-01-23 14:43:30,903 - DEBUG - send_request_headers.complete
2024-01-23 14:43:30,904 - DEBUG - send_request_body.started request=<Request [b'POST']>
2024-01-23 14:43:30,904 - DEBUG - send_request_body.complete
2024-01-23 14:43:30,904 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2024-01-23 14:43:35,622 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Tue, 23 Jan 2024 20:43:35 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'Cache-Control', b'no-cache, must-revalidate'), (b'openai-model', b'gpt-3.5-turbo-instruct'), (b'openai-organization', b'user-lqvxa4hv7a8o8eyohtzmcoc6'), (b'openai-processing-ms', b'4279'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=15724800; includeSubDomains'), (b'x-ratelimit-limit-requests', b'3000'), (b'x-ratelimit-limit-tokens', b'250000'), (b'x-ratelimit-remaining-requests', b'2999'), (b'x-ratelimit-remaining-tokens', b'248975'), (b'x-ratelimit-reset-requests', b'20ms'), (b'x-ratelimit-reset-tokens', b'245ms'), (b'x-request-id', b'4226909cd5183f79d4bf73ce736d7a5e'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'84a2de8e3ef14535-ATL'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=":443"; ma=86400')])
2024-01-23 14:43:35,624 - INFO - HTTP Request: POST https://api.openai.com/v1/completions "HTTP/1.1 200 OK"
2024-01-23 14:43:35,625 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2024-01-23 14:43:35,626 - DEBUG - receive_response_body.complete
2024-01-23 14:43:35,626 - DEBUG - response_closed.started
2024-01-23 14:43:35,627 - DEBUG - response_closed.complete
2024-01-23 14:43:35,627 - DEBUG - HTTP Request: POST https://api.openai.com/v1/completions "200 OK"
2024-01-23 14:43:35,629 - INFO - Received response from OpenAI: ster'
    if not os.path.exists(output_directory):
        # Clone the repository to the desired output directory
        clone_repo(git_repo.git_url, output_directory, branch_name, github_access_token)

    code_files = retrieve_code(output_directory)  # Get list of code files from output directory
    # Loop through each file and generate UML diagram
    for code_file in code_files:
        file_contents = generate_content(code_file)  # Use generate_content function to get the file's contents
        uml_diagram = generate_uml(file_contents)  # Use generate_uml function to create UML diagram
        save_uml(uml_diagram, code_file)  # Save UML diagram as image file in same directory as code file
 
 
 
@git    --- Instance Creation
git <|-- GitRepo
git <|-- RepoManager
RepoManager <|-- git
 
class git {
    string git_url
    string local_dir
 
    Operation clone_repo(git_url, local_dir, branch_name, github_access_token)
}
 
class GitRepo {
    string git_url
    string local_dir
}
 
class RepoManager {
    Operation retrieve_code(output_directory)
    Operation generate_uml(file_contents)
    Operation save_uml(uml_diagram, code_file)
}
 
class code_to_uml {
    Operation generate_content(code_file)
}
 
note bottom
    Import the function from code_to_uml.py
end note
 
class services.retrieve_code {
    Operation clone_repo(git_url, output_directory, branch_name, github_access_token)
}
 
note bottom
    Import the function from retrieve_code.py
end note
 
class utils.code_to_uml {
    Operation generate_content(code_file)
}
 
uml_from_repo.py |> RepoManager
RepoManager *-- git
RepoManager *-- code_to_uml
RepoManager -- services.retrieve_code
 
note bottom
    Configure logging to write to a file
end note
 
RepoManager <|.. uml_from_repo.py
2024-01-23 14:43:35,630 - INFO - Sending prompt to OpenAI: Create UML diagrams in .puml format for the following code:

ster'  # 'master' is the default branch name
    temp_dir = None  # Initialize temp_dir outside the try block

    try:
        if not git_repo.repo_url or not output_directory or not github_access_token or not branch_name:
            logger.error("Missing required parameters")  
            return {"error": "Missing required parameters"}, 400

        try:
            temp_dir = tempfile.mkdtemp()
        except Exception as e:
            logger.error(f"Error during UML generation: {str(e)}", exc_info=True)
            return {"error": str(e)}, 500

        # Check if the directory exists and remove it
        if os.path.exists(output_directory):
            shutil.rmtree(output_directory)

        # Clone the repository
        repo = git.Repo.clone_from(git_repo.repo_url, output_directory)

        # Load the config
        with open('src/routes/config.json', 'r') as f:
            config = json.load(f)
        
        def traverse_directories(repo, git_repo, config):
            included_files = {}
   
2024-01-23 14:43:35,632 - DEBUG - Request options: {'method': 'post', 'url': '/completions', 'files': None, 'json_data': {'model': 'gpt-3.5-turbo-instruct', 'prompt': 'Create UML diagrams in .puml format for the following code:\n\nster\'  # \'master\' is the default branch name\n    temp_dir = None  # Initialize temp_dir outside the try block\n\n    try:\n        if not git_repo.repo_url or not output_directory or not github_access_token or not branch_name:\n            logger.error("Missing required parameters")  \n            return {"error": "Missing required parameters"}, 400\n\n        try:\n            temp_dir = tempfile.mkdtemp()\n        except Exception as e:\n            logger.error(f"Error during UML generation: {str(e)}", exc_info=True)\n            return {"error": str(e)}, 500\n\n        # Check if the directory exists and remove it\n        if os.path.exists(output_directory):\n            shutil.rmtree(output_directory)\n\n        # Clone the repository\n        repo = git.Repo.clone_from(git_repo.repo_url, output_directory)\n\n        # Load the config\n        with open(\'src/routes/config.json\', \'r\') as f:\n            config = json.load(f)\n        \n        def traverse_directories(repo, git_repo, config):\n            included_files = {}\n   ', 'max_tokens': 1024}}
2024-01-23 14:43:35,633 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2024-01-23 14:43:35,634 - DEBUG - send_request_headers.complete
2024-01-23 14:43:35,634 - DEBUG - send_request_body.started request=<Request [b'POST']>
2024-01-23 14:43:35,634 - DEBUG - send_request_body.complete
2024-01-23 14:43:35,634 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2024-01-23 14:43:39,802 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Tue, 23 Jan 2024 20:43:39 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'Cache-Control', b'no-cache, must-revalidate'), (b'openai-model', b'gpt-3.5-turbo-instruct'), (b'openai-organization', b'user-lqvxa4hv7a8o8eyohtzmcoc6'), (b'openai-processing-ms', b'4005'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=15724800; includeSubDomains'), (b'x-ratelimit-limit-requests', b'3000'), (b'x-ratelimit-limit-tokens', b'250000'), (b'x-ratelimit-remaining-requests', b'2999'), (b'x-ratelimit-remaining-tokens', b'248975'), (b'x-ratelimit-reset-requests', b'20ms'), (b'x-ratelimit-reset-tokens', b'245ms'), (b'x-request-id', b'45baaef35383afa9bcebd1da16896f90'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'84a2deabce0d4535-ATL'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=":443"; ma=86400')])
2024-01-23 14:43:39,804 - INFO - HTTP Request: POST https://api.openai.com/v1/completions "HTTP/1.1 200 OK"
2024-01-23 14:43:39,805 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2024-01-23 14:43:39,806 - DEBUG - receive_response_body.complete
2024-01-23 14:43:39,807 - DEBUG - response_closed.started
2024-01-23 14:43:39,807 - DEBUG - response_closed.complete
2024-01-23 14:43:39,808 - DEBUG - HTTP Request: POST https://api.openai.com/v1/completions "200 OK"
2024-01-23 14:43:39,809 - INFO - Received response from OpenAI:  Directory: temp_dir
    Option -File: src/routes/generate_umls.puml
    Option -File: src/routes/config.json
    Option -File: README.md
    Option -File: LICENSE

    Directory: output_directory
    Option -File: README.md
    Option -File: LICENSE
    
    Directory: git_repo
    Option -Repository URL: git_repo.repo_url
    Option -Branch Name: branch_name
    

    Class: UMLGenerator
    -git_repo: GitRepo
    -output_directory: String
    -github_access_token: String
    -branch_name: String
    # -temp_dir: String

    Function: UMLGenerator()
    -temp_dir = None

    try:
        if(not git_repo.repo_url or not output_directory or not github_access_token or not branch_name)
        -logger: Logger
        # returns {"error": "Missing required parameters"}, 400

    Function: generate()
    # temp_dir = tempfile.mkdtemp()
    -os: OS
    -shutil: Shutil
    -git: Git
    -json: JSON
    -f: File
    -config: JSON Object
    
    Function: traverse_directories()
    -repo: Repo
    -git_repo: GitRepo
    -config: JSON Object
    # included_files = {}
    
    Directory: temp_dir
    Option -Directory: output_directory
    
    Class: Logger
    -log_message: String
    
    Function: error()
    # -returns {"error": str(e)}, 500
2024-01-23 14:43:39,810 - INFO - Sending prompt to OpenAI: Create UML diagrams in .puml format for the following code:

         for item in repo.tree().traverse():
                if item.type == 'blob':  # This means it's a file, not a directory
                    for pattern in config['include']:
                        if fnmatch.fnmatch(item.path, pattern):
                            with open(os.path.join(git_repo.local_dir, item.path), 'r') as f:
                                included_files[item.path] = f.read()
                            break  # No need to check the remaining patterns
            return included_files

        # Retrieve the code from the repository
        included_files = traverse_directories(repo, git_repo, config)

        logger.debug(f"Type of included_files: {type(included_files)}")
        logger.debug(f"Value of included_files: {included_files}")

        # Save the UML diagram to a file
        logger.info(f"Saving UML diagram to {output_directory}")
        final_output_paths = generate_content(included_files, output_directory)  # This is now a list of file paths
        logger.info(f"
2024-01-23 14:43:39,813 - DEBUG - Request options: {'method': 'post', 'url': '/completions', 'files': None, 'json_data': {'model': 'gpt-3.5-turbo-instruct', 'prompt': 'Create UML diagrams in .puml format for the following code:\n\n         for item in repo.tree().traverse():\n                if item.type == \'blob\':  # This means it\'s a file, not a directory\n                    for pattern in config[\'include\']:\n                        if fnmatch.fnmatch(item.path, pattern):\n                            with open(os.path.join(git_repo.local_dir, item.path), \'r\') as f:\n                                included_files[item.path] = f.read()\n                            break  # No need to check the remaining patterns\n            return included_files\n\n        # Retrieve the code from the repository\n        included_files = traverse_directories(repo, git_repo, config)\n\n        logger.debug(f"Type of included_files: {type(included_files)}")\n        logger.debug(f"Value of included_files: {included_files}")\n\n        # Save the UML diagram to a file\n        logger.info(f"Saving UML diagram to {output_directory}")\n        final_output_paths = generate_content(included_files, output_directory)  # This is now a list of file paths\n        logger.info(f"', 'max_tokens': 1024}}
2024-01-23 14:43:39,814 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2024-01-23 14:43:39,815 - DEBUG - send_request_headers.complete
2024-01-23 14:43:39,815 - DEBUG - send_request_body.started request=<Request [b'POST']>
2024-01-23 14:43:39,815 - DEBUG - send_request_body.complete
2024-01-23 14:43:39,816 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2024-01-23 14:43:48,773 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Tue, 23 Jan 2024 20:43:48 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'Cache-Control', b'no-cache, must-revalidate'), (b'openai-model', b'gpt-3.5-turbo-instruct'), (b'openai-organization', b'user-lqvxa4hv7a8o8eyohtzmcoc6'), (b'openai-processing-ms', b'8879'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=15724800; includeSubDomains'), (b'x-ratelimit-limit-requests', b'3000'), (b'x-ratelimit-limit-tokens', b'250000'), (b'x-ratelimit-remaining-requests', b'2999'), (b'x-ratelimit-remaining-tokens', b'248975'), (b'x-ratelimit-reset-requests', b'20ms'), (b'x-ratelimit-reset-tokens', b'245ms'), (b'x-request-id', b'9860654a82331c54c76e59ca894845fa'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'84a2dec5f9a84535-ATL'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=":443"; ma=86400')])
2024-01-23 14:43:48,774 - INFO - HTTP Request: POST https://api.openai.com/v1/completions "HTTP/1.1 200 OK"
2024-01-23 14:43:48,774 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2024-01-23 14:43:48,774 - DEBUG - receive_response_body.complete
2024-01-23 14:43:48,775 - DEBUG - response_closed.started
2024-01-23 14:43:48,775 - DEBUG - response_closed.complete
2024-01-23 14:43:48,775 - DEBUG - HTTP Request: POST https://api.openai.com/v1/completions "200 OK"
2024-01-23 14:43:48,776 - INFO - Received response from OpenAI: Finished generating UML diagram")

@startuml
class TraverseDirectory {
    -repo: Repository
    -git_repo: GitRepository
    -config: Config
    +traverse_directories(repo: Repository, git_repo: GitRepository, config: Config): included_files
}
class Repository {
    -tree(): Tree
}
class Tree {
    +traverse(): Item[]
}
class Item {
    -type: String
    -path: String
}
class Config {
    -include: String[]
}
class GitRepository {
    -local_dir: String
}
class Logger {
    +debug(msg: String)
    +info(msg: String)
}
class File {
    -path: String
    -file_contents: String
}
TraverseDirectory -- Repository
TraverseDirectory -- GitRepository
TraverseDirectory -- Config
Repository *-- Tree
Tree *-- Item
Config o-- String
GitRepository *-- File
Logger *-- Provider
TraverseDirectory ..> TraverseDirectoriesRepo : <<create>>
Repo ..> Tree : <<create>>
end

@startuml
class TraverseDirectoriesRepo {
    -repo: Repository
    -git_repo: GitRepository
    -config: Config
    +traverse_directories(repo: Repository, git_repo: GitRepository, config: Config): included_files
}
class Repository {
    -tree(): Tree
}
class Tree {
    +traverse(): Item[]
}
class Item {
    -type: String
    -path: String
}
class Config {
    -include: String[]
}
class GitRepository {
    -local_dir: String
}
class Logger {
    +debug(msg: String)
    +info(msg: String)
}
class File {
    -path: String
    -file_contents: String
}
TraverseDirectoriesRepo -- Repository
TraverseDirectoriesRepo -- GitRepository
TraverseDirectoriesRepo -- Config
Repository *-- Tree
Tree *-- Item
Config o-- String
GitRepository *-- File
Logger *-- Provider
end 

@startuml
class GitRepository {
    -local_dir: String
    -git_repo: String
    +token: String
    -credentials: Credentials
    +get_local_dir(): String
    +get_git_repo(): String
}
class Credentials {
    -username: String
    -password: String
    +get_username(): String
    +get_password(): String
}
class Repository {
    -remote_url: String
    +clone(local_dir: String): GitRepository
    -get_repo(): Repository
}
class Logger {
    +debug(msg: String)
    +info(msg: String)
}
class Config {
    -repo_name: String
    -output_directory: String
    -include: String[]
}
class File {
    -path: String
    -file_contents: String
}
GitRepository *-- Credentials
GitRepository *-- Repository
Repository *-- Credentials
Logger *-- Provider
Config o-- String
Config *-- include : String[] : <<array>>
end

@startuml
class Provider {
    +create_repo(repo_name: String): Repository
    +delete_repo(repo_name: String)
    +generate_uml_repo(repo_name: String): String[]
    +save(repo_name: String, output_directory: String)
}
class Repository {
    -remote_url: String
    +clone(local_dir: String): GitRepository
    +get_repo(): Repository
    +tree(): Tree
    +delete()
}
class Logger {
    +debug(msg: String)
    +info(msg: String)
}
class Tree {
    -items: Item[]
    +traverse(): Item[]
}
class Item {
    -type: String
    -path: String
}
class File {
    -path: String
    -file_contents: String
}
Provider *-- Repository
Provider *-- Logger
Repository *-- File
Repository *-- Tree
Tree *-- Item
end
2024-01-23 14:43:48,777 - INFO - Sending prompt to OpenAI: Create UML diagrams in .puml format for the following code:

Final output paths: {final_output_paths}")

        for path in final_output_paths:
            logger.info(f"UML diagram saved at: {path}")  # Log the path where each UML diagram was saved

        return {
            "message": "UML diagrams generated successfully",
            "details": {
                "Repository": git_repo.repo_url,
                "Output Paths": final_output_paths  # This is now a list of file paths
            }
        }, 200

    except Exception as e:
        logger.error(f"Error during UML generation: {str(e)}")
        return {"error": str(e)}, 500

    finally:
        # Clean up the temporary directory
        if temp_dir is not None:
            logger.info("Cleaning up temporary directory")
            shutil.rmtree(temp_dir)

2024-01-23 14:43:48,779 - DEBUG - Request options: {'method': 'post', 'url': '/completions', 'files': None, 'json_data': {'model': 'gpt-3.5-turbo-instruct', 'prompt': 'Create UML diagrams in .puml format for the following code:\n\nFinal output paths: {final_output_paths}")\n\n        for path in final_output_paths:\n            logger.info(f"UML diagram saved at: {path}")  # Log the path where each UML diagram was saved\n\n        return {\n            "message": "UML diagrams generated successfully",\n            "details": {\n                "Repository": git_repo.repo_url,\n                "Output Paths": final_output_paths  # This is now a list of file paths\n            }\n        }, 200\n\n    except Exception as e:\n        logger.error(f"Error during UML generation: {str(e)}")\n        return {"error": str(e)}, 500\n\n    finally:\n        # Clean up the temporary directory\n        if temp_dir is not None:\n            logger.info("Cleaning up temporary directory")\n            shutil.rmtree(temp_dir)\n', 'max_tokens': 1024}}
2024-01-23 14:43:48,780 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2024-01-23 14:43:48,780 - DEBUG - send_request_headers.complete
2024-01-23 14:43:48,780 - DEBUG - send_request_body.started request=<Request [b'POST']>
2024-01-23 14:43:48,781 - DEBUG - send_request_body.complete
2024-01-23 14:43:48,781 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2024-01-23 14:43:51,577 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Tue, 23 Jan 2024 20:43:51 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'Cache-Control', b'no-cache, must-revalidate'), (b'openai-model', b'gpt-3.5-turbo-instruct'), (b'openai-organization', b'user-lqvxa4hv7a8o8eyohtzmcoc6'), (b'openai-processing-ms', b'2676'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=15724800; includeSubDomains'), (b'x-ratelimit-limit-requests', b'3000'), (b'x-ratelimit-limit-tokens', b'250000'), (b'x-ratelimit-remaining-requests', b'2999'), (b'x-ratelimit-remaining-tokens', b'248975'), (b'x-ratelimit-reset-requests', b'20ms'), (b'x-ratelimit-reset-tokens', b'245ms'), (b'x-request-id', b'9537c044216a9470650e07554f56e352'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'84a2defdfcd14535-ATL'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=":443"; ma=86400')])
2024-01-23 14:43:51,579 - INFO - HTTP Request: POST https://api.openai.com/v1/completions "HTTP/1.1 200 OK"
2024-01-23 14:43:51,580 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2024-01-23 14:43:51,582 - DEBUG - receive_response_body.complete
2024-01-23 14:43:51,582 - DEBUG - response_closed.started
2024-01-23 14:43:51,583 - DEBUG - response_closed.complete
2024-01-23 14:43:51,583 - DEBUG - HTTP Request: POST https://api.openai.com/v1/completions "200 OK"
2024-01-23 14:43:51,586 - INFO - Received response from OpenAI: # UML diagrams generating use as follows
generate_uml(git_repo)

@startuml

class GitRepo {
    {field} .repo_url
}

class GenerateUML {
    {field} .repo = GitRepo
    {field} .output_path
    {field} .temp_dir

    {method} + generate_uml(git_repo)
    {method} - clean_up()
}

class Logger {
    {method} + info(message)
    {method} + error(message)
}

class shutil {
    {method} + rmtree(path)
}

class tempfile {
    {method} + NamedTemporaryFile(delete)
}

class Exception

interface Flask {
    {method} + route()
}

Note: Classes for "GitRepo", "Logger", "shutil", "tempfile", "Exception", and interface "Flask" are not shown here as they are predefined and do not require any additional fields or methods.

Flask --> GenerateUML
GenerateUML --> GitRepo
GenerateUML --> Logger
GenerateUML --> shutil
GenerateUML --> tempfile
GenerateUML --> Flask
GenerateUML --> Exception
2024-01-23 14:43:51,589 - DEBUG - Request options: {'method': 'post', 'url': '/completions', 'files': None, 'json_data': {'model': 'gpt-3.5-turbo-instruct', 'prompt': 'Create a .puml file that integrates the following .puml files:\n\nster\'\n    if not os.path.exists(output_directory):\n        # Clone the repository to the desired output directory\n        clone_repo(git_repo.git_url, output_directory, branch_name, github_access_token)\n\n    code_files = retrieve_code(output_directory)  # Get list of code files from output directory\n    # Loop through each file and generate UML diagram\n    for code_file in code_files:\n        file_contents = generate_content(code_file)  # Use generate_content function to get the file\'s contents\n        uml_diagram = generate_uml(file_contents)  # Use generate_uml function to create UML diagram\n        save_uml(uml_diagram, code_file)  # Save UML diagram as image file in same directory as code file\n \n \n \n@git    --- Instance Creation\ngit <|-- GitRepo\ngit <|-- RepoManager\nRepoManager <|-- git\n \nclass git {\n    string git_url\n    string local_dir\n \n    Operation clone_repo(git_url, local_dir, branch_name, github_access_token)\n}\n \nclass GitRepo {\n    string git_url\n    string local_dir\n}\n \nclass RepoManager {\n    Operation retrieve_code(output_directory)\n    Operation generate_uml(file_contents)\n    Operation save_uml(uml_diagram, code_file)\n}\n \nclass code_to_uml {\n    Operation generate_content(code_file)\n}\n \nnote bottom\n    Import the function from code_to_uml.py\nend note\n \nclass services.retrieve_code {\n    Operation clone_repo(git_url, output_directory, branch_name, github_access_token)\n}\n \nnote bottom\n    Import the function from retrieve_code.py\nend note\n \nclass utils.code_to_uml {\n    Operation generate_content(code_file)\n}\n \numl_from_repo.py |> RepoManager\nRepoManager *-- git\nRepoManager *-- code_to_uml\nRepoManager -- services.retrieve_code\n \nnote bottom\n    Configure logging to write to a file\nend note\n \nRepoManager <|.. uml_from_repo.py\nDirectory: temp_dir\n    Option -File: src/routes/generate_umls.puml\n    Option -File: src/routes/config.json\n    Option -File: README.md\n    Option -File: LICENSE\n\n    Directory: output_directory\n    Option -File: README.md\n    Option -File: LICENSE\n    \n    Directory: git_repo\n    Option -Repository URL: git_repo.repo_url\n    Option -Branch Name: branch_name\n    \n\n    Class: UMLGenerator\n    -git_repo: GitRepo\n    -output_directory: String\n    -github_access_token: String\n    -branch_name: String\n    # -temp_dir: String\n\n    Function: UMLGenerator()\n    -temp_dir = None\n\n    try:\n        if(not git_repo.repo_url or not output_directory or not github_access_token or not branch_name)\n        -logger: Logger\n        # returns {"error": "Missing required parameters"}, 400\n\n    Function: generate()\n    # temp_dir = tempfile.mkdtemp()\n    -os: OS\n    -shutil: Shutil\n    -git: Git\n    -json: JSON\n    -f: File\n    -config: JSON Object\n    \n    Function: traverse_directories()\n    -repo: Repo\n    -git_repo: GitRepo\n    -config: JSON Object\n    # included_files = {}\n    \n    Directory: temp_dir\n    Option -Directory: output_directory\n    \n    Class: Logger\n    -log_message: String\n    \n    Function: error()\n    # -returns {"error": str(e)}, 500\nFinished generating UML diagram")\n\n@startuml\nclass TraverseDirectory {\n    -repo: Repository\n    -git_repo: GitRepository\n    -config: Config\n    +traverse_directories(repo: Repository, git_repo: GitRepository, config: Config): included_files\n}\nclass Repository {\n    -tree(): Tree\n}\nclass Tree {\n    +traverse(): Item[]\n}\nclass Item {\n    -type: String\n    -path: String\n}\nclass Config {\n    -include: String[]\n}\nclass GitRepository {\n    -local_dir: String\n}\nclass Logger {\n    +debug(msg: String)\n    +info(msg: String)\n}\nclass File {\n    -path: String\n    -file_contents: String\n}\nTraverseDirectory -- Repository\nTraverseDirectory -- GitRepository\nTraverseDirectory -- Config\nRepository *-- Tree\nTree *-- Item\nConfig o-- String\nGitRepository *-- File\nLogger *-- Provider\nTraverseDirectory ..> TraverseDirectoriesRepo : <<create>>\nRepo ..> Tree : <<create>>\nend\n\n@startuml\nclass TraverseDirectoriesRepo {\n    -repo: Repository\n    -git_repo: GitRepository\n    -config: Config\n    +traverse_directories(repo: Repository, git_repo: GitRepository, config: Config): included_files\n}\nclass Repository {\n    -tree(): Tree\n}\nclass Tree {\n    +traverse(): Item[]\n}\nclass Item {\n    -type: String\n    -path: String\n}\nclass Config {\n    -include: String[]\n}\nclass GitRepository {\n    -local_dir: String\n}\nclass Logger {\n    +debug(msg: String)\n    +info(msg: String)\n}\nclass File {\n    -path: String\n    -file_contents: String\n}\nTraverseDirectoriesRepo -- Repository\nTraverseDirectoriesRepo -- GitRepository\nTraverseDirectoriesRepo -- Config\nRepository *-- Tree\nTree *-- Item\nConfig o-- String\nGitRepository *-- File\nLogger *-- Provider\nend \n\n@startuml\nclass GitRepository {\n    -local_dir: String\n    -git_repo: String\n    +token: String\n    -credentials: Credentials\n    +get_local_dir(): String\n    +get_git_repo(): String\n}\nclass Credentials {\n    -username: String\n    -password: String\n    +get_username(): String\n    +get_password(): String\n}\nclass Repository {\n    -remote_url: String\n    +clone(local_dir: String): GitRepository\n    -get_repo(): Repository\n}\nclass Logger {\n    +debug(msg: String)\n    +info(msg: String)\n}\nclass Config {\n    -repo_name: String\n    -output_directory: String\n    -include: String[]\n}\nclass File {\n    -path: String\n    -file_contents: String\n}\nGitRepository *-- Credentials\nGitRepository *-- Repository\nRepository *-- Credentials\nLogger *-- Provider\nConfig o-- String\nConfig *-- include : String[] : <<array>>\nend\n\n@startuml\nclass Provider {\n    +create_repo(repo_name: String): Repository\n    +delete_repo(repo_name: String)\n    +generate_uml_repo(repo_name: String): String[]\n    +save(repo_name: String, output_directory: String)\n}\nclass Repository {\n    -remote_url: String\n    +clone(local_dir: String): GitRepository\n    +get_repo(): Repository\n    +tree(): Tree\n    +delete()\n}\nclass Logger {\n    +debug(msg: String)\n    +info(msg: String)\n}\nclass Tree {\n    -items: Item[]\n    +traverse(): Item[]\n}\nclass Item {\n    -type: String\n    -path: String\n}\nclass File {\n    -path: String\n    -file_contents: String\n}\nProvider *-- Repository\nProvider *-- Logger\nRepository *-- File\nRepository *-- Tree\nTree *-- Item\nend\n# UML diagrams generating use as follows\ngenerate_uml(git_repo)\n\n@startuml\n\nclass GitRepo {\n    {field} .repo_url\n}\n\nclass GenerateUML {\n    {field} .repo = GitRepo\n    {field} .output_path\n    {field} .temp_dir\n\n    {method} + generate_uml(git_repo)\n    {method} - clean_up()\n}\n\nclass Logger {\n    {method} + info(message)\n    {method} + error(message)\n}\n\nclass shutil {\n    {method} + rmtree(path)\n}\n\nclass tempfile {\n    {method} + NamedTemporaryFile(delete)\n}\n\nclass Exception\n\ninterface Flask {\n    {method} + route()\n}\n\nNote: Classes for "GitRepo", "Logger", "shutil", "tempfile", "Exception", and interface "Flask" are not shown here as they are predefined and do not require any additional fields or methods.\n\nFlask --> GenerateUML\nGenerateUML --> GitRepo\nGenerateUML --> Logger\nGenerateUML --> shutil\nGenerateUML --> tempfile\nGenerateUML --> Flask\nGenerateUML --> Exception', 'max_tokens': 1024}}
2024-01-23 14:43:51,591 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2024-01-23 14:43:51,592 - DEBUG - send_request_headers.complete
2024-01-23 14:43:51,592 - DEBUG - send_request_body.started request=<Request [b'POST']>
2024-01-23 14:43:51,592 - DEBUG - send_request_body.complete
2024-01-23 14:43:51,592 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2024-01-23 14:43:53,727 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Tue, 23 Jan 2024 20:43:53 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'Cache-Control', b'no-cache, must-revalidate'), (b'openai-model', b'gpt-3.5-turbo-instruct'), (b'openai-organization', b'user-lqvxa4hv7a8o8eyohtzmcoc6'), (b'openai-processing-ms', b'1954'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=15724800; includeSubDomains'), (b'x-ratelimit-limit-requests', b'3000'), (b'x-ratelimit-limit-tokens', b'250000'), (b'x-ratelimit-remaining-requests', b'2999'), (b'x-ratelimit-remaining-tokens', b'248975'), (b'x-ratelimit-reset-requests', b'20ms'), (b'x-ratelimit-reset-tokens', b'245ms'), (b'x-request-id', b'68f1698694d5ed8a45e5aaa3d9d2dbd6'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'84a2df0f9d084535-ATL'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=":443"; ma=86400')])
2024-01-23 14:43:53,729 - INFO - HTTP Request: POST https://api.openai.com/v1/completions "HTTP/1.1 200 OK"
2024-01-23 14:43:53,730 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2024-01-23 14:43:53,731 - DEBUG - receive_response_body.complete
2024-01-23 14:43:53,731 - DEBUG - response_closed.started
2024-01-23 14:43:53,731 - DEBUG - response_closed.complete
2024-01-23 14:43:53,732 - DEBUG - HTTP Request: POST https://api.openai.com/v1/completions "200 OK"
2024-01-23 14:43:53,734 - INFO - UML code generated for src/routes/uml_from_repo.py: @startuml
title uml_from_repo.py
GenerateUML <- code_to_uml.py
GenerateUML - temp_dir
code_to_uml.py -> String
note bottom
    Code files retrieved from temp_dir
end note
GenerateUML - output_path
note bottom
    UML diagrams saved to output_path
end note
GenerateUML - repo
note bottom
    Uses methods from GitRepo class
end note
code_to_uml.py -> File
note bottom
    File contents retrieved from code file
end note
GenerateUML -> Tree
note bottom
    Traverse tree to generate UML diagrams for each file
end note
Tree - Item
note bottom
    File information retrieved from each item in the tree
end note
GenerateUML -> Logger
note bottom
    Log messages for debugging or error handling
end note

@enduml
2024-01-23 14:43:53,735 - INFO - Saving generated output to ../../output/uml_from_repo.py.puml
2024-01-23 14:43:53,735 - INFO - Generated output saved to ../../output/uml_from_repo.py.puml
2024-01-23 14:43:53,736 - INFO - Skipping empty file: src/scripts/__init__.py
2024-01-23 14:43:53,736 - INFO - Processing file: src/scripts/execute_generate_uml.py
2024-01-23 14:43:53,736 - INFO - Sending prompt to OpenAI: Create UML diagrams in .puml format for the following code:

import json
import os
import requests
import logging
from logging import handlers
from dotenv import load_dotenv

# Load environment variables from .env file
load_dotenv()

# Configure logging to write to a file
log_directory = '../../logs'
# Create the directory if it doesn't exist
os.makedirs(log_directory, exist_ok=True)  
log_filename = os.path.join(log_directory, 'execute_generate_uml.log')
log_handler = handlers.RotatingFileHandler(log_filename, maxBytes=1024*1024, backupCount=5)  # Log file with rotation
log_formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')
log_handler.setFormatter(log_formatter)
logger = logging.getLogger()
logger.addHandler(log_handler)
logger.setLevel(logging.DEBUG)


# Configure logging
logging.basicConfig(filename='execute_generate_uml.log', level=logging.DEBUG, format='%(asctime)s - %(levelname)s - %(message)s')

# Current file's directory
current_dir = os.path.dirname(os.path.abspath(__file__))
# Configuration file path
config_file_path = os.path.join(cu
2024-01-23 14:43:53,738 - DEBUG - Request options: {'method': 'post', 'url': '/completions', 'files': None, 'json_data': {'model': 'gpt-3.5-turbo-instruct', 'prompt': "Create UML diagrams in .puml format for the following code:\n\nimport json\nimport os\nimport requests\nimport logging\nfrom logging import handlers\nfrom dotenv import load_dotenv\n\n# Load environment variables from .env file\nload_dotenv()\n\n# Configure logging to write to a file\nlog_directory = '../../logs'\n# Create the directory if it doesn't exist\nos.makedirs(log_directory, exist_ok=True)  \nlog_filename = os.path.join(log_directory, 'execute_generate_uml.log')\nlog_handler = handlers.RotatingFileHandler(log_filename, maxBytes=1024*1024, backupCount=5)  # Log file with rotation\nlog_formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\nlog_handler.setFormatter(log_formatter)\nlogger = logging.getLogger()\nlogger.addHandler(log_handler)\nlogger.setLevel(logging.DEBUG)\n\n\n# Configure logging\nlogging.basicConfig(filename='execute_generate_uml.log', level=logging.DEBUG, format='%(asctime)s - %(levelname)s - %(message)s')\n\n# Current file's directory\ncurrent_dir = os.path.dirname(os.path.abspath(__file__))\n# Configuration file path\nconfig_file_path = os.path.join(cu", 'max_tokens': 1024}}
2024-01-23 14:43:53,739 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2024-01-23 14:43:53,740 - DEBUG - send_request_headers.complete
2024-01-23 14:43:53,740 - DEBUG - send_request_body.started request=<Request [b'POST']>
2024-01-23 14:43:53,740 - DEBUG - send_request_body.complete
2024-01-23 14:43:53,740 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2024-01-23 14:43:56,288 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Tue, 23 Jan 2024 20:43:55 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'Cache-Control', b'no-cache, must-revalidate'), (b'openai-model', b'gpt-3.5-turbo-instruct'), (b'openai-organization', b'user-lqvxa4hv7a8o8eyohtzmcoc6'), (b'openai-processing-ms', b'1932'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=15724800; includeSubDomains'), (b'x-ratelimit-limit-requests', b'3000'), (b'x-ratelimit-limit-tokens', b'250000'), (b'x-ratelimit-remaining-requests', b'2999'), (b'x-ratelimit-remaining-tokens', b'248975'), (b'x-ratelimit-reset-requests', b'20ms'), (b'x-ratelimit-reset-tokens', b'245ms'), (b'x-request-id', b'd38ec8e22f19b2122203c2c85103e67a'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'84a2df1d6fff4535-ATL'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=":443"; ma=86400')])
2024-01-23 14:43:56,291 - INFO - HTTP Request: POST https://api.openai.com/v1/completions "HTTP/1.1 200 OK"
2024-01-23 14:43:56,291 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2024-01-23 14:43:56,293 - DEBUG - receive_response_body.complete
2024-01-23 14:43:56,293 - DEBUG - response_closed.started
2024-01-23 14:43:56,294 - DEBUG - response_closed.complete
2024-01-23 14:43:56,294 - DEBUG - HTTP Request: POST https://api.openai.com/v1/completions "200 OK"
2024-01-23 14:43:56,297 - INFO - Received response from OpenAI: rrent_dir, 'config.json')


@startuml
title UML diagrams for "execute_generate_uml" code

class json {
    - data : any
    + loads(string) : dict
    + dumps(dict) : string
}

class os {
    + path : string
    + makedirs(string, bool)
    + join(string, string) : string
}

class requests {
    - url : string
    + get(string) : Response
    + Response
}

class logging {
    - format : string
    - level : int
    - handlers : list
    - Logger
    + DEBUG : int
    + getLogger() : Logger
}

class handlers {
    - RotatingFileHandler
}

class dotenv {
    + load_dotenv()
}

class execute_generate_uml {
    - log_directory : string
    - log_filename : string
    - log_handler : RotatingFileHandl
2024-01-23 14:43:56,298 - INFO - Sending prompt to OpenAI: Create UML diagrams in .puml format for the following code:

rrent_dir, 'config.json')

# Read configuration from the JSON file
with open(config_file_path, 'r') as config_file:
    config_data = json.load(config_file)

# Retrieve the GitHub access token from environment variables
github_token = os.getenv('GITHUB_PAT')

# Endpoint URL
url = 'http://127.0.0.1:5000/generate-uml'

# Headers
headers = {
    'Content-Type': 'application/json'
}

# Update the GitHub access token, local directory, and configFile in the config data
config_data['gitHubAccessToken'] = github_token
config_data['local_dir'] = os.path.join(current_dir, '../..', 'output')
config_data['configFile'] = config_file_path  

# Log the JSON data being sent in the request
logging.info(f'Sending JSON data to {url}:')
logging.info(json.dumps(config_data, indent=2))

try:
    # Make the POST request
    response = requests.post(url, headers=headers, json=config_data)

    # Log the response from the server
    logging.info(f'Response from server ({url}):')
    logging.info(f'Status Code: {response.status_code}'
2024-01-23 14:43:56,300 - DEBUG - Request options: {'method': 'post', 'url': '/completions', 'files': None, 'json_data': {'model': 'gpt-3.5-turbo-instruct', 'prompt': "Create UML diagrams in .puml format for the following code:\n\nrrent_dir, 'config.json')\n\n# Read configuration from the JSON file\nwith open(config_file_path, 'r') as config_file:\n    config_data = json.load(config_file)\n\n# Retrieve the GitHub access token from environment variables\ngithub_token = os.getenv('GITHUB_PAT')\n\n# Endpoint URL\nurl = 'http://127.0.0.1:5000/generate-uml'\n\n# Headers\nheaders = {\n    'Content-Type': 'application/json'\n}\n\n# Update the GitHub access token, local directory, and configFile in the config data\nconfig_data['gitHubAccessToken'] = github_token\nconfig_data['local_dir'] = os.path.join(current_dir, '../..', 'output')\nconfig_data['configFile'] = config_file_path  \n\n# Log the JSON data being sent in the request\nlogging.info(f'Sending JSON data to {url}:')\nlogging.info(json.dumps(config_data, indent=2))\n\ntry:\n    # Make the POST request\n    response = requests.post(url, headers=headers, json=config_data)\n\n    # Log the response from the server\n    logging.info(f'Response from server ({url}):')\n    logging.info(f'Status Code: {response.status_code}'", 'max_tokens': 1024}}
2024-01-23 14:43:56,301 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2024-01-23 14:43:56,302 - DEBUG - send_request_headers.complete
2024-01-23 14:43:56,302 - DEBUG - send_request_body.started request=<Request [b'POST']>
2024-01-23 14:43:56,302 - DEBUG - send_request_body.complete
2024-01-23 14:43:56,302 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2024-01-23 14:43:59,258 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Tue, 23 Jan 2024 20:43:58 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'Cache-Control', b'no-cache, must-revalidate'), (b'openai-model', b'gpt-3.5-turbo-instruct'), (b'openai-organization', b'user-lqvxa4hv7a8o8eyohtzmcoc6'), (b'openai-processing-ms', b'2534'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=15724800; includeSubDomains'), (b'x-ratelimit-limit-requests', b'3000'), (b'x-ratelimit-limit-tokens', b'250000'), (b'x-ratelimit-remaining-requests', b'2999'), (b'x-ratelimit-remaining-tokens', b'248975'), (b'x-ratelimit-reset-requests', b'20ms'), (b'x-ratelimit-reset-tokens', b'245ms'), (b'x-request-id', b'd8ae3be9eb482445e0dbc1843c1c3fc5'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'84a2df2cfb114535-ATL'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=":443"; ma=86400')])
2024-01-23 14:43:59,260 - INFO - HTTP Request: POST https://api.openai.com/v1/completions "HTTP/1.1 200 OK"
2024-01-23 14:43:59,261 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2024-01-23 14:43:59,262 - DEBUG - receive_response_body.complete
2024-01-23 14:43:59,262 - DEBUG - response_closed.started
2024-01-23 14:43:59,263 - DEBUG - response_closed.complete
2024-01-23 14:43:59,263 - DEBUG - HTTP Request: POST https://api.openai.com/v1/completions "200 OK"
2024-01-23 14:43:59,264 - INFO - Received response from OpenAI: )


@startuml

rectangle "main.py" {
    rectangle "config_file_path" as config_file_path
    database "config.json" as config_file
    rectangle "config_data" as config_data
    interface "json.load()" as json_load
    rectangle "github_token" as github_token
    database "environment variables" as env_variables
    rectangle "url" as url
    rectangle "headers" as headers
    rectangle "logging" as logging
    rectangle "requests" as requests
    interface "json.dumps()" as json_dumps
    rectangle "response" as response

    config_file_path --> config_file
    config_file --( json_load
    json_load --( config_data
    env_variables --> github_token
    url --> headers
    logging --> response
    config_data -> headers
    github_token -> headers
    config_data -> url
    headers --o requests
    json_dumps --> logging
    requests <-- response

    header --> url : POST
    url --> response

    response --> logging
    logging --> logging : info
}

@enduml
2024-01-23 14:43:59,265 - INFO - Sending prompt to OpenAI: Create UML diagrams in .puml format for the following code:

)
    logging.info(f'Response Text: {response.text}')

    # Print the response from the server
    print(response.text)

    # Save the response to a file in the output directory
    output_dir = os.path.join(current_dir, '../..', 'output')
    os.makedirs(output_dir, exist_ok=True)
    with open(os.path.join(output_dir, 'response.json'), 'w') as output_file:
        json.dump(response.json(), output_file, indent=2)

except Exception as e:
    # Log any exceptions that occur
    logging.error(f'Error occurred: {str(e)}')
2024-01-23 14:43:59,267 - DEBUG - Request options: {'method': 'post', 'url': '/completions', 'files': None, 'json_data': {'model': 'gpt-3.5-turbo-instruct', 'prompt': "Create UML diagrams in .puml format for the following code:\n\n)\n    logging.info(f'Response Text: {response.text}')\n\n    # Print the response from the server\n    print(response.text)\n\n    # Save the response to a file in the output directory\n    output_dir = os.path.join(current_dir, '../..', 'output')\n    os.makedirs(output_dir, exist_ok=True)\n    with open(os.path.join(output_dir, 'response.json'), 'w') as output_file:\n        json.dump(response.json(), output_file, indent=2)\n\nexcept Exception as e:\n    # Log any exceptions that occur\n    logging.error(f'Error occurred: {str(e)}')", 'max_tokens': 1024}}
2024-01-23 14:43:59,268 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2024-01-23 14:43:59,268 - DEBUG - send_request_headers.complete
2024-01-23 14:43:59,269 - DEBUG - send_request_body.started request=<Request [b'POST']>
2024-01-23 14:43:59,269 - DEBUG - send_request_body.complete
2024-01-23 14:43:59,269 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2024-01-23 14:44:00,795 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Tue, 23 Jan 2024 20:44:00 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'Cache-Control', b'no-cache, must-revalidate'), (b'openai-model', b'gpt-3.5-turbo-instruct'), (b'openai-organization', b'user-lqvxa4hv7a8o8eyohtzmcoc6'), (b'openai-processing-ms', b'1146'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=15724800; includeSubDomains'), (b'x-ratelimit-limit-requests', b'3000'), (b'x-ratelimit-limit-tokens', b'250000'), (b'x-ratelimit-remaining-requests', b'2999'), (b'x-ratelimit-remaining-tokens', b'248975'), (b'x-ratelimit-reset-requests', b'20ms'), (b'x-ratelimit-reset-tokens', b'245ms'), (b'x-request-id', b'9365f317d2f62eee1a627158f74740e4'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'84a2df3f8c684535-ATL'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=":443"; ma=86400')])
2024-01-23 14:44:00,798 - INFO - HTTP Request: POST https://api.openai.com/v1/completions "HTTP/1.1 200 OK"
2024-01-23 14:44:00,798 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2024-01-23 14:44:00,799 - DEBUG - receive_response_body.complete
2024-01-23 14:44:00,799 - DEBUG - response_closed.started
2024-01-23 14:44:00,800 - DEBUG - response_closed.complete
2024-01-23 14:44:00,800 - DEBUG - HTTP Request: POST https://api.openai.com/v1/completions "200 OK"
2024-01-23 14:44:00,802 - INFO - Received response from OpenAI: 

@startuml
class MainClass {
- output_dir: Str
+ main()
}

MainClass --> File
File --> OutputDir
File -->+ write()
OutputDir -> Output
MainClass -> Logger
Output --> Json
Json --> Response
Response -->+ parse()
Logger -->+ info()
Logger -->+ error()
Response -->+ text()
Output --> File
MainClass -->+ createDir()
OutputDir --> File
File -->+ dump()
Json -->+ dump()
MainClass --> Exception

@enduml
2024-01-23 14:44:00,804 - DEBUG - Request options: {'method': 'post', 'url': '/completions', 'files': None, 'json_data': {'model': 'gpt-3.5-turbo-instruct', 'prompt': 'Create a .puml file that integrates the following .puml files:\n\nrrent_dir, \'config.json\')\n\n\n@startuml\ntitle UML diagrams for "execute_generate_uml" code\n\nclass json {\n    - data : any\n    + loads(string) : dict\n    + dumps(dict) : string\n}\n\nclass os {\n    + path : string\n    + makedirs(string, bool)\n    + join(string, string) : string\n}\n\nclass requests {\n    - url : string\n    + get(string) : Response\n    + Response\n}\n\nclass logging {\n    - format : string\n    - level : int\n    - handlers : list\n    - Logger\n    + DEBUG : int\n    + getLogger() : Logger\n}\n\nclass handlers {\n    - RotatingFileHandler\n}\n\nclass dotenv {\n    + load_dotenv()\n}\n\nclass execute_generate_uml {\n    - log_directory : string\n    - log_filename : string\n    - log_handler : RotatingFileHandl\n)\n\n\n@startuml\n\nrectangle "main.py" {\n    rectangle "config_file_path" as config_file_path\n    database "config.json" as config_file\n    rectangle "config_data" as config_data\n    interface "json.load()" as json_load\n    rectangle "github_token" as github_token\n    database "environment variables" as env_variables\n    rectangle "url" as url\n    rectangle "headers" as headers\n    rectangle "logging" as logging\n    rectangle "requests" as requests\n    interface "json.dumps()" as json_dumps\n    rectangle "response" as response\n\n    config_file_path --> config_file\n    config_file --( json_load\n    json_load --( config_data\n    env_variables --> github_token\n    url --> headers\n    logging --> response\n    config_data -> headers\n    github_token -> headers\n    config_data -> url\n    headers --o requests\n    json_dumps --> logging\n    requests <-- response\n\n    header --> url : POST\n    url --> response\n\n    response --> logging\n    logging --> logging : info\n}\n\n@enduml\n@startuml\nclass MainClass {\n- output_dir: Str\n+ main()\n}\n\nMainClass --> File\nFile --> OutputDir\nFile -->+ write()\nOutputDir -> Output\nMainClass -> Logger\nOutput --> Json\nJson --> Response\nResponse -->+ parse()\nLogger -->+ info()\nLogger -->+ error()\nResponse -->+ text()\nOutput --> File\nMainClass -->+ createDir()\nOutputDir --> File\nFile -->+ dump()\nJson -->+ dump()\nMainClass --> Exception\n\n@enduml', 'max_tokens': 1024}}
2024-01-23 14:44:00,806 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2024-01-23 14:44:00,806 - DEBUG - send_request_headers.complete
2024-01-23 14:44:00,806 - DEBUG - send_request_body.started request=<Request [b'POST']>
2024-01-23 14:44:00,807 - DEBUG - send_request_body.complete
2024-01-23 14:44:00,807 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2024-01-23 14:44:01,061 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Tue, 23 Jan 2024 20:44:01 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'Cache-Control', b'no-cache, must-revalidate'), (b'openai-model', b'gpt-3.5-turbo-instruct'), (b'openai-organization', b'user-lqvxa4hv7a8o8eyohtzmcoc6'), (b'openai-processing-ms', b'185'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=15724800; includeSubDomains'), (b'x-ratelimit-limit-requests', b'3000'), (b'x-ratelimit-limit-tokens', b'250000'), (b'x-ratelimit-remaining-requests', b'2999'), (b'x-ratelimit-remaining-tokens', b'248975'), (b'x-ratelimit-reset-requests', b'20ms'), (b'x-ratelimit-reset-tokens', b'245ms'), (b'x-request-id', b'f03365aab3828ac3fe821b26c829b790'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'84a2df492aef4535-ATL'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=":443"; ma=86400')])
2024-01-23 14:44:01,062 - INFO - HTTP Request: POST https://api.openai.com/v1/completions "HTTP/1.1 200 OK"
2024-01-23 14:44:01,062 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2024-01-23 14:44:01,062 - DEBUG - receive_response_body.complete
2024-01-23 14:44:01,062 - DEBUG - response_closed.started
2024-01-23 14:44:01,063 - DEBUG - response_closed.complete
2024-01-23 14:44:01,063 - DEBUG - HTTP Request: POST https://api.openai.com/v1/completions "200 OK"
2024-01-23 14:44:01,064 - INFO - UML code generated for src/scripts/execute_generate_uml.py: @startuml
title execute_generate_uml.py
execute_generate_uml -- MainClass
@enduml
2024-01-23 14:44:01,064 - INFO - Saving generated output to ../../output/execute_generate_uml.py.puml
2024-01-23 14:44:01,064 - INFO - Generated output saved to ../../output/execute_generate_uml.py.puml
2024-01-23 14:44:01,065 - INFO - Processing file: src/services/UMLGenerator.py
2024-01-23 14:44:01,065 - INFO - Sending prompt to OpenAI: Create UML diagrams in .puml format for the following code:

import json
import os
import logging
from services.openai_api import OpenAIAPI

class UMLGenerator:
    def __init__(self, config_file):
        self.api = OpenAIAPI()
        self.OUTPUT_DIRECTORY = "src/output" 
        # Load the configuration
        with open(config_file) as file:
            self.config = json.load(file)

    def generate_from_codes(self, file_list):
        uml_contents = []
        for file in file_list:
            if self._should_skip_file(file):
                continue
            logging.info(f"Generating UML for file: {file}")
            with open(file, 'r') as f:
                file_content = f.read()
            uml_content = self.api.generate_from_code(file_content, os.path.basename(file))
            if uml_content != "UML generation failed":
                uml_contents.append(uml_content)
            else:
                logging.error(f"UML generation failed for file: {file}")
        return uml_contents
 
    def save_generated_outputs(self, uml_contents, file_names):

2024-01-23 14:44:01,067 - DEBUG - Request options: {'method': 'post', 'url': '/completions', 'files': None, 'json_data': {'model': 'gpt-3.5-turbo-instruct', 'prompt': 'Create UML diagrams in .puml format for the following code:\n\nimport json\nimport os\nimport logging\nfrom services.openai_api import OpenAIAPI\n\nclass UMLGenerator:\n    def __init__(self, config_file):\n        self.api = OpenAIAPI()\n        self.OUTPUT_DIRECTORY = "src/output" \n        # Load the configuration\n        with open(config_file) as file:\n            self.config = json.load(file)\n\n    def generate_from_codes(self, file_list):\n        uml_contents = []\n        for file in file_list:\n            if self._should_skip_file(file):\n                continue\n            logging.info(f"Generating UML for file: {file}")\n            with open(file, \'r\') as f:\n                file_content = f.read()\n            uml_content = self.api.generate_from_code(file_content, os.path.basename(file))\n            if uml_content != "UML generation failed":\n                uml_contents.append(uml_content)\n            else:\n                logging.error(f"UML generation failed for file: {file}")\n        return uml_contents\n \n    def save_generated_outputs(self, uml_contents, file_names):\n', 'max_tokens': 1024}}
2024-01-23 14:44:01,068 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2024-01-23 14:44:01,068 - DEBUG - send_request_headers.complete
2024-01-23 14:44:01,068 - DEBUG - send_request_body.started request=<Request [b'POST']>
2024-01-23 14:44:01,069 - DEBUG - send_request_body.complete
2024-01-23 14:44:01,069 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2024-01-23 14:44:05,005 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Tue, 23 Jan 2024 20:44:04 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'Cache-Control', b'no-cache, must-revalidate'), (b'openai-model', b'gpt-3.5-turbo-instruct'), (b'openai-organization', b'user-lqvxa4hv7a8o8eyohtzmcoc6'), (b'openai-processing-ms', b'3693'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=15724800; includeSubDomains'), (b'x-ratelimit-limit-requests', b'3000'), (b'x-ratelimit-limit-tokens', b'250000'), (b'x-ratelimit-remaining-requests', b'2999'), (b'x-ratelimit-remaining-tokens', b'248975'), (b'x-ratelimit-reset-requests', b'20ms'), (b'x-ratelimit-reset-tokens', b'245ms'), (b'x-request-id', b'd359626fe35df46f2e79300a8e8a0dae'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'84a2df4b4e294535-ATL'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=":443"; ma=86400')])
2024-01-23 14:44:05,006 - INFO - HTTP Request: POST https://api.openai.com/v1/completions "HTTP/1.1 200 OK"
2024-01-23 14:44:05,006 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2024-01-23 14:44:05,006 - DEBUG - receive_response_body.complete
2024-01-23 14:44:05,006 - DEBUG - response_closed.started
2024-01-23 14:44:05,006 - DEBUG - response_closed.complete
2024-01-23 14:44:05,006 - DEBUG - HTTP Request: POST https://api.openai.com/v1/completions "200 OK"
2024-01-23 14:44:05,007 - INFO - Received response from OpenAI:         for uml_content, file_name in zip(uml_contents, file_names):
            output_path = os.path.join(self.OUTPUT_DIRECTORY, file_name+".puml")
            with open(output_path, 'w') as f:
                f.write(uml_content)

class OpenAIAPI:
    def generate_from_code(self, code, file_name):
        # TODO: Implement generating UML from code using OpenAI API
        return "UML generation failed"
        
 ##################################################################################
         	
        		            .puml file
                /        \         * *
- UMLGenerator  -------->|-dependency-> Json
                         |             *   *
                         |             I
                         |          Config File
                         |             *   *
                         \            * *       
                           OpenAIAPI
                        __________|
                       |           |
           File List---|         file name
        .->*Should Skip File*[---/
        |           |
        |     .>File*.r/---
        |     |      |       *
        * ---^      |-dependency-> Log
        |           |            *
        |     |<-info >|-Log.info
        |     |       |            *
        * ---^      |-file_content> OpenAIAPI
        |           |           |
        |     |<---generate-|  code
        |     |              /     |
        |     |<-File Content  |
        * ---^            \ UML Content
        |           |          |
        |           |        "UML"
        |     |<-Content failed |-----/
        * ---^

 Class     Variable/Parameter
Variable
       ,

Function     Function/Method
2024-01-23 14:44:05,007 - INFO - Sending prompt to OpenAI: Create UML diagrams in .puml format for the following code:

        for uml_content, file_name in zip(uml_contents, file_names):
            logging.info(f"Saving UML diagram for file: {file_name}")
            self.api.save_generated_output(uml_content, file_name)

    def _should_skip_file(self, file):
        python_files_only = self.config.get('python_files_only', False)
        ignore_files = self.config.get('ignore_files', [])
        ignore_extensions = self.config.get('ignore_extensions', [])
        return (file in ignore_files or 
                (python_files_only and not file.endswith('.py')) or 
                any(file.endswith(ext) for ext in ignore_extensions))

    def _generate_from_code(self, file):
        logging.info(f"Generating UML diagram for file: {file}")
        return self.api.generate_from_code(file)
2024-01-23 14:44:05,008 - DEBUG - Request options: {'method': 'post', 'url': '/completions', 'files': None, 'json_data': {'model': 'gpt-3.5-turbo-instruct', 'prompt': 'Create UML diagrams in .puml format for the following code:\n\n        for uml_content, file_name in zip(uml_contents, file_names):\n            logging.info(f"Saving UML diagram for file: {file_name}")\n            self.api.save_generated_output(uml_content, file_name)\n\n    def _should_skip_file(self, file):\n        python_files_only = self.config.get(\'python_files_only\', False)\n        ignore_files = self.config.get(\'ignore_files\', [])\n        ignore_extensions = self.config.get(\'ignore_extensions\', [])\n        return (file in ignore_files or \n                (python_files_only and not file.endswith(\'.py\')) or \n                any(file.endswith(ext) for ext in ignore_extensions))\n\n    def _generate_from_code(self, file):\n        logging.info(f"Generating UML diagram for file: {file}")\n        return self.api.generate_from_code(file)', 'max_tokens': 1024}}
2024-01-23 14:44:05,009 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2024-01-23 14:44:05,009 - DEBUG - send_request_headers.complete
2024-01-23 14:44:05,009 - DEBUG - send_request_body.started request=<Request [b'POST']>
2024-01-23 14:44:05,009 - DEBUG - send_request_body.complete
2024-01-23 14:44:05,010 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2024-01-23 14:44:07,599 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Tue, 23 Jan 2024 20:44:07 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'Cache-Control', b'no-cache, must-revalidate'), (b'openai-model', b'gpt-3.5-turbo-instruct'), (b'openai-organization', b'user-lqvxa4hv7a8o8eyohtzmcoc6'), (b'openai-processing-ms', b'2150'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=15724800; includeSubDomains'), (b'x-ratelimit-limit-requests', b'3000'), (b'x-ratelimit-limit-tokens', b'250000'), (b'x-ratelimit-remaining-requests', b'2999'), (b'x-ratelimit-remaining-tokens', b'248975'), (b'x-ratelimit-reset-requests', b'20ms'), (b'x-ratelimit-reset-tokens', b'245ms'), (b'x-request-id', b'2235e8c0612e5baadb9260cba5854e06'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'84a2df636a3e4535-ATL'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=":443"; ma=86400')])
2024-01-23 14:44:07,601 - INFO - HTTP Request: POST https://api.openai.com/v1/completions "HTTP/1.1 200 OK"
2024-01-23 14:44:07,602 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2024-01-23 14:44:07,603 - DEBUG - receive_response_body.complete
2024-01-23 14:44:07,603 - DEBUG - response_closed.started
2024-01-23 14:44:07,604 - DEBUG - response_closed.complete
2024-01-23 14:44:07,604 - DEBUG - HTTP Request: POST https://api.openai.com/v1/completions "200 OK"
2024-01-23 14:44:07,605 - INFO - Received response from OpenAI: 
@startuml
class UMLDiagramGenerator {
    - uml_contents : list
    - file_names : list
    - config : dict
    + save_uml_diagrams() : void
    + skip_file() : boolean
    + generate_uml_from_code() : void
}
UMLDiagramGenerator ..> UMLAPI : uses

class UMLAPI {
    + save_generated_output(uml_content, file_name) : void
    + generate_from_code(file) : void
}

UMLDiagramGenerator --> "*" UMLDiagram : contains

class UMLDiagram {
    - file_name : string
    - uml_content : string
}

UMLDiagramGenerator --> "*" File : contains

class File {
    - file_name : string
}

UMLDiagramGenerator -> Logger : log

class Logger {
    + info(message) : void
}

@enduml
2024-01-23 14:44:07,608 - DEBUG - Request options: {'method': 'post', 'url': '/completions', 'files': None, 'json_data': {'model': 'gpt-3.5-turbo-instruct', 'prompt': 'Create a .puml file that integrates the following .puml files:\n\nfor uml_content, file_name in zip(uml_contents, file_names):\n            output_path = os.path.join(self.OUTPUT_DIRECTORY, file_name+".puml")\n            with open(output_path, \'w\') as f:\n                f.write(uml_content)\n\nclass OpenAIAPI:\n    def generate_from_code(self, code, file_name):\n        # TODO: Implement generating UML from code using OpenAI API\n        return "UML generation failed"\n        \n ##################################################################################\n         \t\n        \t\t            .puml file\n                /        \\         * *\n- UMLGenerator  -------->|-dependency-> Json\n                         |             *   *\n                         |             I\n                         |          Config File\n                         |             *   *\n                         \\            * *       \n                           OpenAIAPI\n                        __________|\n                       |           |\n           File List---|         file name\n        .->*Should Skip File*[---/\n        |           |\n        |     .>File*.r/---\n        |     |      |       *\n        * ---^      |-dependency-> Log\n        |           |            *\n        |     |<-info >|-Log.info\n        |     |       |            *\n        * ---^      |-file_content> OpenAIAPI\n        |           |           |\n        |     |<---generate-|  code\n        |     |              /     |\n        |     |<-File Content  |\n        * ---^            \\ UML Content\n        |           |          |\n        |           |        "UML"\n        |     |<-Content failed |-----/\n        * ---^\n\n Class     Variable/Parameter\nVariable\n       ,\n\nFunction     Function/Method\n@startuml\nclass UMLDiagramGenerator {\n    - uml_contents : list\n    - file_names : list\n    - config : dict\n    + save_uml_diagrams() : void\n    + skip_file() : boolean\n    + generate_uml_from_code() : void\n}\nUMLDiagramGenerator ..> UMLAPI : uses\n\nclass UMLAPI {\n    + save_generated_output(uml_content, file_name) : void\n    + generate_from_code(file) : void\n}\n\nUMLDiagramGenerator --> "*" UMLDiagram : contains\n\nclass UMLDiagram {\n    - file_name : string\n    - uml_content : string\n}\n\nUMLDiagramGenerator --> "*" File : contains\n\nclass File {\n    - file_name : string\n}\n\nUMLDiagramGenerator -> Logger : log\n\nclass Logger {\n    + info(message) : void\n}\n\n@enduml', 'max_tokens': 1024}}
2024-01-23 14:44:07,609 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2024-01-23 14:44:07,609 - DEBUG - send_request_headers.complete
2024-01-23 14:44:07,610 - DEBUG - send_request_body.started request=<Request [b'POST']>
2024-01-23 14:44:07,610 - DEBUG - send_request_body.complete
2024-01-23 14:44:07,610 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2024-01-23 14:44:07,745 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Tue, 23 Jan 2024 20:44:07 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'Cache-Control', b'no-cache, must-revalidate'), (b'openai-model', b'gpt-3.5-turbo-instruct'), (b'openai-organization', b'user-lqvxa4hv7a8o8eyohtzmcoc6'), (b'openai-processing-ms', b'73'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=15724800; includeSubDomains'), (b'x-ratelimit-limit-requests', b'3000'), (b'x-ratelimit-limit-tokens', b'250000'), (b'x-ratelimit-remaining-requests', b'2999'), (b'x-ratelimit-remaining-tokens', b'248975'), (b'x-ratelimit-reset-requests', b'20ms'), (b'x-ratelimit-reset-tokens', b'245ms'), (b'x-request-id', b'1376166e36b9be35a900d23757c6076b'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'84a2df73aa654535-ATL'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=":443"; ma=86400')])
2024-01-23 14:44:07,747 - INFO - HTTP Request: POST https://api.openai.com/v1/completions "HTTP/1.1 200 OK"
2024-01-23 14:44:07,749 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2024-01-23 14:44:07,750 - DEBUG - receive_response_body.complete
2024-01-23 14:44:07,750 - DEBUG - response_closed.started
2024-01-23 14:44:07,751 - DEBUG - response_closed.complete
2024-01-23 14:44:07,752 - DEBUG - HTTP Request: POST https://api.openai.com/v1/completions "200 OK"
2024-01-23 14:44:07,753 - INFO - UML code generated for src/services/UMLGenerator.py: @startuml
title UMLGenerator.py

@enduml
2024-01-23 14:44:07,754 - INFO - Saving generated output to ../../output/UMLGenerator.py.puml
2024-01-23 14:44:07,754 - INFO - Generated output saved to ../../output/UMLGenerator.py.puml
2024-01-23 14:44:07,755 - INFO - Skipping empty file: src/services/__init__.py
2024-01-23 14:44:07,755 - INFO - Processing file: src/services/openai_api.py
2024-01-23 14:44:07,755 - INFO - Sending prompt to OpenAI: Create UML diagrams in .puml format for the following code:

import json
import os
import logging
from logging import handlers
from openai import OpenAI, OpenAIError
from dotenv import load_dotenv
from models.uml_diagram import UMLDiagram
from models.git_repo import GitRepo  # Import GitRepo


# Load environment variables from .env file
load_dotenv()

# Configure logging to write to a file
log_directory = 'logs'
# Create the directory if it doesn't exist
os.makedirs(log_directory, exist_ok=True)  
log_filename = os.path.join(log_directory, 'openai_api.log')
log_handler = handlers.RotatingFileHandler(log_filename, maxBytes=1024*1024, backupCount=5)  # Log file with rotation
log_formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')
log_handler.setFormatter(log_formatter)
logger = logging.getLogger()
logger.addHandler(log_handler)
logger.setLevel(logging.DEBUG)


class OpenAIAPI:
    def __init__(self):
        # Load environment variables from .env file
        load_dotenv()

        # Retrieve the OpenAI API key from the environment
        openai_a
2024-01-23 14:44:07,757 - DEBUG - Request options: {'method': 'post', 'url': '/completions', 'files': None, 'json_data': {'model': 'gpt-3.5-turbo-instruct', 'prompt': "Create UML diagrams in .puml format for the following code:\n\nimport json\nimport os\nimport logging\nfrom logging import handlers\nfrom openai import OpenAI, OpenAIError\nfrom dotenv import load_dotenv\nfrom models.uml_diagram import UMLDiagram\nfrom models.git_repo import GitRepo  # Import GitRepo\n\n\n# Load environment variables from .env file\nload_dotenv()\n\n# Configure logging to write to a file\nlog_directory = 'logs'\n# Create the directory if it doesn't exist\nos.makedirs(log_directory, exist_ok=True)  \nlog_filename = os.path.join(log_directory, 'openai_api.log')\nlog_handler = handlers.RotatingFileHandler(log_filename, maxBytes=1024*1024, backupCount=5)  # Log file with rotation\nlog_formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\nlog_handler.setFormatter(log_formatter)\nlogger = logging.getLogger()\nlogger.addHandler(log_handler)\nlogger.setLevel(logging.DEBUG)\n\n\nclass OpenAIAPI:\n    def __init__(self):\n        # Load environment variables from .env file\n        load_dotenv()\n\n        # Retrieve the OpenAI API key from the environment\n        openai_a", 'max_tokens': 1024}}
2024-01-23 14:44:07,758 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2024-01-23 14:44:07,758 - DEBUG - send_request_headers.complete
2024-01-23 14:44:07,758 - DEBUG - send_request_body.started request=<Request [b'POST']>
2024-01-23 14:44:07,759 - DEBUG - send_request_body.complete
2024-01-23 14:44:07,759 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2024-01-23 14:44:13,448 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Tue, 23 Jan 2024 20:44:13 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'Cache-Control', b'no-cache, must-revalidate'), (b'openai-model', b'gpt-3.5-turbo-instruct'), (b'openai-organization', b'user-lqvxa4hv7a8o8eyohtzmcoc6'), (b'openai-processing-ms', b'5623'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=15724800; includeSubDomains'), (b'x-ratelimit-limit-requests', b'3000'), (b'x-ratelimit-limit-tokens', b'250000'), (b'x-ratelimit-remaining-requests', b'2999'), (b'x-ratelimit-remaining-tokens', b'248911'), (b'x-ratelimit-reset-requests', b'20ms'), (b'x-ratelimit-reset-tokens', b'261ms'), (b'x-request-id', b'dd8149d28c2f572708730ca3e6803131'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'84a2df749bd24535-ATL'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=":443"; ma=86400')])
2024-01-23 14:44:13,448 - INFO - HTTP Request: POST https://api.openai.com/v1/completions "HTTP/1.1 200 OK"
2024-01-23 14:44:13,449 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2024-01-23 14:44:13,449 - DEBUG - receive_response_body.complete
2024-01-23 14:44:13,449 - DEBUG - response_closed.started
2024-01-23 14:44:13,449 - DEBUG - response_closed.complete
2024-01-23 14:44:13,449 - DEBUG - HTTP Request: POST https://api.openai.com/v1/completions "200 OK"
2024-01-23 14:44:13,450 - INFO - Received response from OpenAI: pi_key = os.getenv('OPENAI_API_KEY')

        # Initialize the OpenAI client with the API key
        self.client = OpenAI(openai_api_key)

    # Function to generate text using OpenAI's completion endpoint
    def generate_text(self, prompt):
        try:
            # Make the request to the API using the prompt text
            response = self.client.completion.create(engine="davinci", prompt=prompt, max_tokens=100)

            # Return the generated text
            return response['choices'][0]['text']
        except OpenAIError as e:
            # Log any error that may occur
            logging.error(e)

# Define a class for handling Git repositories
class GitRepoManager:
    def __init__(self):
        # Load environment variables from .env file
        load_dotenv()
        
        # Retrieve the GitHub API key from the environment
        github_api_key = os.getenv('GITHUB_API_KEY')

        # Initialize the GitRepo object with the API key
        self.repo = GitRepo(github_api_key)

    # Function to clone a repository
    def clone_repo(self, repo_url):
        try:
            # Clone the repository using the GitRepo object
            self.repo.clone(repo_url)
        except GitRepoError as e:
            # Log any error that may occur
            logging.error(e)

# Create diagram
@startuml

json <|-- OpenAI
OpenAIError <|-- OpenAI
openapiapi <|-- OpenAIAPI

json: +loads()
json: +dumps()
OpenAI: +client
OpenAI: +generate_text(prompt)
OpenAI: +create(engine, prompt, max_tokens)
OpenAIError: +message

note left : loads() and dumps() functions \nfrom the json library
note right : OpenAI library for interacting \nwith the OpenAI API
note right : Initialize with API key\nand generate text using prompt

OpenAIAPI <-- UMLDiagram
UMLDiagram: +create_diagram() \n constructs diagram
GitRepo <-- GitRepoManager
GitRepo: +clone(repo_url)
GitRepoError <|-- GitRepo

note left : UMLDiagram class for creating and\nexporting UML diagrams in .puml format
note right : GitHub API library for \ninteracting with GitHub repositories
note right : Clone GitHub repository\nusing GitRepo object initialized \nwith API key

@enduml

 
2024-01-23 14:44:13,451 - INFO - Sending prompt to OpenAI: Create UML diagrams in .puml format for the following code:

pi_key = os.getenv("OPENAI_API_KEY")

        self.client = OpenAI(api_key=openai_api_key)

        # Constants
        self.MODEL_NAME = "gpt-3.5-turbo-instruct"
        self.MAX_TOKENS = 1024
        self.OUTPUT_DIRECTORY = "src/output"

    def generate_uml_diagram(self, repo_url, file_path, title):
        # Clone the repository and retrieve the code
        git_repo = GitRepo(repo_url)
        git_repo.clone()
        code = git_repo.retrieve_code(file_path)

        uml_diagram = UMLDiagram(code, title)
        uml_diagram.generate(self)
        return uml_diagram

    def save_uml_diagram(self, uml_diagram, file_path):
        uml_diagram.save(self, file_path)

    def write_response_to_file(self, response, filename):
        output_dir = "output/openai"
        os.makedirs(output_dir, exist_ok=True)  # Create the directory if it doesn't exist
        with open(os.path.join(output_dir, f"{filename}.txt"), 'w') as f:
            f.write(response)

    def generate_from_code(self, code, title):
        #
2024-01-23 14:44:13,452 - DEBUG - Request options: {'method': 'post', 'url': '/completions', 'files': None, 'json_data': {'model': 'gpt-3.5-turbo-instruct', 'prompt': 'Create UML diagrams in .puml format for the following code:\n\npi_key = os.getenv("OPENAI_API_KEY")\n\n        self.client = OpenAI(api_key=openai_api_key)\n\n        # Constants\n        self.MODEL_NAME = "gpt-3.5-turbo-instruct"\n        self.MAX_TOKENS = 1024\n        self.OUTPUT_DIRECTORY = "src/output"\n\n    def generate_uml_diagram(self, repo_url, file_path, title):\n        # Clone the repository and retrieve the code\n        git_repo = GitRepo(repo_url)\n        git_repo.clone()\n        code = git_repo.retrieve_code(file_path)\n\n        uml_diagram = UMLDiagram(code, title)\n        uml_diagram.generate(self)\n        return uml_diagram\n\n    def save_uml_diagram(self, uml_diagram, file_path):\n        uml_diagram.save(self, file_path)\n\n    def write_response_to_file(self, response, filename):\n        output_dir = "output/openai"\n        os.makedirs(output_dir, exist_ok=True)  # Create the directory if it doesn\'t exist\n        with open(os.path.join(output_dir, f"{filename}.txt"), \'w\') as f:\n            f.write(response)\n\n    def generate_from_code(self, code, title):\n        #', 'max_tokens': 1024}}
2024-01-23 14:44:13,452 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2024-01-23 14:44:13,453 - DEBUG - send_request_headers.complete
2024-01-23 14:44:13,453 - DEBUG - send_request_body.started request=<Request [b'POST']>
2024-01-23 14:44:13,453 - DEBUG - send_request_body.complete
2024-01-23 14:44:13,453 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2024-01-23 14:44:14,924 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Tue, 23 Jan 2024 20:44:14 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'Cache-Control', b'no-cache, must-revalidate'), (b'openai-model', b'gpt-3.5-turbo-instruct'), (b'openai-organization', b'user-lqvxa4hv7a8o8eyohtzmcoc6'), (b'openai-processing-ms', b'881'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=15724800; includeSubDomains'), (b'x-ratelimit-limit-requests', b'3000'), (b'x-ratelimit-limit-tokens', b'250000'), (b'x-ratelimit-remaining-requests', b'2999'), (b'x-ratelimit-remaining-tokens', b'248975'), (b'x-ratelimit-reset-requests', b'20ms'), (b'x-ratelimit-reset-tokens', b'245ms'), (b'x-request-id', b'ae1d3a8f17da21e1a6c3e3f302805cb2'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'84a2df982d854535-ATL'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=":443"; ma=86400')])
2024-01-23 14:44:14,924 - INFO - HTTP Request: POST https://api.openai.com/v1/completions "HTTP/1.1 200 OK"
2024-01-23 14:44:14,924 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2024-01-23 14:44:14,925 - DEBUG - receive_response_body.complete
2024-01-23 14:44:14,925 - DEBUG - response_closed.started
2024-01-23 14:44:14,925 - DEBUG - response_closed.complete
2024-01-23 14:44:14,925 - DEBUG - HTTP Request: POST https://api.openai.com/v1/completions "200 OK"
2024-01-23 14:44:14,926 - INFO - Received response from OpenAI:  Generate instructions from given code
        response = self.client.complete(
            engine="gpt-3",
            prompt=f"Given the following code: \n{code}, write instructions for {title}",
            max_tokens=self.MAX_TOKENS,
            temperature=0.1,
            top_p=1,
            stop=["Instructions:", "
2024-01-23 14:44:14,926 - INFO - Sending prompt to OpenAI: Create UML diagrams in .puml format for the following code:

 Split the code into chunks of MAX_TOKENS
        code_chunks = [code[i:i+self.MAX_TOKENS] for i in range(0, len(code), self.MAX_TOKENS)]

        puml_chunks = []
        for chunk in code_chunks:
            prompt_text = f"Create UML diagrams in .puml format for the following code:\n\n{chunk}"
            logging.info(f"Sending prompt to OpenAI: {prompt_text}")  # Log the prompt text
            try:
                response = self.client.completions.create(
                    model=self.MODEL_NAME,
                    prompt=prompt_text,
                    max_tokens=self.MAX_TOKENS)
                
                # Log the response from OpenAI
                logging.info(f"Received response from OpenAI: {response.choices[0].text}")  # Log the response from OpenAI

                # Write the response to a .txt file
                self.write_response_to_file(response.choices[0].text, title)

                # Append the generated text to the UML code
                puml_chunks.append(response.choic
2024-01-23 14:44:14,927 - DEBUG - Request options: {'method': 'post', 'url': '/completions', 'files': None, 'json_data': {'model': 'gpt-3.5-turbo-instruct', 'prompt': 'Create UML diagrams in .puml format for the following code:\n\n Split the code into chunks of MAX_TOKENS\n        code_chunks = [code[i:i+self.MAX_TOKENS] for i in range(0, len(code), self.MAX_TOKENS)]\n\n        puml_chunks = []\n        for chunk in code_chunks:\n            prompt_text = f"Create UML diagrams in .puml format for the following code:\\n\\n{chunk}"\n            logging.info(f"Sending prompt to OpenAI: {prompt_text}")  # Log the prompt text\n            try:\n                response = self.client.completions.create(\n                    model=self.MODEL_NAME,\n                    prompt=prompt_text,\n                    max_tokens=self.MAX_TOKENS)\n                \n                # Log the response from OpenAI\n                logging.info(f"Received response from OpenAI: {response.choices[0].text}")  # Log the response from OpenAI\n\n                # Write the response to a .txt file\n                self.write_response_to_file(response.choices[0].text, title)\n\n                # Append the generated text to the UML code\n                puml_chunks.append(response.choic', 'max_tokens': 1024}}
2024-01-23 14:44:14,928 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2024-01-23 14:44:14,928 - DEBUG - send_request_headers.complete
2024-01-23 14:44:14,928 - DEBUG - send_request_body.started request=<Request [b'POST']>
2024-01-23 14:44:14,928 - DEBUG - send_request_body.complete
2024-01-23 14:44:14,928 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2024-01-23 14:44:21,274 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Tue, 23 Jan 2024 20:44:20 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'Cache-Control', b'no-cache, must-revalidate'), (b'openai-model', b'gpt-3.5-turbo-instruct'), (b'openai-organization', b'user-lqvxa4hv7a8o8eyohtzmcoc6'), (b'openai-processing-ms', b'5728'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=15724800; includeSubDomains'), (b'x-ratelimit-limit-requests', b'3000'), (b'x-ratelimit-limit-tokens', b'250000'), (b'x-ratelimit-remaining-requests', b'2999'), (b'x-ratelimit-remaining-tokens', b'248975'), (b'x-ratelimit-reset-requests', b'20ms'), (b'x-ratelimit-reset-tokens', b'245ms'), (b'x-request-id', b'a3e9d2b003c64f74e74ef84fdc197316'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'84a2dfa189514535-ATL'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=":443"; ma=86400')])
2024-01-23 14:44:21,276 - INFO - HTTP Request: POST https://api.openai.com/v1/completions "HTTP/1.1 200 OK"
2024-01-23 14:44:21,277 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2024-01-23 14:44:21,278 - DEBUG - receive_response_body.complete
2024-01-23 14:44:21,278 - DEBUG - response_closed.started
2024-01-23 14:44:21,279 - DEBUG - response_closed.complete
2024-01-23 14:44:21,279 - DEBUG - HTTP Request: POST https://api.openai.com/v1/completions "200 OK"
2024-01-23 14:44:21,281 - INFO - Received response from OpenAI: es[0].text)

            except BaseException as e:
                logging.exception(f"Error while generating text: {e}")


//Define class GenerateUML
class GenerateUML:
    - MAX_TOKENS: int
    - MODEL_NAME: string
    - client: OpenAI
    + code_chunks: list[string]
    + puml_chunks: list[string]

    //Constructor
    + GenerateUML(MAX_TOKENS: int, MODEL_NAME: string)

    //Methods
    + SplitCodeIntoChunks(code: string): code_chunks: list[string]
    + SendPromptToOpenAI(prompt_text: string): response: string
    + LogPromptText(prompt_text: string)
    + LogResponseFromOpenAI(response: string)
    + WriteResponseToFile(response: string, title: string)
    + AppendPumlChunks(response: string)
    + GenerateUMLChunks(code: string, title: string)

// Define class Main
class Main {
    - main()

    //Methods
    + process_code(code: string)
    + create_uml_chunks(code: string)
    + save_puml_chunks(puml_chunks: list[string])
}

// GenerateUML "1" *-- "1" Main: process_code()
// GenerateUML "1" *-- "1" Main: create_uml_chunks()
// Main *-- "1" GenerateUML: save_puml_chunks()

// GenerateUML "1" *-- "1" OpenAI: SendPromptToOpenAI()
// GenerateUML "1" *-- "1" OpenAI: LogResponseFromOpenAI()

// GenerateUML "1" *-- "1" Log: LogPromptText(prompt_text: string)
// GenerateUML "1" *-- "1" Log: WriteResponseToFile(response: string, title: string)

// Main "1" *-- "1" Log: LogPromptText(prompt_text: string)
// Main "1" *-- "1" Log: LogResponseFromOpenAI(response: string)
// Main "1" *-- "1" Log: WriteResponseToFile(response: string, title: string)

// GenerateUML "1" o-- "1" code: string
// GenerateUML "1" o-- "1" title: string
// Main "1" o-- "1" code: string
// Main "1" o-- "1" code_chunks: list[string]
// Main "1" o-- "1" puml_chunks: list[string]
2024-01-23 14:44:21,282 - INFO - Sending prompt to OpenAI: Create UML diagrams in .puml format for the following code:

es[0].text.strip())
            except OpenAIError as e:
                logging.error(f"An error occurred while sending the prompt: {e}")  # Log the error message
                return f"An error occurred: {e}"  # Return a message if an OpenAI API error occurs

        # Combine the .puml chunks into a single string
        combined_puml = "\n".join(puml_chunks)

        # Send a final request to OpenAI to integrate the .puml chunks
        prompt_text = f"Create a .puml file that integrates the following .puml files:\n\n{combined_puml}"
        response = self.client.completions.create(
            model=self.MODEL_NAME,
            prompt=prompt_text,
            max_tokens=self.MAX_TOKENS)

        # Extract the integrated .puml code from the response
        generated_code = response.choices[0].text.strip()

        # Add the @startuml, title and @enduml tags only once for each UML diagram
        generated_code = f"@startuml\n" + f"title {title}\n" + generated_code + "\n@enduml\n"
        return genera
2024-01-23 14:44:21,285 - DEBUG - Request options: {'method': 'post', 'url': '/completions', 'files': None, 'json_data': {'model': 'gpt-3.5-turbo-instruct', 'prompt': 'Create UML diagrams in .puml format for the following code:\n\nes[0].text.strip())\n            except OpenAIError as e:\n                logging.error(f"An error occurred while sending the prompt: {e}")  # Log the error message\n                return f"An error occurred: {e}"  # Return a message if an OpenAI API error occurs\n\n        # Combine the .puml chunks into a single string\n        combined_puml = "\\n".join(puml_chunks)\n\n        # Send a final request to OpenAI to integrate the .puml chunks\n        prompt_text = f"Create a .puml file that integrates the following .puml files:\\n\\n{combined_puml}"\n        response = self.client.completions.create(\n            model=self.MODEL_NAME,\n            prompt=prompt_text,\n            max_tokens=self.MAX_TOKENS)\n\n        # Extract the integrated .puml code from the response\n        generated_code = response.choices[0].text.strip()\n\n        # Add the @startuml, title and @enduml tags only once for each UML diagram\n        generated_code = f"@startuml\\n" + f"title {title}\\n" + generated_code + "\\n@enduml\\n"\n        return genera', 'max_tokens': 1024}}
2024-01-23 14:44:21,286 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2024-01-23 14:44:21,287 - DEBUG - send_request_headers.complete
2024-01-23 14:44:21,287 - DEBUG - send_request_body.started request=<Request [b'POST']>
2024-01-23 14:44:21,287 - DEBUG - send_request_body.complete
2024-01-23 14:44:21,287 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2024-01-23 14:44:26,621 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Tue, 23 Jan 2024 20:44:26 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'Cache-Control', b'no-cache, must-revalidate'), (b'openai-model', b'gpt-3.5-turbo-instruct'), (b'openai-organization', b'user-lqvxa4hv7a8o8eyohtzmcoc6'), (b'openai-processing-ms', b'5088'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=15724800; includeSubDomains'), (b'x-ratelimit-limit-requests', b'3000'), (b'x-ratelimit-limit-tokens', b'250000'), (b'x-ratelimit-remaining-requests', b'2999'), (b'x-ratelimit-remaining-tokens', b'248975'), (b'x-ratelimit-reset-requests', b'20ms'), (b'x-ratelimit-reset-tokens', b'245ms'), (b'x-request-id', b'aa5a2e9a9b265bad238a8fdb267c785b'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'84a2dfc93da74535-ATL'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=":443"; ma=86400')])
2024-01-23 14:44:26,621 - INFO - HTTP Request: POST https://api.openai.com/v1/completions "HTTP/1.1 200 OK"
2024-01-23 14:44:26,621 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2024-01-23 14:44:26,622 - DEBUG - receive_response_body.complete
2024-01-23 14:44:26,622 - DEBUG - response_closed.started
2024-01-23 14:44:26,622 - DEBUG - response_closed.complete
2024-01-23 14:44:26,622 - DEBUG - HTTP Request: POST https://api.openai.com/v1/completions "200 OK"
2024-01-23 14:44:26,623 - INFO - Received response from OpenAI: ted_code

        class OpenAIError(Exception):
            pass

@startuml
class UMLGenerator {
    - MODEL_NAME: str
    - MAX_TOKENS: int
    - client: OpenAIClient
    + generate_diagram(puml_files: list, title: str): str
    - combine_puml_chunks(puml_chunks: list): str
    - send_request_to_openai(text: str): str
}
class OpenAIClient {
    # API_KEY: str
    - base_url: str
    + completions: OpenAICompletionsResource
    - _handle_errors(response: HttpResponse): None
    + send_request(method: str, path: str, data: dict = None, headers: dict = None): HttpResponse
}
class OpenAICompletionsResource {
    - model: str
    - prompt: str
    - max_tokens: int
    + create(): OpenAICompletionsResponse
}
class OpenAICompletionsResponse {
    - choices: list
    + get_choice(index: int): OpenAICompletionChoice
}
class OpenAICompletionChoice {
    - text: str
    - index: int
    - finish_reason: str
    - logprobs: dict
    - selected_tokens: list
}
UMLGenerator --> OpenAIClient
OpenAIClient <-- OpenAICompletionsResource
OpenAICompletionsResource <-- OpenAICompletionsResponse
UMLGenerator --> OpenAICompletionsResource
class OpenAIError << (E,red) Exception >> #FF7700 {
    + _error_message: str
}
UMLGenerator --> OpenAIError : raise
UMLGenerator : MODEL_NAME
UMLGenerator : MAX_TOKENS
UMLGenerator : client
OpenAIClient : API_KEY
OpenAIClient : base_url
OpenAIClient : _handle_errors
OpenAIClient : send_request
OpenAICompletionsResource : model
OpenAICompletionsResource : prompt
OpenAICompletionsResource : max_tokens
OpenAICompletionsResponse : choices
OpenAICompletionChoice : text
OpenAICompletionChoice : index
OpenAICompletionChoice : finish_reason
OpenAICompletionChoice : logprobs
OpenAICompletionChoice : selected_tokens
@enduml
2024-01-23 14:44:26,623 - INFO - Sending prompt to OpenAI: Create UML diagrams in .puml format for the following code:

ted_code
    

    def save_generated_output(self, generated_code, file_path):
        # Create the directory if it does not exist
        logging.info(f"Saving generated output to {file_path}")
        os.makedirs(os.path.dirname(file_path), exist_ok=True)
        
        # Write the UML code to the file
        with open(file_path, 'w') as file:
            file.write(generated_code)
            logging.info(f"Generated output saved to {file_path}")  # Optional: print out the path where the file was saved

        return file_path   
2024-01-23 14:44:26,624 - DEBUG - Request options: {'method': 'post', 'url': '/completions', 'files': None, 'json_data': {'model': 'gpt-3.5-turbo-instruct', 'prompt': 'Create UML diagrams in .puml format for the following code:\n\nted_code\n    \n\n    def save_generated_output(self, generated_code, file_path):\n        # Create the directory if it does not exist\n        logging.info(f"Saving generated output to {file_path}")\n        os.makedirs(os.path.dirname(file_path), exist_ok=True)\n        \n        # Write the UML code to the file\n        with open(file_path, \'w\') as file:\n            file.write(generated_code)\n            logging.info(f"Generated output saved to {file_path}")  # Optional: print out the path where the file was saved\n\n        return file_path   ', 'max_tokens': 1024}}
2024-01-23 14:44:26,625 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2024-01-23 14:44:26,625 - DEBUG - send_request_headers.complete
2024-01-23 14:44:26,626 - DEBUG - send_request_body.started request=<Request [b'POST']>
2024-01-23 14:44:26,626 - DEBUG - send_request_body.complete
2024-01-23 14:44:26,626 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2024-01-23 14:44:28,052 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Tue, 23 Jan 2024 20:44:28 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'Cache-Control', b'no-cache, must-revalidate'), (b'openai-model', b'gpt-3.5-turbo-instruct'), (b'openai-organization', b'user-lqvxa4hv7a8o8eyohtzmcoc6'), (b'openai-processing-ms', b'1350'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=15724800; includeSubDomains'), (b'x-ratelimit-limit-requests', b'3000'), (b'x-ratelimit-limit-tokens', b'250000'), (b'x-ratelimit-remaining-requests', b'2999'), (b'x-ratelimit-remaining-tokens', b'248975'), (b'x-ratelimit-reset-requests', b'20ms'), (b'x-ratelimit-reset-tokens', b'245ms'), (b'x-request-id', b'9c2c0ed7124d3cd022dad4d1a55b174f'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'84a2dfea8c534535-ATL'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=":443"; ma=86400')])
2024-01-23 14:44:28,053 - INFO - HTTP Request: POST https://api.openai.com/v1/completions "HTTP/1.1 200 OK"
2024-01-23 14:44:28,054 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2024-01-23 14:44:28,055 - DEBUG - receive_response_body.complete
2024-01-23 14:44:28,055 - DEBUG - response_closed.started
2024-01-23 14:44:28,055 - DEBUG - response_closed.complete
2024-01-23 14:44:28,056 - DEBUG - HTTP Request: POST https://api.openai.com/v1/completions "200 OK"
2024-01-23 14:44:28,057 - INFO - Received response from OpenAI: 


Activity Diagram:
@startuml
start
:Create directory if it doesn't exist;
if (Directory exists?) then (yes)
    :Write UML code to file;
else (no)
    :Create directory;
    :Write UML code to file;
endif
:Print path of file; 
:Return file path;
stop
@enduml

Class Diagram:
@startuml
class Ted_code {
    - generated_code : String
    - file_path : String
    + save_generated_output(generated_code, file_path)
}

Ted_code "1" -- "1" FileIO
@enduml
2024-01-23 14:44:28,058 - DEBUG - Request options: {'method': 'post', 'url': '/completions', 'files': None, 'json_data': {'model': 'gpt-3.5-turbo-instruct', 'prompt': 'Create a .puml file that integrates the following .puml files:\n\npi_key = os.getenv(\'OPENAI_API_KEY\')\n\n        # Initialize the OpenAI client with the API key\n        self.client = OpenAI(openai_api_key)\n\n    # Function to generate text using OpenAI\'s completion endpoint\n    def generate_text(self, prompt):\n        try:\n            # Make the request to the API using the prompt text\n            response = self.client.completion.create(engine="davinci", prompt=prompt, max_tokens=100)\n\n            # Return the generated text\n            return response[\'choices\'][0][\'text\']\n        except OpenAIError as e:\n            # Log any error that may occur\n            logging.error(e)\n\n# Define a class for handling Git repositories\nclass GitRepoManager:\n    def __init__(self):\n        # Load environment variables from .env file\n        load_dotenv()\n        \n        # Retrieve the GitHub API key from the environment\n        github_api_key = os.getenv(\'GITHUB_API_KEY\')\n\n        # Initialize the GitRepo object with the API key\n        self.repo = GitRepo(github_api_key)\n\n    # Function to clone a repository\n    def clone_repo(self, repo_url):\n        try:\n            # Clone the repository using the GitRepo object\n            self.repo.clone(repo_url)\n        except GitRepoError as e:\n            # Log any error that may occur\n            logging.error(e)\n\n# Create diagram\n@startuml\n\njson <|-- OpenAI\nOpenAIError <|-- OpenAI\nopenapiapi <|-- OpenAIAPI\n\njson: +loads()\njson: +dumps()\nOpenAI: +client\nOpenAI: +generate_text(prompt)\nOpenAI: +create(engine, prompt, max_tokens)\nOpenAIError: +message\n\nnote left : loads() and dumps() functions \\nfrom the json library\nnote right : OpenAI library for interacting \\nwith the OpenAI API\nnote right : Initialize with API key\\nand generate text using prompt\n\nOpenAIAPI <-- UMLDiagram\nUMLDiagram: +create_diagram() \\n constructs diagram\nGitRepo <-- GitRepoManager\nGitRepo: +clone(repo_url)\nGitRepoError <|-- GitRepo\n\nnote left : UMLDiagram class for creating and\\nexporting UML diagrams in .puml format\nnote right : GitHub API library for \\ninteracting with GitHub repositories\nnote right : Clone GitHub repository\\nusing GitRepo object initialized \\nwith API key\n\n@enduml\nGenerate instructions from given code\n        response = self.client.complete(\n            engine="gpt-3",\n            prompt=f"Given the following code: \\n{code}, write instructions for {title}",\n            max_tokens=self.MAX_TOKENS,\n            temperature=0.1,\n            top_p=1,\n            stop=["Instructions:", "\nes[0].text)\n\n            except BaseException as e:\n                logging.exception(f"Error while generating text: {e}")\n\n\n//Define class GenerateUML\nclass GenerateUML:\n    - MAX_TOKENS: int\n    - MODEL_NAME: string\n    - client: OpenAI\n    + code_chunks: list[string]\n    + puml_chunks: list[string]\n\n    //Constructor\n    + GenerateUML(MAX_TOKENS: int, MODEL_NAME: string)\n\n    //Methods\n    + SplitCodeIntoChunks(code: string): code_chunks: list[string]\n    + SendPromptToOpenAI(prompt_text: string): response: string\n    + LogPromptText(prompt_text: string)\n    + LogResponseFromOpenAI(response: string)\n    + WriteResponseToFile(response: string, title: string)\n    + AppendPumlChunks(response: string)\n    + GenerateUMLChunks(code: string, title: string)\n\n// Define class Main\nclass Main {\n    - main()\n\n    //Methods\n    + process_code(code: string)\n    + create_uml_chunks(code: string)\n    + save_puml_chunks(puml_chunks: list[string])\n}\n\n// GenerateUML "1" *-- "1" Main: process_code()\n// GenerateUML "1" *-- "1" Main: create_uml_chunks()\n// Main *-- "1" GenerateUML: save_puml_chunks()\n\n// GenerateUML "1" *-- "1" OpenAI: SendPromptToOpenAI()\n// GenerateUML "1" *-- "1" OpenAI: LogResponseFromOpenAI()\n\n// GenerateUML "1" *-- "1" Log: LogPromptText(prompt_text: string)\n// GenerateUML "1" *-- "1" Log: WriteResponseToFile(response: string, title: string)\n\n// Main "1" *-- "1" Log: LogPromptText(prompt_text: string)\n// Main "1" *-- "1" Log: LogResponseFromOpenAI(response: string)\n// Main "1" *-- "1" Log: WriteResponseToFile(response: string, title: string)\n\n// GenerateUML "1" o-- "1" code: string\n// GenerateUML "1" o-- "1" title: string\n// Main "1" o-- "1" code: string\n// Main "1" o-- "1" code_chunks: list[string]\n// Main "1" o-- "1" puml_chunks: list[string]\nted_code\n\n        class OpenAIError(Exception):\n            pass\n\n@startuml\nclass UMLGenerator {\n    - MODEL_NAME: str\n    - MAX_TOKENS: int\n    - client: OpenAIClient\n    + generate_diagram(puml_files: list, title: str): str\n    - combine_puml_chunks(puml_chunks: list): str\n    - send_request_to_openai(text: str): str\n}\nclass OpenAIClient {\n    # API_KEY: str\n    - base_url: str\n    + completions: OpenAICompletionsResource\n    - _handle_errors(response: HttpResponse): None\n    + send_request(method: str, path: str, data: dict = None, headers: dict = None): HttpResponse\n}\nclass OpenAICompletionsResource {\n    - model: str\n    - prompt: str\n    - max_tokens: int\n    + create(): OpenAICompletionsResponse\n}\nclass OpenAICompletionsResponse {\n    - choices: list\n    + get_choice(index: int): OpenAICompletionChoice\n}\nclass OpenAICompletionChoice {\n    - text: str\n    - index: int\n    - finish_reason: str\n    - logprobs: dict\n    - selected_tokens: list\n}\nUMLGenerator --> OpenAIClient\nOpenAIClient <-- OpenAICompletionsResource\nOpenAICompletionsResource <-- OpenAICompletionsResponse\nUMLGenerator --> OpenAICompletionsResource\nclass OpenAIError << (E,red) Exception >> #FF7700 {\n    + _error_message: str\n}\nUMLGenerator --> OpenAIError : raise\nUMLGenerator : MODEL_NAME\nUMLGenerator : MAX_TOKENS\nUMLGenerator : client\nOpenAIClient : API_KEY\nOpenAIClient : base_url\nOpenAIClient : _handle_errors\nOpenAIClient : send_request\nOpenAICompletionsResource : model\nOpenAICompletionsResource : prompt\nOpenAICompletionsResource : max_tokens\nOpenAICompletionsResponse : choices\nOpenAICompletionChoice : text\nOpenAICompletionChoice : index\nOpenAICompletionChoice : finish_reason\nOpenAICompletionChoice : logprobs\nOpenAICompletionChoice : selected_tokens\n@enduml\nActivity Diagram:\n@startuml\nstart\n:Create directory if it doesn\'t exist;\nif (Directory exists?) then (yes)\n    :Write UML code to file;\nelse (no)\n    :Create directory;\n    :Write UML code to file;\nendif\n:Print path of file; \n:Return file path;\nstop\n@enduml\n\nClass Diagram:\n@startuml\nclass Ted_code {\n    - generated_code : String\n    - file_path : String\n    + save_generated_output(generated_code, file_path)\n}\n\nTed_code "1" -- "1" FileIO\n@enduml', 'max_tokens': 1024}}
2024-01-23 14:44:28,060 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2024-01-23 14:44:28,060 - DEBUG - send_request_headers.complete
2024-01-23 14:44:28,061 - DEBUG - send_request_body.started request=<Request [b'POST']>
2024-01-23 14:44:28,061 - DEBUG - send_request_body.complete
2024-01-23 14:44:28,061 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2024-01-23 14:44:28,441 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Tue, 23 Jan 2024 20:44:28 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'Cache-Control', b'no-cache, must-revalidate'), (b'openai-model', b'gpt-3.5-turbo-instruct'), (b'openai-organization', b'user-lqvxa4hv7a8o8eyohtzmcoc6'), (b'openai-processing-ms', b'125'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=15724800; includeSubDomains'), (b'x-ratelimit-limit-requests', b'3000'), (b'x-ratelimit-limit-tokens', b'250000'), (b'x-ratelimit-remaining-requests', b'2999'), (b'x-ratelimit-remaining-tokens', b'248975'), (b'x-ratelimit-reset-requests', b'20ms'), (b'x-ratelimit-reset-tokens', b'245ms'), (b'x-request-id', b'8fd5240aa822883cbcf9d85c907f2a03'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'84a2dff37ef14535-ATL'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=":443"; ma=86400')])
2024-01-23 14:44:28,442 - INFO - HTTP Request: POST https://api.openai.com/v1/completions "HTTP/1.1 200 OK"
2024-01-23 14:44:28,442 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2024-01-23 14:44:28,442 - DEBUG - receive_response_body.complete
2024-01-23 14:44:28,443 - DEBUG - response_closed.started
2024-01-23 14:44:28,443 - DEBUG - response_closed.complete
2024-01-23 14:44:28,443 - DEBUG - HTTP Request: POST https://api.openai.com/v1/completions "200 OK"
2024-01-23 14:44:28,444 - INFO - UML code generated for src/services/openai_api.py: @startuml
title openai_api.py

@enduml
2024-01-23 14:44:28,445 - INFO - Saving generated output to ../../output/openai_api.py.puml
2024-01-23 14:44:28,445 - INFO - Generated output saved to ../../output/openai_api.py.puml
2024-01-23 14:44:28,445 - INFO - Processing file: src/services/retrieve_code.py
2024-01-23 14:44:28,446 - INFO - Sending prompt to OpenAI: Create UML diagrams in .puml format for the following code:

import git
import json
import os
import logging
from logging import handlers


# Configure logging to write to a file
log_directory = 'logs'
# Create the directory if it doesn't exist
os.makedirs(log_directory, exist_ok=True)  
log_filename = os.path.join(log_directory, 'retrieve_code.log')
log_handler = handlers.RotatingFileHandler(log_filename, maxBytes=1024*1024, backupCount=5)  # Log file with rotation
log_formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')
log_handler.setFormatter(log_formatter)
logger = logging.getLogger()
logger.addHandler(log_handler)
logger.setLevel(logging.DEBUG)


def clone_repo(repo_url, temp_dir, access_token):
    try:
        # Modify the URL to include the access token
        if access_token:
            repo_url = repo_url.replace('https://', f'https://{access_token}@')
        logger.info(f"Cloning repository: {repo_url}")
        repo = git.Repo.clone_from(repo_url, temp_dir)
        logger.info(f"Successfully cloned repository: {repo_url}")
        
2024-01-23 14:44:28,447 - DEBUG - Request options: {'method': 'post', 'url': '/completions', 'files': None, 'json_data': {'model': 'gpt-3.5-turbo-instruct', 'prompt': 'Create UML diagrams in .puml format for the following code:\n\nimport git\nimport json\nimport os\nimport logging\nfrom logging import handlers\n\n\n# Configure logging to write to a file\nlog_directory = \'logs\'\n# Create the directory if it doesn\'t exist\nos.makedirs(log_directory, exist_ok=True)  \nlog_filename = os.path.join(log_directory, \'retrieve_code.log\')\nlog_handler = handlers.RotatingFileHandler(log_filename, maxBytes=1024*1024, backupCount=5)  # Log file with rotation\nlog_formatter = logging.Formatter(\'%(asctime)s - %(levelname)s - %(message)s\')\nlog_handler.setFormatter(log_formatter)\nlogger = logging.getLogger()\nlogger.addHandler(log_handler)\nlogger.setLevel(logging.DEBUG)\n\n\ndef clone_repo(repo_url, temp_dir, access_token):\n    try:\n        # Modify the URL to include the access token\n        if access_token:\n            repo_url = repo_url.replace(\'https://\', f\'https://{access_token}@\')\n        logger.info(f"Cloning repository: {repo_url}")\n        repo = git.Repo.clone_from(repo_url, temp_dir)\n        logger.info(f"Successfully cloned repository: {repo_url}")\n        ', 'max_tokens': 1024}}
2024-01-23 14:44:28,448 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2024-01-23 14:44:28,448 - DEBUG - send_request_headers.complete
2024-01-23 14:44:28,449 - DEBUG - send_request_body.started request=<Request [b'POST']>
2024-01-23 14:44:28,449 - DEBUG - send_request_body.complete
2024-01-23 14:44:28,449 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2024-01-23 14:44:33,710 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Tue, 23 Jan 2024 20:44:33 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'Cache-Control', b'no-cache, must-revalidate'), (b'openai-model', b'gpt-3.5-turbo-instruct'), (b'openai-organization', b'user-lqvxa4hv7a8o8eyohtzmcoc6'), (b'openai-processing-ms', b'5151'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=15724800; includeSubDomains'), (b'x-ratelimit-limit-requests', b'3000'), (b'x-ratelimit-limit-tokens', b'250000'), (b'x-ratelimit-remaining-requests', b'2999'), (b'x-ratelimit-remaining-tokens', b'248495'), (b'x-ratelimit-reset-requests', b'20ms'), (b'x-ratelimit-reset-tokens', b'360ms'), (b'x-request-id', b'd3c0cb10f0d2ff8670964b3934a81874'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'84a2dff5ea494535-ATL'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=":443"; ma=86400')])
2024-01-23 14:44:33,713 - INFO - HTTP Request: POST https://api.openai.com/v1/completions "HTTP/1.1 200 OK"
2024-01-23 14:44:33,714 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2024-01-23 14:44:33,715 - DEBUG - receive_response_body.complete
2024-01-23 14:44:33,715 - DEBUG - response_closed.started
2024-01-23 14:44:33,716 - DEBUG - response_closed.complete
2024-01-23 14:44:33,717 - DEBUG - HTTP Request: POST https://api.openai.com/v1/completions "200 OK"
2024-01-23 14:44:33,720 - INFO - Received response from OpenAI:  return True
    except git.GitError as e:
        logger.error(f"Error cloning repository: {repo_url}. Error message: {str(e)}")
        return False


def get_json_file(relative_path, branch='master'):
    # Get the current working directory
    cwd = os.getcwd()
    # Move to the repository directory
    os.chdir(temp_dir)
    # Find the file in the specified branch
    git_checkout = git.Repo(temp_dir).git.checkout(branch)
    if not git_checkout:
        logger.error(f"Error checking out branch: {branch}")
        return None
    # Open the file and load the JSON data
    with open(relative_path) as json_file:
        data = json.load(json_file)
    # Move back to the original working directory
    os.chdir(cwd)
    return data


@startuml

title UML Diagram for Retrieving Code from a Git Repository

actor User as u
participant logger
participant git
participant json
participant os
participant handlers


rectangle "Retrieving Code Process" {
    u -> logger: import
    u -> json: import
    u -> os: import
    u -> handlers: import

    u -> logger: log_directory
    u -> os: makedirs()
    logger -> os: exists()
    os -> logger: create directory

    u -> logger: log_filename
    u -> os: join()
    u -> handlers: RotatingFileHandler()
    u -> logger: setFormatter()
    u -> logger: addHandler()
    u -> logger: setLevel()

    u -> git: import
    git -> logger: Repo.clone_from()

    u -> logger: access_token
    u -> logger: repo_url
    logger -> git: replace()

    u -> logger: git.GitError
    return git.GitError

    logger -> git: Repo.clone_from()
    logger -> git: Repo(temp_dir).git.checkout()

    u -> logger: cwd
    u -> os: chdir()
    u -> git: Repo()

    git -> json: open()
    json -> logger: load()

    u -> os: chdir()
    return data
}


@enduml
2024-01-23 14:44:33,722 - INFO - Sending prompt to OpenAI: Create UML diagrams in .puml format for the following code:

return repo
    except Exception as e:
        logger.error(f"Failed to clone repository: {str(e)}")
        raise ValueError(f"Failed to clone repository: {str(e)}")

def retrieve_code(repo, branch_name):
    try:
        print(f"Attempting to checkout branch: {branch_name}")  # Diagnostic print statement
        logger.info(f"Attempting to checkout branch: {branch_name}")
        repo.git.fetch()  # Fetch the latest updates from the remote
        repo.git.checkout(branch_name)
        logger.info(f"Successfully checked out branch: {branch_name}")
        
        # Load the config
        config_file_path = os.path.join(os.path.dirname(os.path.abspath(__file__)), 'config.json')
        with open(config_file_path, 'r') as f:
            config = json.load(f)
        ignore_list = config.get('ignore', [])
        include_list = config.get('include', [])

        # Create a new dictionary to store file paths and their corresponding code
        included_files = {}
        for file in repo.tree():
            
2024-01-23 14:44:33,727 - DEBUG - Request options: {'method': 'post', 'url': '/completions', 'files': None, 'json_data': {'model': 'gpt-3.5-turbo-instruct', 'prompt': 'Create UML diagrams in .puml format for the following code:\n\nreturn repo\n    except Exception as e:\n        logger.error(f"Failed to clone repository: {str(e)}")\n        raise ValueError(f"Failed to clone repository: {str(e)}")\n\ndef retrieve_code(repo, branch_name):\n    try:\n        print(f"Attempting to checkout branch: {branch_name}")  # Diagnostic print statement\n        logger.info(f"Attempting to checkout branch: {branch_name}")\n        repo.git.fetch()  # Fetch the latest updates from the remote\n        repo.git.checkout(branch_name)\n        logger.info(f"Successfully checked out branch: {branch_name}")\n        \n        # Load the config\n        config_file_path = os.path.join(os.path.dirname(os.path.abspath(__file__)), \'config.json\')\n        with open(config_file_path, \'r\') as f:\n            config = json.load(f)\n        ignore_list = config.get(\'ignore\', [])\n        include_list = config.get(\'include\', [])\n\n        # Create a new dictionary to store file paths and their corresponding code\n        included_files = {}\n        for file in repo.tree():\n            ', 'max_tokens': 1024}}
2024-01-23 14:44:33,729 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2024-01-23 14:44:33,730 - DEBUG - send_request_headers.complete
2024-01-23 14:44:33,730 - DEBUG - send_request_body.started request=<Request [b'POST']>
2024-01-23 14:44:33,731 - DEBUG - send_request_body.complete
2024-01-23 14:44:33,731 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2024-01-23 14:44:34,007 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Tue, 23 Jan 2024 20:44:33 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'Cache-Control', b'no-cache, must-revalidate'), (b'openai-model', b'gpt-3.5-turbo-instruct'), (b'openai-organization', b'user-lqvxa4hv7a8o8eyohtzmcoc6'), (b'openai-processing-ms', b'95'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=15724800; includeSubDomains'), (b'x-ratelimit-limit-requests', b'3000'), (b'x-ratelimit-limit-tokens', b'250000'), (b'x-ratelimit-remaining-requests', b'2999'), (b'x-ratelimit-remaining-tokens', b'248975'), (b'x-ratelimit-reset-requests', b'20ms'), (b'x-ratelimit-reset-tokens', b'245ms'), (b'x-request-id', b'99a6bf8a32a85736778aa7614e2deb8e'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'84a2e01758554535-ATL'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=":443"; ma=86400')])
2024-01-23 14:44:34,009 - INFO - HTTP Request: POST https://api.openai.com/v1/completions "HTTP/1.1 200 OK"
2024-01-23 14:44:34,009 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2024-01-23 14:44:34,010 - DEBUG - receive_response_body.complete
2024-01-23 14:44:34,010 - DEBUG - response_closed.started
2024-01-23 14:44:34,011 - DEBUG - response_closed.complete
2024-01-23 14:44:34,011 - DEBUG - HTTP Request: POST https://api.openai.com/v1/completions "200 OK"
2024-01-23 14:44:34,014 - INFO - Received response from OpenAI: 
2024-01-23 14:44:34,015 - INFO - Sending prompt to OpenAI: Create UML diagrams in .puml format for the following code:

if any(file.path.endswith(ext) for ext in include_list) and not any(ignored_file in file.path for ignored_file in ignore_list):
                try:
                    with open(file.abspath, 'r') as f:
                        included_files[file.path] = f.read()
                    logger.info(f"Included file: {file.path}")
                except FileNotFoundError:
                    print(f"Ignoring missing file: {file.path}")  # Diagnostic print statement
                    logger.warning(f"Ignoring missing file: {file.path}")

        return included_files
    except Exception as e:
        logger.error(f"Failed to retrieve code: {str(e)}")
        raise ValueError(f"Failed to retrieve code: {str(e)}")
2024-01-23 14:44:34,018 - DEBUG - Request options: {'method': 'post', 'url': '/completions', 'files': None, 'json_data': {'model': 'gpt-3.5-turbo-instruct', 'prompt': 'Create UML diagrams in .puml format for the following code:\n\nif any(file.path.endswith(ext) for ext in include_list) and not any(ignored_file in file.path for ignored_file in ignore_list):\n                try:\n                    with open(file.abspath, \'r\') as f:\n                        included_files[file.path] = f.read()\n                    logger.info(f"Included file: {file.path}")\n                except FileNotFoundError:\n                    print(f"Ignoring missing file: {file.path}")  # Diagnostic print statement\n                    logger.warning(f"Ignoring missing file: {file.path}")\n\n        return included_files\n    except Exception as e:\n        logger.error(f"Failed to retrieve code: {str(e)}")\n        raise ValueError(f"Failed to retrieve code: {str(e)}")', 'max_tokens': 1024}}
2024-01-23 14:44:34,019 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2024-01-23 14:44:34,020 - DEBUG - send_request_headers.complete
2024-01-23 14:44:34,020 - DEBUG - send_request_body.started request=<Request [b'POST']>
2024-01-23 14:44:34,021 - DEBUG - send_request_body.complete
2024-01-23 14:44:34,021 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2024-01-23 14:44:35,917 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Tue, 23 Jan 2024 20:44:35 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'Cache-Control', b'no-cache, must-revalidate'), (b'openai-model', b'gpt-3.5-turbo-instruct'), (b'openai-organization', b'user-lqvxa4hv7a8o8eyohtzmcoc6'), (b'openai-processing-ms', b'1696'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=15724800; includeSubDomains'), (b'x-ratelimit-limit-requests', b'3000'), (b'x-ratelimit-limit-tokens', b'250000'), (b'x-ratelimit-remaining-requests', b'2999'), (b'x-ratelimit-remaining-tokens', b'248975'), (b'x-ratelimit-reset-requests', b'20ms'), (b'x-ratelimit-reset-tokens', b'245ms'), (b'x-request-id', b'5e856d8aadb4f1ffd98bc527a319d0b0'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'84a2e018b9f24535-ATL'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=":443"; ma=86400')])
2024-01-23 14:44:35,919 - INFO - HTTP Request: POST https://api.openai.com/v1/completions "HTTP/1.1 200 OK"
2024-01-23 14:44:35,920 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2024-01-23 14:44:35,921 - DEBUG - receive_response_body.complete
2024-01-23 14:44:35,921 - DEBUG - response_closed.started
2024-01-23 14:44:35,922 - DEBUG - response_closed.complete
2024-01-23 14:44:35,924 - DEBUG - HTTP Request: POST https://api.openai.com/v1/completions "200 OK"
2024-01-23 14:44:35,927 - INFO - Received response from OpenAI: 

@startuml

class Main {
+ main()
}

class File {
- path
+ endswith(ext: str) bool
+ abspath: str
}

class Logger {
+ info(message: str)
+ warning(message: str)
+ error(message: str)
}

class IncludedFiles {
- included_files: dict
+ get_files() dict
+ add_file(path: str, content: str)
}

Main --> File: file
File --> File: endswith()
File --> Logger: logger
File --> IncludedFiles: included_files
Logger --> File: logger
IncludedFiles --> File: get_files()
IncludedFiles --> File: add_file()
File --> Logger: logger
File --> Main: included_files
Logger --> Main: logger

@enduml
2024-01-23 14:44:35,936 - DEBUG - Request options: {'method': 'post', 'url': '/completions', 'files': None, 'json_data': {'model': 'gpt-3.5-turbo-instruct', 'prompt': 'Create a .puml file that integrates the following .puml files:\n\nreturn True\n    except git.GitError as e:\n        logger.error(f"Error cloning repository: {repo_url}. Error message: {str(e)}")\n        return False\n\n\ndef get_json_file(relative_path, branch=\'master\'):\n    # Get the current working directory\n    cwd = os.getcwd()\n    # Move to the repository directory\n    os.chdir(temp_dir)\n    # Find the file in the specified branch\n    git_checkout = git.Repo(temp_dir).git.checkout(branch)\n    if not git_checkout:\n        logger.error(f"Error checking out branch: {branch}")\n        return None\n    # Open the file and load the JSON data\n    with open(relative_path) as json_file:\n        data = json.load(json_file)\n    # Move back to the original working directory\n    os.chdir(cwd)\n    return data\n\n\n@startuml\n\ntitle UML Diagram for Retrieving Code from a Git Repository\n\nactor User as u\nparticipant logger\nparticipant git\nparticipant json\nparticipant os\nparticipant handlers\n\n\nrectangle "Retrieving Code Process" {\n    u -> logger: import\n    u -> json: import\n    u -> os: import\n    u -> handlers: import\n\n    u -> logger: log_directory\n    u -> os: makedirs()\n    logger -> os: exists()\n    os -> logger: create directory\n\n    u -> logger: log_filename\n    u -> os: join()\n    u -> handlers: RotatingFileHandler()\n    u -> logger: setFormatter()\n    u -> logger: addHandler()\n    u -> logger: setLevel()\n\n    u -> git: import\n    git -> logger: Repo.clone_from()\n\n    u -> logger: access_token\n    u -> logger: repo_url\n    logger -> git: replace()\n\n    u -> logger: git.GitError\n    return git.GitError\n\n    logger -> git: Repo.clone_from()\n    logger -> git: Repo(temp_dir).git.checkout()\n\n    u -> logger: cwd\n    u -> os: chdir()\n    u -> git: Repo()\n\n    git -> json: open()\n    json -> logger: load()\n\n    u -> os: chdir()\n    return data\n}\n\n\n@enduml\n\n@startuml\n\nclass Main {\n+ main()\n}\n\nclass File {\n- path\n+ endswith(ext: str) bool\n+ abspath: str\n}\n\nclass Logger {\n+ info(message: str)\n+ warning(message: str)\n+ error(message: str)\n}\n\nclass IncludedFiles {\n- included_files: dict\n+ get_files() dict\n+ add_file(path: str, content: str)\n}\n\nMain --> File: file\nFile --> File: endswith()\nFile --> Logger: logger\nFile --> IncludedFiles: included_files\nLogger --> File: logger\nIncludedFiles --> File: get_files()\nIncludedFiles --> File: add_file()\nFile --> Logger: logger\nFile --> Main: included_files\nLogger --> Main: logger\n\n@enduml', 'max_tokens': 1024}}
2024-01-23 14:44:35,938 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2024-01-23 14:44:35,939 - DEBUG - send_request_headers.complete
2024-01-23 14:44:35,939 - DEBUG - send_request_body.started request=<Request [b'POST']>
2024-01-23 14:44:35,939 - DEBUG - send_request_body.complete
2024-01-23 14:44:35,940 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2024-01-23 14:44:36,065 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Tue, 23 Jan 2024 20:44:36 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'Cache-Control', b'no-cache, must-revalidate'), (b'openai-model', b'gpt-3.5-turbo-instruct'), (b'openai-organization', b'user-lqvxa4hv7a8o8eyohtzmcoc6'), (b'openai-processing-ms', b'65'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=15724800; includeSubDomains'), (b'x-ratelimit-limit-requests', b'3000'), (b'x-ratelimit-limit-tokens', b'250000'), (b'x-ratelimit-remaining-requests', b'2999'), (b'x-ratelimit-remaining-tokens', b'248975'), (b'x-ratelimit-reset-requests', b'20ms'), (b'x-ratelimit-reset-tokens', b'245ms'), (b'x-request-id', b'417209bc870a6fa75f4cb32b056a563a'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'84a2e024b9f64535-ATL'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=":443"; ma=86400')])
2024-01-23 14:44:36,067 - INFO - HTTP Request: POST https://api.openai.com/v1/completions "HTTP/1.1 200 OK"
2024-01-23 14:44:36,068 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2024-01-23 14:44:36,069 - DEBUG - receive_response_body.complete
2024-01-23 14:44:36,069 - DEBUG - response_closed.started
2024-01-23 14:44:36,070 - DEBUG - response_closed.complete
2024-01-23 14:44:36,071 - DEBUG - HTTP Request: POST https://api.openai.com/v1/completions "200 OK"
2024-01-23 14:44:36,075 - INFO - UML code generated for src/services/retrieve_code.py: @startuml
title retrieve_code.py

@enduml
2024-01-23 14:44:36,076 - INFO - Saving generated output to ../../output/retrieve_code.py.puml
2024-01-23 14:44:36,077 - INFO - Generated output saved to ../../output/retrieve_code.py.puml
2024-01-23 14:44:36,078 - INFO - Skipping empty file: src/utils/__init__.py
2024-01-23 14:44:36,079 - INFO - Processing file: src/utils/code_to_uml.py
2024-01-23 14:44:36,079 - INFO - Sending prompt to OpenAI: Create UML diagrams in .puml format for the following code:

# code_to_uml.py
import os
from services.openai_api import OpenAIAPI 
import logging
from logging import handlers  

# Create an instance of the OpenAI API
api = OpenAIAPI()

# Configure logging to write to a file
log_directory = 'logs'
# Create the directory if it doesn't exist
os.makedirs(log_directory, exist_ok=True)  
log_filename = os.path.join(log_directory, 'code_to_uml.log')
log_handler = handlers.RotatingFileHandler(log_filename, maxBytes=1024*1024, backupCount=5)  # Log file with rotation
log_formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')
log_handler.setFormatter(log_formatter)
logger = logging.getLogger()
logger.addHandler(log_handler)
logger.setLevel(logging.DEBUG)

def generate_content(files, output_directory):
    logging.info(f"Files to process: {files}")  # Log the files dictionary
    file_paths = []  # Initialize a list to store the file paths
    for file_path, code in files.items():
        # Skip if the file is empty
        if not code.strip():
            log
2024-01-23 14:44:36,083 - DEBUG - Request options: {'method': 'post', 'url': '/completions', 'files': None, 'json_data': {'model': 'gpt-3.5-turbo-instruct', 'prompt': 'Create UML diagrams in .puml format for the following code:\n\n# code_to_uml.py\nimport os\nfrom services.openai_api import OpenAIAPI \nimport logging\nfrom logging import handlers  \n\n# Create an instance of the OpenAI API\napi = OpenAIAPI()\n\n# Configure logging to write to a file\nlog_directory = \'logs\'\n# Create the directory if it doesn\'t exist\nos.makedirs(log_directory, exist_ok=True)  \nlog_filename = os.path.join(log_directory, \'code_to_uml.log\')\nlog_handler = handlers.RotatingFileHandler(log_filename, maxBytes=1024*1024, backupCount=5)  # Log file with rotation\nlog_formatter = logging.Formatter(\'%(asctime)s - %(levelname)s - %(message)s\')\nlog_handler.setFormatter(log_formatter)\nlogger = logging.getLogger()\nlogger.addHandler(log_handler)\nlogger.setLevel(logging.DEBUG)\n\ndef generate_content(files, output_directory):\n    logging.info(f"Files to process: {files}")  # Log the files dictionary\n    file_paths = []  # Initialize a list to store the file paths\n    for file_path, code in files.items():\n        # Skip if the file is empty\n        if not code.strip():\n            log', 'max_tokens': 1024}}
2024-01-23 14:44:36,085 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2024-01-23 14:44:36,086 - DEBUG - send_request_headers.complete
2024-01-23 14:44:36,087 - DEBUG - send_request_body.started request=<Request [b'POST']>
2024-01-23 14:44:36,087 - DEBUG - send_request_body.complete
2024-01-23 14:44:36,087 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2024-01-23 14:44:38,296 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Tue, 23 Jan 2024 20:44:38 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'Cache-Control', b'no-cache, must-revalidate'), (b'openai-model', b'gpt-3.5-turbo-instruct'), (b'openai-organization', b'user-lqvxa4hv7a8o8eyohtzmcoc6'), (b'openai-processing-ms', b'2143'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=15724800; includeSubDomains'), (b'x-ratelimit-limit-requests', b'3000'), (b'x-ratelimit-limit-tokens', b'250000'), (b'x-ratelimit-remaining-requests', b'2999'), (b'x-ratelimit-remaining-tokens', b'248947'), (b'x-ratelimit-reset-requests', b'20ms'), (b'x-ratelimit-reset-tokens', b'252ms'), (b'x-request-id', b'bced79c667cbeeef55745f1694fd1a60'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'84a2e025ab034535-ATL'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=":443"; ma=86400')])
2024-01-23 14:44:38,299 - INFO - HTTP Request: POST https://api.openai.com/v1/completions "HTTP/1.1 200 OK"
2024-01-23 14:44:38,300 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2024-01-23 14:44:38,301 - DEBUG - receive_response_body.complete
2024-01-23 14:44:38,302 - DEBUG - response_closed.started
2024-01-23 14:44:38,302 - DEBUG - response_closed.complete
2024-01-23 14:44:38,303 - DEBUG - HTTP Request: POST https://api.openai.com/v1/completions "200 OK"
2024-01-23 14:44:38,306 - INFO - Received response from OpenAI: ging.warning(f"Skipping empty file: {file_path}")
            continue
        # Write the code to a file in the output directory
        output_file = os.path.join(output_directory, os.path.basename(file_path))
        with open(output_file, 'w+') as f:
            f.write(code)
        file_paths.append(output_file)  # Append the file path to the list
        logging.info(f"Code written to file: {output_file}")
    # Generate UML diagrams using the OpenAI API
    uml_diagrams = api.generate_diagrams(file_paths)
    logging.info("UML diagrams generated!")
    # Save the diagrams to files in the output directory
    for i, diagram in enumerate(uml_diagrams):
        output_filename = f"uml_diagram_{i+1}.png"
        output_path = os.path.join(output_directory, output_filename)
        diagram.save(output_path)
        logging.info(f"UML diagram saved: {output_path}")
    
![UML Diagram](https://raw.githubusercontent.com/SirJamsi/Lambdata_guide/master/UML.jpg)

2024-01-23 14:44:38,312 - INFO - Sending prompt to OpenAI: Create UML diagrams in .puml format for the following code:

ging.info(f"Skipping empty file: {file_path}")
            continue
        logging.info(f"Processing file: {file_path}")
        response = api.generate_from_code(code, os.path.basename(file_path))
        # Extract the UML code from the response
        generated_code_for_file = extract_uml_code(response)
        if generated_code_for_file is None:
            logging.error(f"Failed to generate UML diagram for {file_path}")
            continue
        logging.info(f"UML code generated for {file_path}: {generated_code_for_file}")  # Log the generated UML code

        # Save the UML code for each file to a separate .puml file
        file_name = f"{os.path.basename(file_path)}.puml"
        final_output_path = api.save_generated_output(generated_code_for_file, os.path.join(output_directory, file_name))
        file_paths.append(final_output_path)  # Append the file path to the list

    logging.info(f"Generated file paths: {file_paths}")
    # Return the list of file paths
    return file_paths

def extract
2024-01-23 14:44:38,316 - DEBUG - Request options: {'method': 'post', 'url': '/completions', 'files': None, 'json_data': {'model': 'gpt-3.5-turbo-instruct', 'prompt': 'Create UML diagrams in .puml format for the following code:\n\nging.info(f"Skipping empty file: {file_path}")\n            continue\n        logging.info(f"Processing file: {file_path}")\n        response = api.generate_from_code(code, os.path.basename(file_path))\n        # Extract the UML code from the response\n        generated_code_for_file = extract_uml_code(response)\n        if generated_code_for_file is None:\n            logging.error(f"Failed to generate UML diagram for {file_path}")\n            continue\n        logging.info(f"UML code generated for {file_path}: {generated_code_for_file}")  # Log the generated UML code\n\n        # Save the UML code for each file to a separate .puml file\n        file_name = f"{os.path.basename(file_path)}.puml"\n        final_output_path = api.save_generated_output(generated_code_for_file, os.path.join(output_directory, file_name))\n        file_paths.append(final_output_path)  # Append the file path to the list\n\n    logging.info(f"Generated file paths: {file_paths}")\n    # Return the list of file paths\n    return file_paths\n\ndef extract', 'max_tokens': 1024}}
2024-01-23 14:44:38,318 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2024-01-23 14:44:38,319 - DEBUG - send_request_headers.complete
2024-01-23 14:44:38,319 - DEBUG - send_request_body.started request=<Request [b'POST']>
2024-01-23 14:44:38,320 - DEBUG - send_request_body.complete
2024-01-23 14:44:38,320 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2024-01-23 14:44:41,687 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Tue, 23 Jan 2024 20:44:41 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'Cache-Control', b'no-cache, must-revalidate'), (b'openai-model', b'gpt-3.5-turbo-instruct'), (b'openai-organization', b'user-lqvxa4hv7a8o8eyohtzmcoc6'), (b'openai-processing-ms', b'3222'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=15724800; includeSubDomains'), (b'x-ratelimit-limit-requests', b'3000'), (b'x-ratelimit-limit-tokens', b'250000'), (b'x-ratelimit-remaining-requests', b'2999'), (b'x-ratelimit-remaining-tokens', b'248975'), (b'x-ratelimit-reset-requests', b'20ms'), (b'x-ratelimit-reset-tokens', b'245ms'), (b'x-request-id', b'450c844d5593050e7105f4d3a80b983d'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'84a2e0339eed4535-ATL'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=":443"; ma=86400')])
2024-01-23 14:44:41,691 - INFO - HTTP Request: POST https://api.openai.com/v1/completions "HTTP/1.1 200 OK"
2024-01-23 14:44:41,693 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2024-01-23 14:44:41,696 - DEBUG - receive_response_body.complete
2024-01-23 14:44:41,697 - DEBUG - response_closed.started
2024-01-23 14:44:41,697 - DEBUG - response_closed.complete
2024-01-23 14:44:41,698 - DEBUG - HTTP Request: POST https://api.openai.com/v1/completions "200 OK"
2024-01-23 14:44:41,701 - INFO - Received response from OpenAI: _uml_code(response):
    """
    Extracts the UML code from the response of the API call.

    Parameters:
        response (str): Response from the API.

    Returns:
        str: UML code generated by the API.
    """

    if response is not None:
        split_response = response.split("\n")
        code_start_index = split_response.index("@startuml")
        code_end_index = split_response.index("@enduml")

        return "\n".join(split_response[code_start_index:code_end_index + 1])


@startuml
class UMLWriter {
    -output_directory: String
    -code: String
    -file_paths: List<String>
    -api: API

    +generate_uml() : List<String>
    -extract_uml_code(response: String) : String
}

class API {
    -base_url: String
    #generate_from_code()
    #save_generated_output(code: String, file_path: String) : String
}

class File {
    #file_path: String
    +basename() : String
}

class LoggingManager {
    #log_level: int
    +info(message: String)
    +error(message: String)
}

UMLWriter *-- "1" API
UMLWriter *-- "1..*" File
UMLWriter *-- "1" LoggingManager
API ..> File 
@enduml 
2024-01-23 14:44:41,703 - INFO - Sending prompt to OpenAI: Create UML diagrams in .puml format for the following code:

_uml_code(response):
    # Split the response into lines
    lines = response.split('\n')
    # Check if '@startuml' and '@enduml' are in the response
    if '@startuml' not in response or '@enduml' not in response:
        logging.error("Failed to extract UML code from the response. '@startuml' or '@enduml' not found.")
        return None
    # Find the start and end of the UML code
    start_index = lines.index('@startuml')
    end_index = lines.index('@enduml') if '@enduml' in lines else -1
    # Extract the UML code
    uml_code = '\n'.join(lines[start_index:end_index+1])
    return uml_code
2024-01-23 14:44:41,706 - DEBUG - Request options: {'method': 'post', 'url': '/completions', 'files': None, 'json_data': {'model': 'gpt-3.5-turbo-instruct', 'prompt': 'Create UML diagrams in .puml format for the following code:\n\n_uml_code(response):\n    # Split the response into lines\n    lines = response.split(\'\\n\')\n    # Check if \'@startuml\' and \'@enduml\' are in the response\n    if \'@startuml\' not in response or \'@enduml\' not in response:\n        logging.error("Failed to extract UML code from the response. \'@startuml\' or \'@enduml\' not found.")\n        return None\n    # Find the start and end of the UML code\n    start_index = lines.index(\'@startuml\')\n    end_index = lines.index(\'@enduml\') if \'@enduml\' in lines else -1\n    # Extract the UML code\n    uml_code = \'\\n\'.join(lines[start_index:end_index+1])\n    return uml_code', 'max_tokens': 1024}}
2024-01-23 14:44:41,708 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2024-01-23 14:44:41,708 - DEBUG - send_request_headers.complete
2024-01-23 14:44:41,709 - DEBUG - send_request_body.started request=<Request [b'POST']>
2024-01-23 14:44:41,709 - DEBUG - send_request_body.complete
2024-01-23 14:44:41,709 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2024-01-23 14:44:46,441 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Tue, 23 Jan 2024 20:44:46 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'Cache-Control', b'no-cache, must-revalidate'), (b'openai-model', b'gpt-3.5-turbo-instruct'), (b'openai-organization', b'user-lqvxa4hv7a8o8eyohtzmcoc6'), (b'openai-processing-ms', b'4636'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=15724800; includeSubDomains'), (b'x-ratelimit-limit-requests', b'3000'), (b'x-ratelimit-limit-tokens', b'250000'), (b'x-ratelimit-remaining-requests', b'2999'), (b'x-ratelimit-remaining-tokens', b'248975'), (b'x-ratelimit-reset-requests', b'20ms'), (b'x-ratelimit-reset-tokens', b'245ms'), (b'x-request-id', b'bfe693bd9e19175f11b357e26485d522'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'84a2e048cc694535-ATL'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=":443"; ma=86400')])
2024-01-23 14:44:46,444 - INFO - HTTP Request: POST https://api.openai.com/v1/completions "HTTP/1.1 200 OK"
2024-01-23 14:44:46,445 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2024-01-23 14:44:46,448 - DEBUG - receive_response_body.complete
2024-01-23 14:44:46,449 - DEBUG - response_closed.started
2024-01-23 14:44:46,449 - DEBUG - response_closed.complete
2024-01-23 14:44:46,451 - DEBUG - HTTP Request: POST https://api.openai.com/v1/completions "200 OK"
2024-01-23 14:44:46,456 - INFO - Received response from OpenAI: 

@startuml
title UML Code Extraction

class _uml_code
+response : string
-lines : string[]
--------
#split(response: string): string[]
#findUmlCode(): void
-extractCode(): string
@enduml

rectangle _uml_code {
    +response
    -lines
    {method}+split(response)
    {method}+findUmlCode()
    -extractCode()
}

start
:get response;
activate _uml_code
:split response into lines;
if('|startuml|' not in response and '|enduml|' not in response)
_activate logging
:error "Failed to extract UML code...";
return none;
else
:find start and end of UML code;
:extract UML code;
:return UML code;
end

_uml_code --> _uml_code : activate
_logging --> _uml_code : activate
_uml_code --> _uml_code : split
_uml_code --> _uml_code : findUmlCode
_uml_code --> _uml_code : extractCode
note right of _uml_code
    # split the response into lines
    lines = response.split('\n')
end note
note right of _uml_code
    if '@startuml' not in response or '@enduml' not in response:
        logging.error("Failed to extract UML code from the response. '@startuml' or '@enduml' not found.")
        return None
end note
note right of _uml_code
    start_index = lines.index('@startuml')
    end_index = lines.index('@enduml') if '@enduml' in lines else -1
end note
_uml_code --> _uml_code : return uml_code
note right of _uml_code
    # Extract the UML code
    uml_code = '\n'.join(lines[start_index:end_index+1])
end note
_uml_code --> end

@enduml
2024-01-23 14:44:46,460 - DEBUG - Request options: {'method': 'post', 'url': '/completions', 'files': None, 'json_data': {'model': 'gpt-3.5-turbo-instruct', 'prompt': 'Create a .puml file that integrates the following .puml files:\n\nging.warning(f"Skipping empty file: {file_path}")\n            continue\n        # Write the code to a file in the output directory\n        output_file = os.path.join(output_directory, os.path.basename(file_path))\n        with open(output_file, \'w+\') as f:\n            f.write(code)\n        file_paths.append(output_file)  # Append the file path to the list\n        logging.info(f"Code written to file: {output_file}")\n    # Generate UML diagrams using the OpenAI API\n    uml_diagrams = api.generate_diagrams(file_paths)\n    logging.info("UML diagrams generated!")\n    # Save the diagrams to files in the output directory\n    for i, diagram in enumerate(uml_diagrams):\n        output_filename = f"uml_diagram_{i+1}.png"\n        output_path = os.path.join(output_directory, output_filename)\n        diagram.save(output_path)\n        logging.info(f"UML diagram saved: {output_path}")\n    \n![UML Diagram](https://raw.githubusercontent.com/SirJamsi/Lambdata_guide/master/UML.jpg)\n_uml_code(response):\n    """\n    Extracts the UML code from the response of the API call.\n\n    Parameters:\n        response (str): Response from the API.\n\n    Returns:\n        str: UML code generated by the API.\n    """\n\n    if response is not None:\n        split_response = response.split("\\n")\n        code_start_index = split_response.index("@startuml")\n        code_end_index = split_response.index("@enduml")\n\n        return "\\n".join(split_response[code_start_index:code_end_index + 1])\n\n\n@startuml\nclass UMLWriter {\n    -output_directory: String\n    -code: String\n    -file_paths: List<String>\n    -api: API\n\n    +generate_uml() : List<String>\n    -extract_uml_code(response: String) : String\n}\n\nclass API {\n    -base_url: String\n    #generate_from_code()\n    #save_generated_output(code: String, file_path: String) : String\n}\n\nclass File {\n    #file_path: String\n    +basename() : String\n}\n\nclass LoggingManager {\n    #log_level: int\n    +info(message: String)\n    +error(message: String)\n}\n\nUMLWriter *-- "1" API\nUMLWriter *-- "1..*" File\nUMLWriter *-- "1" LoggingManager\nAPI ..> File \n@enduml\n@startuml\ntitle UML Code Extraction\n\nclass _uml_code\n+response : string\n-lines : string[]\n--------\n#split(response: string): string[]\n#findUmlCode(): void\n-extractCode(): string\n@enduml\n\nrectangle _uml_code {\n    +response\n    -lines\n    {method}+split(response)\n    {method}+findUmlCode()\n    -extractCode()\n}\n\nstart\n:get response;\nactivate _uml_code\n:split response into lines;\nif(\'|startuml|\' not in response and \'|enduml|\' not in response)\n_activate logging\n:error "Failed to extract UML code...";\nreturn none;\nelse\n:find start and end of UML code;\n:extract UML code;\n:return UML code;\nend\n\n_uml_code --> _uml_code : activate\n_logging --> _uml_code : activate\n_uml_code --> _uml_code : split\n_uml_code --> _uml_code : findUmlCode\n_uml_code --> _uml_code : extractCode\nnote right of _uml_code\n    # split the response into lines\n    lines = response.split(\'\\n\')\nend note\nnote right of _uml_code\n    if \'@startuml\' not in response or \'@enduml\' not in response:\n        logging.error("Failed to extract UML code from the response. \'@startuml\' or \'@enduml\' not found.")\n        return None\nend note\nnote right of _uml_code\n    start_index = lines.index(\'@startuml\')\n    end_index = lines.index(\'@enduml\') if \'@enduml\' in lines else -1\nend note\n_uml_code --> _uml_code : return uml_code\nnote right of _uml_code\n    # Extract the UML code\n    uml_code = \'\\n\'.join(lines[start_index:end_index+1])\nend note\n_uml_code --> end\n\n@enduml', 'max_tokens': 1024}}
2024-01-23 14:44:46,463 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2024-01-23 14:44:46,464 - DEBUG - send_request_headers.complete
2024-01-23 14:44:46,464 - DEBUG - send_request_body.started request=<Request [b'POST']>
2024-01-23 14:44:46,464 - DEBUG - send_request_body.complete
2024-01-23 14:44:46,464 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2024-01-23 14:44:47,831 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Tue, 23 Jan 2024 20:44:47 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'Cache-Control', b'no-cache, must-revalidate'), (b'openai-model', b'gpt-3.5-turbo-instruct'), (b'openai-organization', b'user-lqvxa4hv7a8o8eyohtzmcoc6'), (b'openai-processing-ms', b'1222'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=15724800; includeSubDomains'), (b'x-ratelimit-limit-requests', b'3000'), (b'x-ratelimit-limit-tokens', b'250000'), (b'x-ratelimit-remaining-requests', b'2999'), (b'x-ratelimit-remaining-tokens', b'248975'), (b'x-ratelimit-reset-requests', b'20ms'), (b'x-ratelimit-reset-tokens', b'245ms'), (b'x-request-id', b'4156006e792d2111e5fc1eb5f2cc09ec'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'84a2e0667e984535-ATL'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=":443"; ma=86400')])
2024-01-23 14:44:47,832 - INFO - HTTP Request: POST https://api.openai.com/v1/completions "HTTP/1.1 200 OK"
2024-01-23 14:44:47,832 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2024-01-23 14:44:47,833 - DEBUG - receive_response_body.complete
2024-01-23 14:44:47,833 - DEBUG - response_closed.started
2024-01-23 14:44:47,833 - DEBUG - response_closed.complete
2024-01-23 14:44:47,833 - DEBUG - HTTP Request: POST https://api.openai.com/v1/completions "200 OK"
2024-01-23 14:44:47,835 - INFO - UML code generated for src/utils/code_to_uml.py: @startuml
title code_to_uml.py
This .puml file integrates the following .puml files: UMLDiagram.puml, UMLCodeExtraction.puml, and UMLWriter.puml. It represents the process of generating UML diagrams from code using the OpenAI API and saving them to files in an output directory. It also includes a class for extracting UML code from the API response and a class for logging the process. The integration of these .puml files allows for a visual representation of the entire process and how the classes and methods interact with each other.
@enduml
2024-01-23 14:44:47,835 - INFO - Saving generated output to ../../output/code_to_uml.py.puml
2024-01-23 14:44:47,836 - INFO - Generated output saved to ../../output/code_to_uml.py.puml
2024-01-23 14:44:47,836 - INFO - Generated file paths: ['../../output/git_repo.py.puml', '../../output/uml_diagram.py.puml', '../../output/uml_from_repo.py.puml', '../../output/execute_generate_uml.py.puml', '../../output/UMLGenerator.py.puml', '../../output/openai_api.py.puml', '../../output/retrieve_code.py.puml', '../../output/code_to_uml.py.puml']
2024-01-23 14:44:47,836 - INFO - Final output paths: ['../../output/git_repo.py.puml', '../../output/uml_diagram.py.puml', '../../output/uml_from_repo.py.puml', '../../output/execute_generate_uml.py.puml', '../../output/UMLGenerator.py.puml', '../../output/openai_api.py.puml', '../../output/retrieve_code.py.puml', '../../output/code_to_uml.py.puml']
2024-01-23 14:44:47,836 - INFO - UML diagram saved at: ../../output/git_repo.py.puml
2024-01-23 14:44:47,837 - INFO - UML diagram saved at: ../../output/uml_diagram.py.puml
2024-01-23 14:44:47,837 - INFO - UML diagram saved at: ../../output/uml_from_repo.py.puml
2024-01-23 14:44:47,837 - INFO - UML diagram saved at: ../../output/execute_generate_uml.py.puml
2024-01-23 14:44:47,837 - INFO - UML diagram saved at: ../../output/UMLGenerator.py.puml
2024-01-23 14:44:47,837 - INFO - UML diagram saved at: ../../output/openai_api.py.puml
2024-01-23 14:44:47,837 - INFO - UML diagram saved at: ../../output/retrieve_code.py.puml
2024-01-23 14:44:47,838 - INFO - UML diagram saved at: ../../output/code_to_uml.py.puml
2024-01-23 14:44:47,838 - INFO - Cleaning up temporary directory
2024-01-23 14:44:47,841 - INFO - 127.0.0.1 - - [23/Jan/2024 14:44:47] "POST /generate-uml HTTP/1.1" 200 -
2024-01-23 15:29:43,500 - INFO -  * Detected change in '/Users/preston/Documents/gitRepos/diagrammr/src/services/openai_api.py', reloading
2024-01-23 15:29:43,569 - INFO -  * Restarting with stat
2024-01-23 15:29:43,897 - WARNING -  * Debugger is active!
2024-01-23 15:29:43,904 - INFO -  * Debugger PIN: 139-904-016
2024-01-23 15:29:47,590 - INFO - Received JSON data: {'gitRepoUrl': 'https://github.com/mprestonsparks/diagrammr.git', 'local_dir': '/Users/preston/Documents/gitRepos/diagrammr/src/scripts/../../output', 'gitHubAccessToken': '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', 'configFile': '/Users/preston/Documents/gitRepos/diagrammr/src/scripts/config.json'}
2024-01-23 15:29:47,590 - INFO - Received configFile: /Users/preston/Documents/gitRepos/diagrammr/src/scripts/config.json
2024-01-23 15:29:47,590 - INFO - Received gitHubAccessToken: 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
2024-01-23 15:29:47,805 - DEBUG - Failed checking if running in CYGWIN due to: FileNotFoundError(2, 'No such file or directory')
2024-01-23 15:29:47,806 - DEBUG - Popen(['git', 'clone', '-v', '--', 'https://github.com/mprestonsparks/diagrammr.git', '../../output'], cwd=/Users/preston/Documents/gitRepos/diagrammr, stdin=None, shell=False, universal_newlines=True)
2024-01-23 15:29:53,413 - DEBUG - Cmd(['git', 'clone', '-v', '--', 'https://github.com/mprestonsparks/diagrammr.git', '../../output'])'s unused stdout: 
2024-01-23 15:29:53,588 - DEBUG - Popen(['git', 'clone', '-v', '--', 'https://github.com/mprestonsparks/diagrammr.git', '../../output'], cwd=/Users/preston/Documents/gitRepos/diagrammr, stdin=None, shell=False, universal_newlines=True)
2024-01-23 15:29:59,572 - DEBUG - Cmd(['git', 'clone', '-v', '--', 'https://github.com/mprestonsparks/diagrammr.git', '../../output'])'s unused stdout: 
2024-01-23 15:29:59,573 - DEBUG - Popen(['git', 'cat-file', '--batch-check'], cwd=/Users/preston/Documents/output, stdin=<valid stream>, shell=False, universal_newlines=False)
2024-01-23 15:29:59,579 - DEBUG - Popen(['git', 'cat-file', '--batch'], cwd=/Users/preston/Documents/output, stdin=<valid stream>, shell=False, universal_newlines=False)
2024-01-23 15:29:59,602 - DEBUG - Type of included_files: <class 'dict'>
2024-01-23 15:29:59,602 - DEBUG - Value of included_files: {'src/models/__init__.py': '', 'src/models/git_repo.py': "# src/models/git_repo.py\nimport git\nimport json\nimport shutil\nimport os\n\nclass GitRepo:\n    def __init__(self, config_file):\n        with open(config_file) as json_file:\n            data = json.load(json_file)\n        self.repo_url = data['gitRepoUrl']\n        self.local_dir = data['local_dir']\n\n    def clone(self):\n        # Delete the directory if it exists and is not empty\n        if os.path.exists(self.local_dir) and os.listdir(self.local_dir):\n            shutil.rmtree(self.local_dir)\n        # Clone the repository and return the local path\n        # This is a simple example and doesn't handle errors\n        git.Repo.clone_from(self.repo_url, self.local_dir)\n\n    def retrieve_code(self, file_path):\n        # Retrieve the code from a file in the repository\n        # This is a simple example and doesn't handle errors\n        with open(f'{self.local_dir}/{file_path}') as file:\n            return file.read()", 'src/models/uml_diagram.py': "# src/models/uml_diagram.py\nimport os\nimport logging\nfrom logging import handlers\n\n# Configure logging to write to a file\nlog_directory = 'logs'\n# Create the directory if it doesn't exist\nos.makedirs(log_directory, exist_ok=True)  \nlog_filename = os.path.join(log_directory, 'models_uml_diagram.log')\nlog_handler = handlers.RotatingFileHandler(log_filename, maxBytes=1024*1024, backupCount=5)  # Log file with rotation\nlog_formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\nlog_handler.setFormatter(log_formatter)\nlogger = logging.getLogger()\nlogger.addHandler(log_handler)\nlogger.setLevel(logging.DEBUG)\n\nclass UMLDiagram:\n    def __init__(self, code, title):\n        self.code = code\n        self.title = title\n        self.uml_code = None\n\n    def generate(self, openai_api):\n        self.uml_code = openai_api.generate_from_code(self.code, self.title)\n\n    def save(self, openai_api, file_path):\n        if self.uml_code is not None:\n            openai_api.save_generated_output(self.uml_code, file_path)", 'src/routes/__init__.py': '', 'src/routes/uml_from_repo.py': '# uml_from_repo.py\nimport os\nimport fnmatch\nimport subprocess\nimport json\nimport git\nimport tempfile\nimport shutil\nimport logging\nfrom logging import handlers  \nfrom services.retrieve_code import clone_repo, retrieve_code\nfrom utils.code_to_uml import generate_content  # Import the function from code_to_uml.py\n\n \n# Configure logging to write to a file\nlog_directory = \'logs\'\n# Create the directory if it doesn\'t exist\nos.makedirs(log_directory, exist_ok=True)  \nlog_filename = os.path.join(log_directory, \'uml_from_repo.log\')\nlog_handler = handlers.RotatingFileHandler(log_filename, maxBytes=1024*1024, backupCount=5)  # Log file with rotation\nlog_formatter = logging.Formatter(\'%(asctime)s - %(levelname)s - %(message)s\')\nlog_handler.setFormatter(log_formatter)\nlogger = logging.getLogger()\nlogger.addHandler(log_handler)\nlogger.setLevel(logging.DEBUG)\n\ndef process_request(git_repo, github_access_token):\n    output_directory = git_repo.local_dir  # Get the output directory from the GitRepo object\n    branch_name = \'master\'  # \'master\' is the default branch name\n    temp_dir = None  # Initialize temp_dir outside the try block\n\n    try:\n        if not git_repo.repo_url or not output_directory or not github_access_token or not branch_name:\n            logger.error("Missing required parameters")  \n            return {"error": "Missing required parameters"}, 400\n\n        try:\n            temp_dir = tempfile.mkdtemp()\n        except Exception as e:\n            logger.error(f"Error during UML generation: {str(e)}", exc_info=True)\n            return {"error": str(e)}, 500\n\n        # Check if the directory exists and remove it\n        if os.path.exists(output_directory):\n            shutil.rmtree(output_directory)\n\n        # Clone the repository\n        repo = git.Repo.clone_from(git_repo.repo_url, output_directory)\n\n        # Load the config\n        with open(\'src/routes/config.json\', \'r\') as f:\n            config = json.load(f)\n        \n        def traverse_directories(repo, git_repo, config):\n            included_files = {}\n            for item in repo.tree().traverse():\n                if item.type == \'blob\':  # This means it\'s a file, not a directory\n                    for pattern in config[\'include\']:\n                        if fnmatch.fnmatch(item.path, pattern):\n                            with open(os.path.join(git_repo.local_dir, item.path), \'r\') as f:\n                                included_files[item.path] = f.read()\n                            break  # No need to check the remaining patterns\n            return included_files\n\n        # Retrieve the code from the repository\n        included_files = traverse_directories(repo, git_repo, config)\n\n        logger.debug(f"Type of included_files: {type(included_files)}")\n        logger.debug(f"Value of included_files: {included_files}")\n\n        # Save the UML diagram to a file\n        logger.info(f"Saving UML diagram to {output_directory}")\n        final_output_paths = generate_content(included_files, output_directory)  # This is now a list of file paths\n        logger.info(f"Final output paths: {final_output_paths}")\n\n        for path in final_output_paths:\n            logger.info(f"UML diagram saved at: {path}")  # Log the path where each UML diagram was saved\n\n        return {\n            "message": "UML diagrams generated successfully",\n            "details": {\n                "Repository": git_repo.repo_url,\n                "Output Paths": final_output_paths  # This is now a list of file paths\n            }\n        }, 200\n\n    except Exception as e:\n        logger.error(f"Error during UML generation: {str(e)}")\n        return {"error": str(e)}, 500\n\n    finally:\n        # Clean up the temporary directory\n        if temp_dir is not None:\n            logger.info("Cleaning up temporary directory")\n            shutil.rmtree(temp_dir)\n', 'src/scripts/__init__.py': '', 'src/scripts/execute_generate_uml.py': "import json\nimport os\nimport requests\nimport logging\nfrom logging import handlers\nfrom dotenv import load_dotenv\n\n# Load environment variables from .env file\nload_dotenv()\n\n# Configure logging to write to a file\nlog_directory = '../../logs'\n# Create the directory if it doesn't exist\nos.makedirs(log_directory, exist_ok=True)  \nlog_filename = os.path.join(log_directory, 'execute_generate_uml.log')\nlog_handler = handlers.RotatingFileHandler(log_filename, maxBytes=1024*1024, backupCount=5)  # Log file with rotation\nlog_formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\nlog_handler.setFormatter(log_formatter)\nlogger = logging.getLogger()\nlogger.addHandler(log_handler)\nlogger.setLevel(logging.DEBUG)\n\n\n# Configure logging\nlogging.basicConfig(filename='execute_generate_uml.log', level=logging.DEBUG, format='%(asctime)s - %(levelname)s - %(message)s')\n\n# Current file's directory\ncurrent_dir = os.path.dirname(os.path.abspath(__file__))\n# Configuration file path\nconfig_file_path = os.path.join(current_dir, 'config.json')\n\n# Read configuration from the JSON file\nwith open(config_file_path, 'r') as config_file:\n    config_data = json.load(config_file)\n\n# Retrieve the GitHub access token from environment variables\ngithub_token = os.getenv('GITHUB_PAT')\n\n# Endpoint URL\nurl = 'http://127.0.0.1:5000/generate-uml'\n\n# Headers\nheaders = {\n    'Content-Type': 'application/json'\n}\n\n# Update the GitHub access token, local directory, and configFile in the config data\nconfig_data['gitHubAccessToken'] = github_token\nconfig_data['local_dir'] = os.path.join(current_dir, '../..', 'output')\nconfig_data['configFile'] = config_file_path  \n\n# Log the JSON data being sent in the request\nlogging.info(f'Sending JSON data to {url}:')\nlogging.info(json.dumps(config_data, indent=2))\n\ntry:\n    # Make the POST request\n    response = requests.post(url, headers=headers, json=config_data)\n\n    # Log the response from the server\n    logging.info(f'Response from server ({url}):')\n    logging.info(f'Status Code: {response.status_code}')\n    logging.info(f'Response Text: {response.text}')\n\n    # Print the response from the server\n    print(response.text)\n\n    # Save the response to a file in the output directory\n    output_dir = os.path.join(current_dir, '../..', 'output')\n    os.makedirs(output_dir, exist_ok=True)\n    with open(os.path.join(output_dir, 'response.json'), 'w') as output_file:\n        json.dump(response.json(), output_file, indent=2)\n\nexcept Exception as e:\n    # Log any exceptions that occur\n    logging.error(f'Error occurred: {str(e)}')", 'src/services/UMLGenerator.py': 'import json\nimport os\nimport logging\nfrom services.openai_api import OpenAIAPI\n\nclass UMLGenerator:\n    def __init__(self, config_file):\n        self.api = OpenAIAPI()\n        self.OUTPUT_DIRECTORY = "src/output" \n        # Load the configuration\n        with open(config_file) as file:\n            self.config = json.load(file)\n\n    def generate_from_codes(self, file_list):\n        uml_contents = []\n        for file in file_list:\n            if self._should_skip_file(file):\n                continue\n            logging.info(f"Generating UML for file: {file}")\n            with open(file, \'r\') as f:\n                file_content = f.read()\n            uml_content = self.api.generate_from_code(file_content, os.path.basename(file))\n            if uml_content != "UML generation failed":\n                uml_contents.append(uml_content)\n            else:\n                logging.error(f"UML generation failed for file: {file}")\n        return uml_contents\n \n    def save_generated_outputs(self, uml_contents, file_names):\n        for uml_content, file_name in zip(uml_contents, file_names):\n            logging.info(f"Saving UML diagram for file: {file_name}")\n            self.api.save_generated_output(uml_content, file_name)\n\n    def _should_skip_file(self, file):\n        python_files_only = self.config.get(\'python_files_only\', False)\n        ignore_files = self.config.get(\'ignore_files\', [])\n        ignore_extensions = self.config.get(\'ignore_extensions\', [])\n        return (file in ignore_files or \n                (python_files_only and not file.endswith(\'.py\')) or \n                any(file.endswith(ext) for ext in ignore_extensions))\n\n    def _generate_from_code(self, file):\n        logging.info(f"Generating UML diagram for file: {file}")\n        return self.api.generate_from_code(file)', 'src/services/__init__.py': '', 'src/services/openai_api.py': 'import json\nimport os\nimport logging\nfrom logging import handlers\nfrom openai import OpenAI, OpenAIError\nfrom dotenv import load_dotenv\nfrom models.uml_diagram import UMLDiagram\nfrom models.git_repo import GitRepo  # Import GitRepo\n\n\n# Load environment variables from .env file\nload_dotenv()\n\n# Configure logging to write to a file\nlog_directory = \'logs\'\n# Create the directory if it doesn\'t exist\nos.makedirs(log_directory, exist_ok=True)  \nlog_filename = os.path.join(log_directory, \'openai_api.log\')\nlog_handler = handlers.RotatingFileHandler(log_filename, maxBytes=1024*1024, backupCount=5)  # Log file with rotation\nlog_formatter = logging.Formatter(\'%(asctime)s - %(levelname)s - %(message)s\')\nlog_handler.setFormatter(log_formatter)\nlogger = logging.getLogger()\nlogger.addHandler(log_handler)\nlogger.setLevel(logging.DEBUG)\n\n\nclass OpenAIAPI:\n    def __init__(self):\n        # Load environment variables from .env file\n        load_dotenv()\n\n        # Retrieve the OpenAI API key from the environment\n        openai_api_key = os.getenv("OPENAI_API_KEY")\n\n        self.client = OpenAI(api_key=openai_api_key)\n\n        # Constants\n        self.MODEL_NAME = "gpt-3.5-turbo-instruct"\n        self.MAX_TOKENS = 1024\n        self.OUTPUT_DIRECTORY = "src/output"\n\n    def generate_uml_diagram(self, repo_url, file_path, title):\n        # Clone the repository and retrieve the code\n        git_repo = GitRepo(repo_url)\n        git_repo.clone()\n        code = git_repo.retrieve_code(file_path)\n\n        uml_diagram = UMLDiagram(code, title)\n        uml_diagram.generate(self)\n        return uml_diagram\n\n    def save_uml_diagram(self, uml_diagram, file_path):\n        uml_diagram.save(self, file_path)\n\n    def write_response_to_file(self, response, filename):\n        output_dir = "output/openai"\n        os.makedirs(output_dir, exist_ok=True)  # Create the directory if it doesn\'t exist\n        with open(os.path.join(output_dir, f"{filename}.txt"), \'w\') as f:\n            f.write(response)\n\n    def generate_from_code(self, code, title):\n        # Split the code into chunks of MAX_TOKENS\n        code_chunks = [code[i:i+self.MAX_TOKENS] for i in range(0, len(code), self.MAX_TOKENS)]\n\n        puml_chunks = []\n        for chunk in code_chunks:\n            prompt_text = f"Create UML diagrams in .puml format for the following code:\\n\\n{chunk}"\n            logging.info(f"Sending prompt to OpenAI: {prompt_text}")  # Log the prompt text\n            try:\n                response = self.client.completions.create(\n                    model=self.MODEL_NAME,\n                    prompt=prompt_text,\n                    max_tokens=self.MAX_TOKENS)\n                \n                # Log the response from OpenAI\n                logging.info(f"Received response from OpenAI: {response.choices[0].text}")  # Log the response from OpenAI\n\n                # Write the response to a .txt file\n                self.write_response_to_file(response.choices[0].text, title)\n\n                # Append the generated text to the UML code\n                puml_chunks.append(response.choices[0].text.strip())\n            except OpenAIError as e:\n                logging.error(f"An error occurred while sending the prompt: {e}")  # Log the error message\n                return f"An error occurred: {e}"  # Return a message if an OpenAI API error occurs\n\n        # Combine the .puml chunks into a single string\n        combined_puml = "\\n".join(puml_chunks)\n\n        # Send a final request to OpenAI to integrate the .puml chunks\n        prompt_text = f"Create a .puml file that integrates the following .puml files:\\n\\n{combined_puml}"\n        response = self.client.completions.create(\n            model=self.MODEL_NAME,\n            prompt=prompt_text,\n            max_tokens=self.MAX_TOKENS)\n\n        # Extract the integrated .puml code from the response\n        generated_code = response.choices[0].text.strip()\n\n        # Add the @startuml, title and @enduml tags only once for each UML diagram\n        generated_code = f"@startuml\\n" + f"title {title}\\n" + generated_code + "\\n@enduml\\n"\n        return generated_code\n    \n\n    def save_generated_output(self, generated_code, file_path):\n        # Create the directory if it does not exist\n        logging.info(f"Saving generated output to {file_path}")\n        os.makedirs(os.path.dirname(file_path), exist_ok=True)\n        \n        # Write the UML code to the file\n        with open(file_path, \'w\') as file:\n            file.write(generated_code)\n            logging.info(f"Generated output saved to {file_path}")  # Optional: print out the path where the file was saved\n\n        return file_path   ', 'src/services/retrieve_code.py': 'import git\nimport json\nimport os\nimport logging\nfrom logging import handlers\n\n\n# Configure logging to write to a file\nlog_directory = \'logs\'\n# Create the directory if it doesn\'t exist\nos.makedirs(log_directory, exist_ok=True)  \nlog_filename = os.path.join(log_directory, \'retrieve_code.log\')\nlog_handler = handlers.RotatingFileHandler(log_filename, maxBytes=1024*1024, backupCount=5)  # Log file with rotation\nlog_formatter = logging.Formatter(\'%(asctime)s - %(levelname)s - %(message)s\')\nlog_handler.setFormatter(log_formatter)\nlogger = logging.getLogger()\nlogger.addHandler(log_handler)\nlogger.setLevel(logging.DEBUG)\n\n\ndef clone_repo(repo_url, temp_dir, access_token):\n    try:\n        # Modify the URL to include the access token\n        if access_token:\n            repo_url = repo_url.replace(\'https://\', f\'https://{access_token}@\')\n        logger.info(f"Cloning repository: {repo_url}")\n        repo = git.Repo.clone_from(repo_url, temp_dir)\n        logger.info(f"Successfully cloned repository: {repo_url}")\n        return repo\n    except Exception as e:\n        logger.error(f"Failed to clone repository: {str(e)}")\n        raise ValueError(f"Failed to clone repository: {str(e)}")\n\ndef retrieve_code(repo, branch_name):\n    try:\n        print(f"Attempting to checkout branch: {branch_name}")  # Diagnostic print statement\n        logger.info(f"Attempting to checkout branch: {branch_name}")\n        repo.git.fetch()  # Fetch the latest updates from the remote\n        repo.git.checkout(branch_name)\n        logger.info(f"Successfully checked out branch: {branch_name}")\n        \n        # Load the config\n        config_file_path = os.path.join(os.path.dirname(os.path.abspath(__file__)), \'config.json\')\n        with open(config_file_path, \'r\') as f:\n            config = json.load(f)\n        ignore_list = config.get(\'ignore\', [])\n        include_list = config.get(\'include\', [])\n\n        # Create a new dictionary to store file paths and their corresponding code\n        included_files = {}\n        for file in repo.tree():\n            if any(file.path.endswith(ext) for ext in include_list) and not any(ignored_file in file.path for ignored_file in ignore_list):\n                try:\n                    with open(file.abspath, \'r\') as f:\n                        included_files[file.path] = f.read()\n                    logger.info(f"Included file: {file.path}")\n                except FileNotFoundError:\n                    print(f"Ignoring missing file: {file.path}")  # Diagnostic print statement\n                    logger.warning(f"Ignoring missing file: {file.path}")\n\n        return included_files\n    except Exception as e:\n        logger.error(f"Failed to retrieve code: {str(e)}")\n        raise ValueError(f"Failed to retrieve code: {str(e)}")', 'src/utils/__init__.py': '', 'src/utils/code_to_uml.py': '# code_to_uml.py\nimport os\nfrom services.openai_api import OpenAIAPI \nimport logging\nfrom logging import handlers  \n\n# Create an instance of the OpenAI API\napi = OpenAIAPI()\n\n# Configure logging to write to a file\nlog_directory = \'logs\'\n# Create the directory if it doesn\'t exist\nos.makedirs(log_directory, exist_ok=True)  \nlog_filename = os.path.join(log_directory, \'code_to_uml.log\')\nlog_handler = handlers.RotatingFileHandler(log_filename, maxBytes=1024*1024, backupCount=5)  # Log file with rotation\nlog_formatter = logging.Formatter(\'%(asctime)s - %(levelname)s - %(message)s\')\nlog_handler.setFormatter(log_formatter)\nlogger = logging.getLogger()\nlogger.addHandler(log_handler)\nlogger.setLevel(logging.DEBUG)\n\ndef generate_content(files, output_directory):\n    logging.info(f"Files to process: {files}")  # Log the files dictionary\n    file_paths = []  # Initialize a list to store the file paths\n    for file_path, code in files.items():\n        # Skip if the file is empty\n        if not code.strip():\n            logging.info(f"Skipping empty file: {file_path}")\n            continue\n        logging.info(f"Processing file: {file_path}")\n        response = api.generate_from_code(code, os.path.basename(file_path))\n        # Extract the UML code from the response\n        generated_code_for_file = extract_uml_code(response)\n        if generated_code_for_file is None:\n            logging.error(f"Failed to generate UML diagram for {file_path}")\n            continue\n        logging.info(f"UML code generated for {file_path}: {generated_code_for_file}")  # Log the generated UML code\n\n        # Save the UML code for each file to a separate .puml file\n        file_name = f"{os.path.basename(file_path)}.puml"\n        final_output_path = api.save_generated_output(generated_code_for_file, os.path.join(output_directory, file_name))\n        file_paths.append(final_output_path)  # Append the file path to the list\n\n    logging.info(f"Generated file paths: {file_paths}")\n    # Return the list of file paths\n    return file_paths\n\ndef extract_uml_code(response):\n    # Split the response into lines\n    lines = response.split(\'\\n\')\n    # Check if \'@startuml\' and \'@enduml\' are in the response\n    if \'@startuml\' not in response or \'@enduml\' not in response:\n        logging.error("Failed to extract UML code from the response. \'@startuml\' or \'@enduml\' not found.")\n        return None\n    # Find the start and end of the UML code\n    start_index = lines.index(\'@startuml\')\n    end_index = lines.index(\'@enduml\') if \'@enduml\' in lines else -1\n    # Extract the UML code\n    uml_code = \'\\n\'.join(lines[start_index:end_index+1])\n    return uml_code'}
2024-01-23 15:29:59,602 - INFO - Saving UML diagram to ../../output
2024-01-23 15:29:59,602 - INFO - Files to process: {'src/models/__init__.py': '', 'src/models/git_repo.py': "# src/models/git_repo.py\nimport git\nimport json\nimport shutil\nimport os\n\nclass GitRepo:\n    def __init__(self, config_file):\n        with open(config_file) as json_file:\n            data = json.load(json_file)\n        self.repo_url = data['gitRepoUrl']\n        self.local_dir = data['local_dir']\n\n    def clone(self):\n        # Delete the directory if it exists and is not empty\n        if os.path.exists(self.local_dir) and os.listdir(self.local_dir):\n            shutil.rmtree(self.local_dir)\n        # Clone the repository and return the local path\n        # This is a simple example and doesn't handle errors\n        git.Repo.clone_from(self.repo_url, self.local_dir)\n\n    def retrieve_code(self, file_path):\n        # Retrieve the code from a file in the repository\n        # This is a simple example and doesn't handle errors\n        with open(f'{self.local_dir}/{file_path}') as file:\n            return file.read()", 'src/models/uml_diagram.py': "# src/models/uml_diagram.py\nimport os\nimport logging\nfrom logging import handlers\n\n# Configure logging to write to a file\nlog_directory = 'logs'\n# Create the directory if it doesn't exist\nos.makedirs(log_directory, exist_ok=True)  \nlog_filename = os.path.join(log_directory, 'models_uml_diagram.log')\nlog_handler = handlers.RotatingFileHandler(log_filename, maxBytes=1024*1024, backupCount=5)  # Log file with rotation\nlog_formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\nlog_handler.setFormatter(log_formatter)\nlogger = logging.getLogger()\nlogger.addHandler(log_handler)\nlogger.setLevel(logging.DEBUG)\n\nclass UMLDiagram:\n    def __init__(self, code, title):\n        self.code = code\n        self.title = title\n        self.uml_code = None\n\n    def generate(self, openai_api):\n        self.uml_code = openai_api.generate_from_code(self.code, self.title)\n\n    def save(self, openai_api, file_path):\n        if self.uml_code is not None:\n            openai_api.save_generated_output(self.uml_code, file_path)", 'src/routes/__init__.py': '', 'src/routes/uml_from_repo.py': '# uml_from_repo.py\nimport os\nimport fnmatch\nimport subprocess\nimport json\nimport git\nimport tempfile\nimport shutil\nimport logging\nfrom logging import handlers  \nfrom services.retrieve_code import clone_repo, retrieve_code\nfrom utils.code_to_uml import generate_content  # Import the function from code_to_uml.py\n\n \n# Configure logging to write to a file\nlog_directory = \'logs\'\n# Create the directory if it doesn\'t exist\nos.makedirs(log_directory, exist_ok=True)  \nlog_filename = os.path.join(log_directory, \'uml_from_repo.log\')\nlog_handler = handlers.RotatingFileHandler(log_filename, maxBytes=1024*1024, backupCount=5)  # Log file with rotation\nlog_formatter = logging.Formatter(\'%(asctime)s - %(levelname)s - %(message)s\')\nlog_handler.setFormatter(log_formatter)\nlogger = logging.getLogger()\nlogger.addHandler(log_handler)\nlogger.setLevel(logging.DEBUG)\n\ndef process_request(git_repo, github_access_token):\n    output_directory = git_repo.local_dir  # Get the output directory from the GitRepo object\n    branch_name = \'master\'  # \'master\' is the default branch name\n    temp_dir = None  # Initialize temp_dir outside the try block\n\n    try:\n        if not git_repo.repo_url or not output_directory or not github_access_token or not branch_name:\n            logger.error("Missing required parameters")  \n            return {"error": "Missing required parameters"}, 400\n\n        try:\n            temp_dir = tempfile.mkdtemp()\n        except Exception as e:\n            logger.error(f"Error during UML generation: {str(e)}", exc_info=True)\n            return {"error": str(e)}, 500\n\n        # Check if the directory exists and remove it\n        if os.path.exists(output_directory):\n            shutil.rmtree(output_directory)\n\n        # Clone the repository\n        repo = git.Repo.clone_from(git_repo.repo_url, output_directory)\n\n        # Load the config\n        with open(\'src/routes/config.json\', \'r\') as f:\n            config = json.load(f)\n        \n        def traverse_directories(repo, git_repo, config):\n            included_files = {}\n            for item in repo.tree().traverse():\n                if item.type == \'blob\':  # This means it\'s a file, not a directory\n                    for pattern in config[\'include\']:\n                        if fnmatch.fnmatch(item.path, pattern):\n                            with open(os.path.join(git_repo.local_dir, item.path), \'r\') as f:\n                                included_files[item.path] = f.read()\n                            break  # No need to check the remaining patterns\n            return included_files\n\n        # Retrieve the code from the repository\n        included_files = traverse_directories(repo, git_repo, config)\n\n        logger.debug(f"Type of included_files: {type(included_files)}")\n        logger.debug(f"Value of included_files: {included_files}")\n\n        # Save the UML diagram to a file\n        logger.info(f"Saving UML diagram to {output_directory}")\n        final_output_paths = generate_content(included_files, output_directory)  # This is now a list of file paths\n        logger.info(f"Final output paths: {final_output_paths}")\n\n        for path in final_output_paths:\n            logger.info(f"UML diagram saved at: {path}")  # Log the path where each UML diagram was saved\n\n        return {\n            "message": "UML diagrams generated successfully",\n            "details": {\n                "Repository": git_repo.repo_url,\n                "Output Paths": final_output_paths  # This is now a list of file paths\n            }\n        }, 200\n\n    except Exception as e:\n        logger.error(f"Error during UML generation: {str(e)}")\n        return {"error": str(e)}, 500\n\n    finally:\n        # Clean up the temporary directory\n        if temp_dir is not None:\n            logger.info("Cleaning up temporary directory")\n            shutil.rmtree(temp_dir)\n', 'src/scripts/__init__.py': '', 'src/scripts/execute_generate_uml.py': "import json\nimport os\nimport requests\nimport logging\nfrom logging import handlers\nfrom dotenv import load_dotenv\n\n# Load environment variables from .env file\nload_dotenv()\n\n# Configure logging to write to a file\nlog_directory = '../../logs'\n# Create the directory if it doesn't exist\nos.makedirs(log_directory, exist_ok=True)  \nlog_filename = os.path.join(log_directory, 'execute_generate_uml.log')\nlog_handler = handlers.RotatingFileHandler(log_filename, maxBytes=1024*1024, backupCount=5)  # Log file with rotation\nlog_formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\nlog_handler.setFormatter(log_formatter)\nlogger = logging.getLogger()\nlogger.addHandler(log_handler)\nlogger.setLevel(logging.DEBUG)\n\n\n# Configure logging\nlogging.basicConfig(filename='execute_generate_uml.log', level=logging.DEBUG, format='%(asctime)s - %(levelname)s - %(message)s')\n\n# Current file's directory\ncurrent_dir = os.path.dirname(os.path.abspath(__file__))\n# Configuration file path\nconfig_file_path = os.path.join(current_dir, 'config.json')\n\n# Read configuration from the JSON file\nwith open(config_file_path, 'r') as config_file:\n    config_data = json.load(config_file)\n\n# Retrieve the GitHub access token from environment variables\ngithub_token = os.getenv('GITHUB_PAT')\n\n# Endpoint URL\nurl = 'http://127.0.0.1:5000/generate-uml'\n\n# Headers\nheaders = {\n    'Content-Type': 'application/json'\n}\n\n# Update the GitHub access token, local directory, and configFile in the config data\nconfig_data['gitHubAccessToken'] = github_token\nconfig_data['local_dir'] = os.path.join(current_dir, '../..', 'output')\nconfig_data['configFile'] = config_file_path  \n\n# Log the JSON data being sent in the request\nlogging.info(f'Sending JSON data to {url}:')\nlogging.info(json.dumps(config_data, indent=2))\n\ntry:\n    # Make the POST request\n    response = requests.post(url, headers=headers, json=config_data)\n\n    # Log the response from the server\n    logging.info(f'Response from server ({url}):')\n    logging.info(f'Status Code: {response.status_code}')\n    logging.info(f'Response Text: {response.text}')\n\n    # Print the response from the server\n    print(response.text)\n\n    # Save the response to a file in the output directory\n    output_dir = os.path.join(current_dir, '../..', 'output')\n    os.makedirs(output_dir, exist_ok=True)\n    with open(os.path.join(output_dir, 'response.json'), 'w') as output_file:\n        json.dump(response.json(), output_file, indent=2)\n\nexcept Exception as e:\n    # Log any exceptions that occur\n    logging.error(f'Error occurred: {str(e)}')", 'src/services/UMLGenerator.py': 'import json\nimport os\nimport logging\nfrom services.openai_api import OpenAIAPI\n\nclass UMLGenerator:\n    def __init__(self, config_file):\n        self.api = OpenAIAPI()\n        self.OUTPUT_DIRECTORY = "src/output" \n        # Load the configuration\n        with open(config_file) as file:\n            self.config = json.load(file)\n\n    def generate_from_codes(self, file_list):\n        uml_contents = []\n        for file in file_list:\n            if self._should_skip_file(file):\n                continue\n            logging.info(f"Generating UML for file: {file}")\n            with open(file, \'r\') as f:\n                file_content = f.read()\n            uml_content = self.api.generate_from_code(file_content, os.path.basename(file))\n            if uml_content != "UML generation failed":\n                uml_contents.append(uml_content)\n            else:\n                logging.error(f"UML generation failed for file: {file}")\n        return uml_contents\n \n    def save_generated_outputs(self, uml_contents, file_names):\n        for uml_content, file_name in zip(uml_contents, file_names):\n            logging.info(f"Saving UML diagram for file: {file_name}")\n            self.api.save_generated_output(uml_content, file_name)\n\n    def _should_skip_file(self, file):\n        python_files_only = self.config.get(\'python_files_only\', False)\n        ignore_files = self.config.get(\'ignore_files\', [])\n        ignore_extensions = self.config.get(\'ignore_extensions\', [])\n        return (file in ignore_files or \n                (python_files_only and not file.endswith(\'.py\')) or \n                any(file.endswith(ext) for ext in ignore_extensions))\n\n    def _generate_from_code(self, file):\n        logging.info(f"Generating UML diagram for file: {file}")\n        return self.api.generate_from_code(file)', 'src/services/__init__.py': '', 'src/services/openai_api.py': 'import json\nimport os\nimport logging\nfrom logging import handlers\nfrom openai import OpenAI, OpenAIError\nfrom dotenv import load_dotenv\nfrom models.uml_diagram import UMLDiagram\nfrom models.git_repo import GitRepo  # Import GitRepo\n\n\n# Load environment variables from .env file\nload_dotenv()\n\n# Configure logging to write to a file\nlog_directory = \'logs\'\n# Create the directory if it doesn\'t exist\nos.makedirs(log_directory, exist_ok=True)  \nlog_filename = os.path.join(log_directory, \'openai_api.log\')\nlog_handler = handlers.RotatingFileHandler(log_filename, maxBytes=1024*1024, backupCount=5)  # Log file with rotation\nlog_formatter = logging.Formatter(\'%(asctime)s - %(levelname)s - %(message)s\')\nlog_handler.setFormatter(log_formatter)\nlogger = logging.getLogger()\nlogger.addHandler(log_handler)\nlogger.setLevel(logging.DEBUG)\n\n\nclass OpenAIAPI:\n    def __init__(self):\n        # Load environment variables from .env file\n        load_dotenv()\n\n        # Retrieve the OpenAI API key from the environment\n        openai_api_key = os.getenv("OPENAI_API_KEY")\n\n        self.client = OpenAI(api_key=openai_api_key)\n\n        # Constants\n        self.MODEL_NAME = "gpt-3.5-turbo-instruct"\n        self.MAX_TOKENS = 1024\n        self.OUTPUT_DIRECTORY = "src/output"\n\n    def generate_uml_diagram(self, repo_url, file_path, title):\n        # Clone the repository and retrieve the code\n        git_repo = GitRepo(repo_url)\n        git_repo.clone()\n        code = git_repo.retrieve_code(file_path)\n\n        uml_diagram = UMLDiagram(code, title)\n        uml_diagram.generate(self)\n        return uml_diagram\n\n    def save_uml_diagram(self, uml_diagram, file_path):\n        uml_diagram.save(self, file_path)\n\n    def write_response_to_file(self, response, filename):\n        output_dir = "output/openai"\n        os.makedirs(output_dir, exist_ok=True)  # Create the directory if it doesn\'t exist\n        with open(os.path.join(output_dir, f"{filename}.txt"), \'w\') as f:\n            f.write(response)\n\n    def generate_from_code(self, code, title):\n        # Split the code into chunks of MAX_TOKENS\n        code_chunks = [code[i:i+self.MAX_TOKENS] for i in range(0, len(code), self.MAX_TOKENS)]\n\n        puml_chunks = []\n        for chunk in code_chunks:\n            prompt_text = f"Create UML diagrams in .puml format for the following code:\\n\\n{chunk}"\n            logging.info(f"Sending prompt to OpenAI: {prompt_text}")  # Log the prompt text\n            try:\n                response = self.client.completions.create(\n                    model=self.MODEL_NAME,\n                    prompt=prompt_text,\n                    max_tokens=self.MAX_TOKENS)\n                \n                # Log the response from OpenAI\n                logging.info(f"Received response from OpenAI: {response.choices[0].text}")  # Log the response from OpenAI\n\n                # Write the response to a .txt file\n                self.write_response_to_file(response.choices[0].text, title)\n\n                # Append the generated text to the UML code\n                puml_chunks.append(response.choices[0].text.strip())\n            except OpenAIError as e:\n                logging.error(f"An error occurred while sending the prompt: {e}")  # Log the error message\n                return f"An error occurred: {e}"  # Return a message if an OpenAI API error occurs\n\n        # Combine the .puml chunks into a single string\n        combined_puml = "\\n".join(puml_chunks)\n\n        # Send a final request to OpenAI to integrate the .puml chunks\n        prompt_text = f"Create a .puml file that integrates the following .puml files:\\n\\n{combined_puml}"\n        response = self.client.completions.create(\n            model=self.MODEL_NAME,\n            prompt=prompt_text,\n            max_tokens=self.MAX_TOKENS)\n\n        # Extract the integrated .puml code from the response\n        generated_code = response.choices[0].text.strip()\n\n        # Add the @startuml, title and @enduml tags only once for each UML diagram\n        generated_code = f"@startuml\\n" + f"title {title}\\n" + generated_code + "\\n@enduml\\n"\n        return generated_code\n    \n\n    def save_generated_output(self, generated_code, file_path):\n        # Create the directory if it does not exist\n        logging.info(f"Saving generated output to {file_path}")\n        os.makedirs(os.path.dirname(file_path), exist_ok=True)\n        \n        # Write the UML code to the file\n        with open(file_path, \'w\') as file:\n            file.write(generated_code)\n            logging.info(f"Generated output saved to {file_path}")  # Optional: print out the path where the file was saved\n\n        return file_path   ', 'src/services/retrieve_code.py': 'import git\nimport json\nimport os\nimport logging\nfrom logging import handlers\n\n\n# Configure logging to write to a file\nlog_directory = \'logs\'\n# Create the directory if it doesn\'t exist\nos.makedirs(log_directory, exist_ok=True)  \nlog_filename = os.path.join(log_directory, \'retrieve_code.log\')\nlog_handler = handlers.RotatingFileHandler(log_filename, maxBytes=1024*1024, backupCount=5)  # Log file with rotation\nlog_formatter = logging.Formatter(\'%(asctime)s - %(levelname)s - %(message)s\')\nlog_handler.setFormatter(log_formatter)\nlogger = logging.getLogger()\nlogger.addHandler(log_handler)\nlogger.setLevel(logging.DEBUG)\n\n\ndef clone_repo(repo_url, temp_dir, access_token):\n    try:\n        # Modify the URL to include the access token\n        if access_token:\n            repo_url = repo_url.replace(\'https://\', f\'https://{access_token}@\')\n        logger.info(f"Cloning repository: {repo_url}")\n        repo = git.Repo.clone_from(repo_url, temp_dir)\n        logger.info(f"Successfully cloned repository: {repo_url}")\n        return repo\n    except Exception as e:\n        logger.error(f"Failed to clone repository: {str(e)}")\n        raise ValueError(f"Failed to clone repository: {str(e)}")\n\ndef retrieve_code(repo, branch_name):\n    try:\n        print(f"Attempting to checkout branch: {branch_name}")  # Diagnostic print statement\n        logger.info(f"Attempting to checkout branch: {branch_name}")\n        repo.git.fetch()  # Fetch the latest updates from the remote\n        repo.git.checkout(branch_name)\n        logger.info(f"Successfully checked out branch: {branch_name}")\n        \n        # Load the config\n        config_file_path = os.path.join(os.path.dirname(os.path.abspath(__file__)), \'config.json\')\n        with open(config_file_path, \'r\') as f:\n            config = json.load(f)\n        ignore_list = config.get(\'ignore\', [])\n        include_list = config.get(\'include\', [])\n\n        # Create a new dictionary to store file paths and their corresponding code\n        included_files = {}\n        for file in repo.tree():\n            if any(file.path.endswith(ext) for ext in include_list) and not any(ignored_file in file.path for ignored_file in ignore_list):\n                try:\n                    with open(file.abspath, \'r\') as f:\n                        included_files[file.path] = f.read()\n                    logger.info(f"Included file: {file.path}")\n                except FileNotFoundError:\n                    print(f"Ignoring missing file: {file.path}")  # Diagnostic print statement\n                    logger.warning(f"Ignoring missing file: {file.path}")\n\n        return included_files\n    except Exception as e:\n        logger.error(f"Failed to retrieve code: {str(e)}")\n        raise ValueError(f"Failed to retrieve code: {str(e)}")', 'src/utils/__init__.py': '', 'src/utils/code_to_uml.py': '# code_to_uml.py\nimport os\nfrom services.openai_api import OpenAIAPI \nimport logging\nfrom logging import handlers  \n\n# Create an instance of the OpenAI API\napi = OpenAIAPI()\n\n# Configure logging to write to a file\nlog_directory = \'logs\'\n# Create the directory if it doesn\'t exist\nos.makedirs(log_directory, exist_ok=True)  \nlog_filename = os.path.join(log_directory, \'code_to_uml.log\')\nlog_handler = handlers.RotatingFileHandler(log_filename, maxBytes=1024*1024, backupCount=5)  # Log file with rotation\nlog_formatter = logging.Formatter(\'%(asctime)s - %(levelname)s - %(message)s\')\nlog_handler.setFormatter(log_formatter)\nlogger = logging.getLogger()\nlogger.addHandler(log_handler)\nlogger.setLevel(logging.DEBUG)\n\ndef generate_content(files, output_directory):\n    logging.info(f"Files to process: {files}")  # Log the files dictionary\n    file_paths = []  # Initialize a list to store the file paths\n    for file_path, code in files.items():\n        # Skip if the file is empty\n        if not code.strip():\n            logging.info(f"Skipping empty file: {file_path}")\n            continue\n        logging.info(f"Processing file: {file_path}")\n        response = api.generate_from_code(code, os.path.basename(file_path))\n        # Extract the UML code from the response\n        generated_code_for_file = extract_uml_code(response)\n        if generated_code_for_file is None:\n            logging.error(f"Failed to generate UML diagram for {file_path}")\n            continue\n        logging.info(f"UML code generated for {file_path}: {generated_code_for_file}")  # Log the generated UML code\n\n        # Save the UML code for each file to a separate .puml file\n        file_name = f"{os.path.basename(file_path)}.puml"\n        final_output_path = api.save_generated_output(generated_code_for_file, os.path.join(output_directory, file_name))\n        file_paths.append(final_output_path)  # Append the file path to the list\n\n    logging.info(f"Generated file paths: {file_paths}")\n    # Return the list of file paths\n    return file_paths\n\ndef extract_uml_code(response):\n    # Split the response into lines\n    lines = response.split(\'\\n\')\n    # Check if \'@startuml\' and \'@enduml\' are in the response\n    if \'@startuml\' not in response or \'@enduml\' not in response:\n        logging.error("Failed to extract UML code from the response. \'@startuml\' or \'@enduml\' not found.")\n        return None\n    # Find the start and end of the UML code\n    start_index = lines.index(\'@startuml\')\n    end_index = lines.index(\'@enduml\') if \'@enduml\' in lines else -1\n    # Extract the UML code\n    uml_code = \'\\n\'.join(lines[start_index:end_index+1])\n    return uml_code'}
2024-01-23 15:29:59,603 - INFO - Skipping empty file: src/models/__init__.py
2024-01-23 15:29:59,603 - INFO - Processing file: src/models/git_repo.py
2024-01-23 15:29:59,603 - INFO - Sending prompt to OpenAI: Create UML diagrams in .puml format for the following code:

# src/models/git_repo.py
import git
import json
import shutil
import os

class GitRepo:
    def __init__(self, config_file):
        with open(config_file) as json_file:
            data = json.load(json_file)
        self.repo_url = data['gitRepoUrl']
        self.local_dir = data['local_dir']

    def clone(self):
        # Delete the directory if it exists and is not empty
        if os.path.exists(self.local_dir) and os.listdir(self.local_dir):
            shutil.rmtree(self.local_dir)
        # Clone the repository and return the local path
        # This is a simple example and doesn't handle errors
        git.Repo.clone_from(self.repo_url, self.local_dir)

    def retrieve_code(self, file_path):
        # Retrieve the code from a file in the repository
        # This is a simple example and doesn't handle errors
        with open(f'{self.local_dir}/{file_path}') as file:
            return file.read()
2024-01-23 15:29:59,604 - DEBUG - Request options: {'method': 'post', 'url': '/completions', 'files': None, 'json_data': {'model': 'gpt-3.5-turbo-instruct', 'prompt': "Create UML diagrams in .puml format for the following code:\n\n# src/models/git_repo.py\nimport git\nimport json\nimport shutil\nimport os\n\nclass GitRepo:\n    def __init__(self, config_file):\n        with open(config_file) as json_file:\n            data = json.load(json_file)\n        self.repo_url = data['gitRepoUrl']\n        self.local_dir = data['local_dir']\n\n    def clone(self):\n        # Delete the directory if it exists and is not empty\n        if os.path.exists(self.local_dir) and os.listdir(self.local_dir):\n            shutil.rmtree(self.local_dir)\n        # Clone the repository and return the local path\n        # This is a simple example and doesn't handle errors\n        git.Repo.clone_from(self.repo_url, self.local_dir)\n\n    def retrieve_code(self, file_path):\n        # Retrieve the code from a file in the repository\n        # This is a simple example and doesn't handle errors\n        with open(f'{self.local_dir}/{file_path}') as file:\n            return file.read()", 'max_tokens': 1024}}
2024-01-23 15:29:59,622 - DEBUG - connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=5.0 socket_options=None
2024-01-23 15:29:59,719 - DEBUG - connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x1106e6c10>
2024-01-23 15:29:59,720 - DEBUG - start_tls.started ssl_context=<ssl.SSLContext object at 0x10685f6e0> server_hostname='api.openai.com' timeout=5.0
2024-01-23 15:29:59,737 - DEBUG - start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x1106ccf50>
2024-01-23 15:29:59,737 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2024-01-23 15:29:59,738 - DEBUG - send_request_headers.complete
2024-01-23 15:29:59,738 - DEBUG - send_request_body.started request=<Request [b'POST']>
2024-01-23 15:29:59,738 - DEBUG - send_request_body.complete
2024-01-23 15:29:59,738 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2024-01-23 15:30:01,229 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Tue, 23 Jan 2024 21:30:01 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'Cache-Control', b'no-cache, must-revalidate'), (b'openai-model', b'gpt-3.5-turbo-instruct'), (b'openai-organization', b'user-lqvxa4hv7a8o8eyohtzmcoc6'), (b'openai-processing-ms', b'1254'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=15724800; includeSubDomains'), (b'x-ratelimit-limit-requests', b'3000'), (b'x-ratelimit-limit-tokens', b'250000'), (b'x-ratelimit-remaining-requests', b'2999'), (b'x-ratelimit-remaining-tokens', b'248975'), (b'x-ratelimit-reset-requests', b'20ms'), (b'x-ratelimit-reset-tokens', b'245ms'), (b'x-request-id', b'ec4daa454a952b83db04e1dc6ca84354'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Set-Cookie', b'__cf_bm=AI4M1iFwGU.DlJE8rO3eVxvVK19mYyfOMAXcur40Vic-1706045401-1-Aca7BsEH19MBGdvAQsURoqdQNTr/n6pauxx5RWBC7JmqqMuRbkkH/OlXhJE+/DgF1EfBvrRdm1lp+NK155C/sP8=; path=/; expires=Tue, 23-Jan-24 22:00:01 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'Set-Cookie', b'_cfuvid=E5pFzPBkmE0L6CCQzH5wnYDdeOPb_DqCVx2abSx2x34-1706045401111-0-604800000; path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'Server', b'cloudflare'), (b'CF-RAY', b'84a322a49a5e6740-ATL'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=":443"; ma=86400')])
2024-01-23 15:30:01,232 - INFO - HTTP Request: POST https://api.openai.com/v1/completions "HTTP/1.1 200 OK"
2024-01-23 15:30:01,233 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2024-01-23 15:30:01,233 - DEBUG - receive_response_body.complete
2024-01-23 15:30:01,234 - DEBUG - response_closed.started
2024-01-23 15:30:01,234 - DEBUG - response_closed.complete
2024-01-23 15:30:01,234 - DEBUG - HTTP Request: POST https://api.openai.com/v1/completions "200 OK"
2024-01-23 15:30:01,238 - INFO - Received response from OpenAI: 

@startuml

class GitRepo {
    - repo_url: String
    - local_dir: String

    + GitRepo(config_file)
    + clone()
    + retrieve_code(file_path)
}

GitRepo o-- git.Repo

namespace os {
    class path {
        + exists(directory)
    }
    class shutil {
        + rmtree(path)
    }
}

namespace json {
    class json {
        + load(file)
    }
}

namespace git {
    class Repo {
        + clone_from(url, path)
    }
}

@enduml
2024-01-23 15:30:01,241 - DEBUG - Request options: {'method': 'post', 'url': '/completions', 'files': None, 'json_data': {'model': 'gpt-3.5-turbo-instruct', 'prompt': 'Create a .puml file that integrates the following .puml files:\n\n@startuml\n\nclass GitRepo {\n    - repo_url: String\n    - local_dir: String\n\n    + GitRepo(config_file)\n    + clone()\n    + retrieve_code(file_path)\n}\n\nGitRepo o-- git.Repo\n\nnamespace os {\n    class path {\n        + exists(directory)\n    }\n    class shutil {\n        + rmtree(path)\n    }\n}\n\nnamespace json {\n    class json {\n        + load(file)\n    }\n}\n\nnamespace git {\n    class Repo {\n        + clone_from(url, path)\n    }\n}\n\n@enduml', 'max_tokens': 1024}}
2024-01-23 15:30:01,242 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2024-01-23 15:30:01,242 - DEBUG - send_request_headers.complete
2024-01-23 15:30:01,242 - DEBUG - send_request_body.started request=<Request [b'POST']>
2024-01-23 15:30:01,243 - DEBUG - send_request_body.complete
2024-01-23 15:30:01,243 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2024-01-23 15:30:06,009 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Tue, 23 Jan 2024 21:30:05 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'Cache-Control', b'no-cache, must-revalidate'), (b'openai-model', b'gpt-3.5-turbo-instruct'), (b'openai-organization', b'user-lqvxa4hv7a8o8eyohtzmcoc6'), (b'openai-processing-ms', b'4548'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=15724800; includeSubDomains'), (b'x-ratelimit-limit-requests', b'3000'), (b'x-ratelimit-limit-tokens', b'250000'), (b'x-ratelimit-remaining-requests', b'2999'), (b'x-ratelimit-remaining-tokens', b'248975'), (b'x-ratelimit-reset-requests', b'20ms'), (b'x-ratelimit-reset-tokens', b'245ms'), (b'x-request-id', b'd162ea80ac73ac3a9890f84d8462334e'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'84a322adf8646740-ATL'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=":443"; ma=86400')])
2024-01-23 15:30:06,011 - INFO - HTTP Request: POST https://api.openai.com/v1/completions "HTTP/1.1 200 OK"
2024-01-23 15:30:06,012 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2024-01-23 15:30:06,013 - DEBUG - receive_response_body.complete
2024-01-23 15:30:06,013 - DEBUG - response_closed.started
2024-01-23 15:30:06,013 - DEBUG - response_closed.complete
2024-01-23 15:30:06,014 - DEBUG - HTTP Request: POST https://api.openai.com/v1/completions "200 OK"
2024-01-23 15:30:06,015 - INFO - UML code generated for src/models/git_repo.py: @startuml
title git_repo.py
@startuml

abstract class Vehicle {
    - make: String
    - model: String
    - year: Int

    + Vehicle(selected_make, selected_model, selected_year)
    + drive()
    + park()
}

class Car {
    - num_doors: Int
    - body_style: String

    + Car(selected_make, selected_model, selected_year, selected_doors, selected_style)
    + open_doors(num_doors)
    + change_body_style(new_style)
}

Car --|> Vehicle

class Motorcycle {
    - engine_size: Int
    - top_speed: Int

    + Motorcycle(selected_make, selected_model, selected_year, selected_engine, selected_speed)
    + rev_engine()
    + update_top_speed(new_speed)
}

Motorcycle --|> Vehicle

@enduml
2024-01-23 15:30:06,016 - INFO - Saving generated output to ../../output/git_repo.py.puml
2024-01-23 15:30:06,016 - INFO - Generated output saved to ../../output/git_repo.py.puml
2024-01-23 15:30:06,017 - INFO - Processing file: src/models/uml_diagram.py
2024-01-23 15:30:06,017 - INFO - Sending prompt to OpenAI: Create UML diagrams in .puml format for the following code:

# src/models/uml_diagram.py
import os
import logging
from logging import handlers

# Configure logging to write to a file
log_directory = 'logs'
# Create the directory if it doesn't exist
os.makedirs(log_directory, exist_ok=True)  
log_filename = os.path.join(log_directory, 'models_uml_diagram.log')
log_handler = handlers.RotatingFileHandler(log_filename, maxBytes=1024*1024, backupCount=5)  # Log file with rotation
log_formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')
log_handler.setFormatter(log_formatter)
logger = logging.getLogger()
logger.addHandler(log_handler)
logger.setLevel(logging.DEBUG)

class UMLDiagram:
    def __init__(self, code, title):
        self.code = code
        self.title = title
        self.uml_code = None

    def generate(self, openai_api):
        self.uml_code = openai_api.generate_from_code(self.code, self.title)

    def save(self, openai_api, file_path):
        if self.uml_code is not None:
            openai_api.save_generated_output(self.uml_code, fi
2024-01-23 15:30:06,019 - DEBUG - Request options: {'method': 'post', 'url': '/completions', 'files': None, 'json_data': {'model': 'gpt-3.5-turbo-instruct', 'prompt': "Create UML diagrams in .puml format for the following code:\n\n# src/models/uml_diagram.py\nimport os\nimport logging\nfrom logging import handlers\n\n# Configure logging to write to a file\nlog_directory = 'logs'\n# Create the directory if it doesn't exist\nos.makedirs(log_directory, exist_ok=True)  \nlog_filename = os.path.join(log_directory, 'models_uml_diagram.log')\nlog_handler = handlers.RotatingFileHandler(log_filename, maxBytes=1024*1024, backupCount=5)  # Log file with rotation\nlog_formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\nlog_handler.setFormatter(log_formatter)\nlogger = logging.getLogger()\nlogger.addHandler(log_handler)\nlogger.setLevel(logging.DEBUG)\n\nclass UMLDiagram:\n    def __init__(self, code, title):\n        self.code = code\n        self.title = title\n        self.uml_code = None\n\n    def generate(self, openai_api):\n        self.uml_code = openai_api.generate_from_code(self.code, self.title)\n\n    def save(self, openai_api, file_path):\n        if self.uml_code is not None:\n            openai_api.save_generated_output(self.uml_code, fi", 'max_tokens': 1024}}
2024-01-23 15:30:06,020 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2024-01-23 15:30:06,021 - DEBUG - send_request_headers.complete
2024-01-23 15:30:06,021 - DEBUG - send_request_body.started request=<Request [b'POST']>
2024-01-23 15:30:06,021 - DEBUG - send_request_body.complete
2024-01-23 15:30:06,021 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2024-01-23 15:30:08,797 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Tue, 23 Jan 2024 21:30:08 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'Cache-Control', b'no-cache, must-revalidate'), (b'openai-model', b'gpt-3.5-turbo-instruct'), (b'openai-organization', b'user-lqvxa4hv7a8o8eyohtzmcoc6'), (b'openai-processing-ms', b'2293'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=15724800; includeSubDomains'), (b'x-ratelimit-limit-requests', b'3000'), (b'x-ratelimit-limit-tokens', b'250000'), (b'x-ratelimit-remaining-requests', b'2999'), (b'x-ratelimit-remaining-tokens', b'248975'), (b'x-ratelimit-reset-requests', b'20ms'), (b'x-ratelimit-reset-tokens', b'245ms'), (b'x-request-id', b'f4c01ff88a13da4ab2aa0b31d65697cd'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'84a322cbdce76740-ATL'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=":443"; ma=86400')])
2024-01-23 15:30:08,798 - INFO - HTTP Request: POST https://api.openai.com/v1/completions "HTTP/1.1 200 OK"
2024-01-23 15:30:08,799 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2024-01-23 15:30:08,799 - DEBUG - receive_response_body.complete
2024-01-23 15:30:08,799 - DEBUG - response_closed.started
2024-01-23 15:30:08,799 - DEBUG - response_closed.complete
2024-01-23 15:30:08,800 - DEBUG - HTTP Request: POST https://api.openai.com/v1/completions "200 OK"
2024-01-23 15:30:08,801 - INFO - Received response from OpenAI: le_path)

@startuml
class UMLDiagram {
    - code: str
    - title: str
    - uml_code: str
    + __init__(code: str, title: str)
    + generate(openai_api)
    + save(openai_api, file_path)
}
@enduml

@startuml
package models {
    class UMLDiagram {
        - code: str
        - title: str
        - uml_code: str
        + __init__(code: str, title: str)
        + generate(openai_api)
        + save(openai_api, file_path)
    }
}
@enduml

@startuml
namespace models {
    class UMLDiagram {
        - code: str
        - title: str
        - uml_code: str
        + __init__(code: str, title: str)
        + generate(openai_api)
        + save(openai_api, file_path)
    }
}
@enduml
2024-01-23 15:30:08,802 - INFO - Sending prompt to OpenAI: Create UML diagrams in .puml format for the following code:

le_path)
2024-01-23 15:30:08,803 - DEBUG - Request options: {'method': 'post', 'url': '/completions', 'files': None, 'json_data': {'model': 'gpt-3.5-turbo-instruct', 'prompt': 'Create UML diagrams in .puml format for the following code:\n\nle_path)', 'max_tokens': 1024}}
2024-01-23 15:30:08,804 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2024-01-23 15:30:08,804 - DEBUG - send_request_headers.complete
2024-01-23 15:30:08,804 - DEBUG - send_request_body.started request=<Request [b'POST']>
2024-01-23 15:30:08,805 - DEBUG - send_request_body.complete
2024-01-23 15:30:08,805 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2024-01-23 15:30:10,816 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Tue, 23 Jan 2024 21:30:10 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'Cache-Control', b'no-cache, must-revalidate'), (b'openai-model', b'gpt-3.5-turbo-instruct'), (b'openai-organization', b'user-lqvxa4hv7a8o8eyohtzmcoc6'), (b'openai-processing-ms', b'1417'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=15724800; includeSubDomains'), (b'x-ratelimit-limit-requests', b'3000'), (b'x-ratelimit-limit-tokens', b'250000'), (b'x-ratelimit-remaining-requests', b'2999'), (b'x-ratelimit-remaining-tokens', b'248975'), (b'x-ratelimit-reset-requests', b'20ms'), (b'x-ratelimit-reset-tokens', b'245ms'), (b'x-request-id', b'46d25aa15feabd4934080d7376ced1b9'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'84a322dd3ebc6740-ATL'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=":443"; ma=86400')])
2024-01-23 15:30:10,816 - INFO - HTTP Request: POST https://api.openai.com/v1/completions "HTTP/1.1 200 OK"
2024-01-23 15:30:10,816 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2024-01-23 15:30:10,816 - DEBUG - receive_response_body.complete
2024-01-23 15:30:10,816 - DEBUG - response_closed.started
2024-01-23 15:30:10,817 - DEBUG - response_closed.complete
2024-01-23 15:30:10,817 - DEBUG - HTTP Request: POST https://api.openai.com/v1/completions "200 OK"
2024-01-23 15:30:10,817 - INFO - Received response from OpenAI:  float:
//puml<

Class Code1:
   INumber1
   INumber2
   some readOnly float

   // puml<
   (number1, number2, important_string): Code1
   number1: = inputNumber1
   number2: = inputNumber2
   some: = important_string
   
   + performOperation(): float
end Class

Code1: ----------------
|____________________|
|____________________|
| number1: float     |
| number2: float     |
| some: float        |
|____________________|
| performOperation() |
|____________________|

puml>
2024-01-23 15:30:10,818 - DEBUG - Request options: {'method': 'post', 'url': '/completions', 'files': None, 'json_data': {'model': 'gpt-3.5-turbo-instruct', 'prompt': 'Create a .puml file that integrates the following .puml files:\n\nle_path)\n\n@startuml\nclass UMLDiagram {\n    - code: str\n    - title: str\n    - uml_code: str\n    + __init__(code: str, title: str)\n    + generate(openai_api)\n    + save(openai_api, file_path)\n}\n@enduml\n\n@startuml\npackage models {\n    class UMLDiagram {\n        - code: str\n        - title: str\n        - uml_code: str\n        + __init__(code: str, title: str)\n        + generate(openai_api)\n        + save(openai_api, file_path)\n    }\n}\n@enduml\n\n@startuml\nnamespace models {\n    class UMLDiagram {\n        - code: str\n        - title: str\n        - uml_code: str\n        + __init__(code: str, title: str)\n        + generate(openai_api)\n        + save(openai_api, file_path)\n    }\n}\n@enduml\nfloat:\n//puml<\n\nClass Code1:\n   INumber1\n   INumber2\n   some readOnly float\n\n   // puml<\n   (number1, number2, important_string): Code1\n   number1: = inputNumber1\n   number2: = inputNumber2\n   some: = important_string\n   \n   + performOperation(): float\nend Class\n\nCode1: ----------------\n|____________________|\n|____________________|\n| number1: float     |\n| number2: float     |\n| some: float        |\n|____________________|\n| performOperation() |\n|____________________|\n\npuml>', 'max_tokens': 1024}}
2024-01-23 15:30:10,819 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2024-01-23 15:30:10,819 - DEBUG - send_request_headers.complete
2024-01-23 15:30:10,819 - DEBUG - send_request_body.started request=<Request [b'POST']>
2024-01-23 15:30:10,819 - DEBUG - send_request_body.complete
2024-01-23 15:30:10,820 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2024-01-23 15:30:12,787 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Tue, 23 Jan 2024 21:30:12 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'Cache-Control', b'no-cache, must-revalidate'), (b'openai-model', b'gpt-3.5-turbo-instruct'), (b'openai-organization', b'user-lqvxa4hv7a8o8eyohtzmcoc6'), (b'openai-processing-ms', b'1501'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=15724800; includeSubDomains'), (b'x-ratelimit-limit-requests', b'3000'), (b'x-ratelimit-limit-tokens', b'250000'), (b'x-ratelimit-remaining-requests', b'2999'), (b'x-ratelimit-remaining-tokens', b'248975'), (b'x-ratelimit-reset-requests', b'20ms'), (b'x-ratelimit-reset-tokens', b'245ms'), (b'x-request-id', b'c61610deb3f7ec92c048102aea2d2f79'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'84a322e9f99d6740-ATL'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=":443"; ma=86400')])
2024-01-23 15:30:12,787 - INFO - HTTP Request: POST https://api.openai.com/v1/completions "HTTP/1.1 200 OK"
2024-01-23 15:30:12,787 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2024-01-23 15:30:12,787 - DEBUG - receive_response_body.complete
2024-01-23 15:30:12,788 - DEBUG - response_closed.started
2024-01-23 15:30:12,788 - DEBUG - response_closed.complete
2024-01-23 15:30:12,788 - DEBUG - HTTP Request: POST https://api.openai.com/v1/completions "200 OK"
2024-01-23 15:30:12,789 - INFO - UML code generated for src/models/uml_diagram.py: @startuml
title uml_diagram.py
@startuml
package models {
    class UMLDiagram {
        - code: str
        - title: str
        - uml_code: str
        + __init__(code: str, title: str)
        + generate(openai_api)
        + save(openai_api, file_path)
    }

    class Code1 {
        - INumber1: float
        - INumber2: float
        - some: float
        + __init__(number1: float, number2: float, important_string: str)
        + performOperation(): float
    }

    UMLDiagram -- Code1

}
@enduml
2024-01-23 15:30:12,789 - INFO - Saving generated output to ../../output/uml_diagram.py.puml
2024-01-23 15:30:12,789 - INFO - Generated output saved to ../../output/uml_diagram.py.puml
2024-01-23 15:30:12,789 - INFO - Skipping empty file: src/routes/__init__.py
2024-01-23 15:30:12,789 - INFO - Processing file: src/routes/uml_from_repo.py
2024-01-23 15:30:12,789 - INFO - Sending prompt to OpenAI: Create UML diagrams in .puml format for the following code:

# uml_from_repo.py
import os
import fnmatch
import subprocess
import json
import git
import tempfile
import shutil
import logging
from logging import handlers  
from services.retrieve_code import clone_repo, retrieve_code
from utils.code_to_uml import generate_content  # Import the function from code_to_uml.py

 
# Configure logging to write to a file
log_directory = 'logs'
# Create the directory if it doesn't exist
os.makedirs(log_directory, exist_ok=True)  
log_filename = os.path.join(log_directory, 'uml_from_repo.log')
log_handler = handlers.RotatingFileHandler(log_filename, maxBytes=1024*1024, backupCount=5)  # Log file with rotation
log_formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')
log_handler.setFormatter(log_formatter)
logger = logging.getLogger()
logger.addHandler(log_handler)
logger.setLevel(logging.DEBUG)

def process_request(git_repo, github_access_token):
    output_directory = git_repo.local_dir  # Get the output directory from the GitRepo object
    branch_name = 'ma
2024-01-23 15:30:12,790 - DEBUG - Request options: {'method': 'post', 'url': '/completions', 'files': None, 'json_data': {'model': 'gpt-3.5-turbo-instruct', 'prompt': "Create UML diagrams in .puml format for the following code:\n\n# uml_from_repo.py\nimport os\nimport fnmatch\nimport subprocess\nimport json\nimport git\nimport tempfile\nimport shutil\nimport logging\nfrom logging import handlers  \nfrom services.retrieve_code import clone_repo, retrieve_code\nfrom utils.code_to_uml import generate_content  # Import the function from code_to_uml.py\n\n \n# Configure logging to write to a file\nlog_directory = 'logs'\n# Create the directory if it doesn't exist\nos.makedirs(log_directory, exist_ok=True)  \nlog_filename = os.path.join(log_directory, 'uml_from_repo.log')\nlog_handler = handlers.RotatingFileHandler(log_filename, maxBytes=1024*1024, backupCount=5)  # Log file with rotation\nlog_formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\nlog_handler.setFormatter(log_formatter)\nlogger = logging.getLogger()\nlogger.addHandler(log_handler)\nlogger.setLevel(logging.DEBUG)\n\ndef process_request(git_repo, github_access_token):\n    output_directory = git_repo.local_dir  # Get the output directory from the GitRepo object\n    branch_name = 'ma", 'max_tokens': 1024}}
2024-01-23 15:30:12,791 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2024-01-23 15:30:12,791 - DEBUG - send_request_headers.complete
2024-01-23 15:30:12,791 - DEBUG - send_request_body.started request=<Request [b'POST']>
2024-01-23 15:30:12,791 - DEBUG - send_request_body.complete
2024-01-23 15:30:12,791 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2024-01-23 15:30:18,523 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Tue, 23 Jan 2024 21:30:18 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'Cache-Control', b'no-cache, must-revalidate'), (b'openai-model', b'gpt-3.5-turbo-instruct'), (b'openai-organization', b'user-lqvxa4hv7a8o8eyohtzmcoc6'), (b'openai-processing-ms', b'5614'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=15724800; includeSubDomains'), (b'x-ratelimit-limit-requests', b'3000'), (b'x-ratelimit-limit-tokens', b'250000'), (b'x-ratelimit-remaining-requests', b'2999'), (b'x-ratelimit-remaining-tokens', b'248975'), (b'x-ratelimit-reset-requests', b'20ms'), (b'x-ratelimit-reset-tokens', b'245ms'), (b'x-request-id', b'7cc9bcc40b2c2ab19a32bc7e5cdb8a04'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'84a322f62dc66740-ATL'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=":443"; ma=86400')])
2024-01-23 15:30:18,524 - INFO - HTTP Request: POST https://api.openai.com/v1/completions "HTTP/1.1 200 OK"
2024-01-23 15:30:18,524 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2024-01-23 15:30:18,524 - DEBUG - receive_response_body.complete
2024-01-23 15:30:18,524 - DEBUG - response_closed.started
2024-01-23 15:30:18,524 - DEBUG - response_closed.complete
2024-01-23 15:30:18,524 - DEBUG - HTTP Request: POST https://api.openai.com/v1/completions "200 OK"
2024-01-23 15:30:18,525 - INFO - Received response from OpenAI: in'
    
    print("Cloning repository: {}".format(git_repo.url))
    clone_repo(git_repo.url, output_directory, branch_name, github_access_token)  # Clone the repository
    print("Repository cloned successfully!")
    
    print("Retrieving code from repository...")
    repo_dir, repo_name = retrieve_code(output_directory, branch_name)  # Retrieve the code from the output directory
    print("Code retrieved successfully!")
    
    # Convert code to UML format
    print("Generating UML diagrams...")
    uml_data = generate_content(repo_dir, repo_name)  # Use the generate_content function to create UML diagrams
    print("UML diagrams generated!")
    
    # Save UML data to a file
    print("Saving UML data to file...")
    with open(os.path.join(output_directory, 'uml_data.json'), 'w') as f:
        json.dump(uml_data, f)  # Save the UML data to a JSON file
    print("UML data saved to file successfully!")


@startuml

class GitRepo {
    - url: str
    ~ local_dir: str
    + GitRepo(url, local_dir)
    + get_url(): str
    + get_local_dir(): str
}

class logging {
    - log_directory: str
    - log_filename: str
    - log_handler: handlers.RotatingFileHandler
    - log_formatter: logging.Formatter
    ~ logger: logging.Logger
    + configure(log_directory, log_filename, maxBytes, backupCount): void
    + process_request(git_repo, github_access_token): void
}

class retrieve_code {
    + clone_repo(url, output_directory, branch_name, github_access_token): void
    + retrieve_code(output_directory, branch_name): tuple
}

class code_to_uml {
    ~ process(source): void
    + generate_content(repo_dir, repo_name): object
}

class main {
    - git_repo: GitRepo
    - github_access_token: str
    - output_directory: str
    - branch_name: str
    + main(): void
    - process_request(git_repo, github_access_token): void
}

main --|> GitRepo
main --|> logging 
main --|> retrieve_code
main --|> code_to_uml
main .> logging : uses
main .> retrieve_code : uses
main .> code_to_uml : uses
main ..> git_repo : contains
main.Behavior : process_request(git_repo, github_access_token)

@enduml
2024-01-23 15:30:18,525 - INFO - Sending prompt to OpenAI: Create UML diagrams in .puml format for the following code:

ster'  # 'master' is the default branch name
    temp_dir = None  # Initialize temp_dir outside the try block

    try:
        if not git_repo.repo_url or not output_directory or not github_access_token or not branch_name:
            logger.error("Missing required parameters")  
            return {"error": "Missing required parameters"}, 400

        try:
            temp_dir = tempfile.mkdtemp()
        except Exception as e:
            logger.error(f"Error during UML generation: {str(e)}", exc_info=True)
            return {"error": str(e)}, 500

        # Check if the directory exists and remove it
        if os.path.exists(output_directory):
            shutil.rmtree(output_directory)

        # Clone the repository
        repo = git.Repo.clone_from(git_repo.repo_url, output_directory)

        # Load the config
        with open('src/routes/config.json', 'r') as f:
            config = json.load(f)
        
        def traverse_directories(repo, git_repo, config):
            included_files = {}
   
2024-01-23 15:30:18,526 - DEBUG - Request options: {'method': 'post', 'url': '/completions', 'files': None, 'json_data': {'model': 'gpt-3.5-turbo-instruct', 'prompt': 'Create UML diagrams in .puml format for the following code:\n\nster\'  # \'master\' is the default branch name\n    temp_dir = None  # Initialize temp_dir outside the try block\n\n    try:\n        if not git_repo.repo_url or not output_directory or not github_access_token or not branch_name:\n            logger.error("Missing required parameters")  \n            return {"error": "Missing required parameters"}, 400\n\n        try:\n            temp_dir = tempfile.mkdtemp()\n        except Exception as e:\n            logger.error(f"Error during UML generation: {str(e)}", exc_info=True)\n            return {"error": str(e)}, 500\n\n        # Check if the directory exists and remove it\n        if os.path.exists(output_directory):\n            shutil.rmtree(output_directory)\n\n        # Clone the repository\n        repo = git.Repo.clone_from(git_repo.repo_url, output_directory)\n\n        # Load the config\n        with open(\'src/routes/config.json\', \'r\') as f:\n            config = json.load(f)\n        \n        def traverse_directories(repo, git_repo, config):\n            included_files = {}\n   ', 'max_tokens': 1024}}
2024-01-23 15:30:18,526 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2024-01-23 15:30:18,527 - DEBUG - send_request_headers.complete
2024-01-23 15:30:18,527 - DEBUG - send_request_body.started request=<Request [b'POST']>
2024-01-23 15:30:18,527 - DEBUG - send_request_body.complete
2024-01-23 15:30:18,527 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2024-01-23 15:30:24,300 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Tue, 23 Jan 2024 21:30:24 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'Cache-Control', b'no-cache, must-revalidate'), (b'openai-model', b'gpt-3.5-turbo-instruct'), (b'openai-organization', b'user-lqvxa4hv7a8o8eyohtzmcoc6'), (b'openai-processing-ms', b'5669'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=15724800; includeSubDomains'), (b'x-ratelimit-limit-requests', b'3000'), (b'x-ratelimit-limit-tokens', b'250000'), (b'x-ratelimit-remaining-requests', b'2999'), (b'x-ratelimit-remaining-tokens', b'248975'), (b'x-ratelimit-reset-requests', b'20ms'), (b'x-ratelimit-reset-tokens', b'245ms'), (b'x-request-id', b'8848a13c472f5ff090967bc01eb41496'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'84a3231a2c076740-ATL'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=":443"; ma=86400')])
2024-01-23 15:30:24,304 - INFO - HTTP Request: POST https://api.openai.com/v1/completions "HTTP/1.1 200 OK"
2024-01-23 15:30:24,305 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2024-01-23 15:30:24,306 - DEBUG - receive_response_body.complete
2024-01-23 15:30:24,306 - DEBUG - response_closed.started
2024-01-23 15:30:24,307 - DEBUG - response_closed.complete
2024-01-23 15:30:24,307 - DEBUG - HTTP Request: POST https://api.openai.com/v1/completions "200 OK"
2024-01-23 15:30:24,309 - INFO - Received response from OpenAI:  </nav>
    
@startuml

class GithubRepo { 
    - repo_url: string 
} 
class Temp { 
    - temp_dir: None 
} 
class GithubAccess { 
    - github_access_token: string 
} 
class GitRepo { 
    - repo_url: string 
    - output_directory: string 
    - branch_name: string 
} 
class Logger { 
    + error(message: string) 
} 
class UMLGenerator { 
    + generateUML(repo: GitRepo, github_repo: GithubRepo, temp: Temp, github_access: GithubAccess) 
    + traverse_directories(repo: git.Repo, git_repo: GitRepo, config: json) 
} 
class OutputDirectory { 
    - output_directory: string 
} 
class Tempfile { 
    + mkdtemp() 
} 
class Exception { 
    - message: string 
} 
class JsonFile { 
    - config: json 
} 

UMLGenerator *- github_repo: GithubRepo 
UMLGenerator *- temp: Temp 
UMLGenerator *- github_access: GithubAccess 
UMLGenerator *- repo: GitRepo 
UMLGenerator *- logger: Logger 
UMLGenerator *- temp_file: Tempfile 

UMLGenerator --> repo: GitRepo 
UMLGenerator ..> logger: Logger 
UMLGenerator ..> temp_file: Tempfile 

GithubRepo ..> repo: GitRepo 

Temp ..> temp_dir: None 

GithubAccess ..> github_access_token: string 

Logger ..> error(message) 

GitRepo *- repo: git.Repo 
GitRepo *- output_directory: OutputDirectory 

OutputDirectory --> output_directory: string 

JsonFile --> config: json 

OutputDirectory ..> repo: GitRepo 

Temp *- temp_file: Tempfile 

Tempfile --> mkdtemp() 

exception Exception 

GithubRepo: -repo_url : string

GitRepo : -repo_url: string
GitRepo : -output_directory: string
GitRepo : -branch_name: stirng
GitRepo --> git:Repo

GithubAccess: -github_access_token: string

outputDirectory: -output_directory: string

Tempfile: +mkdtemp()

Logger: +error(message: string)

JsonFile: -config: json
JsonFile ..> GitRepo : obj key

GitRepo ..> exception : Catch

@enduml
2024-01-23 15:30:24,310 - INFO - Sending prompt to OpenAI: Create UML diagrams in .puml format for the following code:

         for item in repo.tree().traverse():
                if item.type == 'blob':  # This means it's a file, not a directory
                    for pattern in config['include']:
                        if fnmatch.fnmatch(item.path, pattern):
                            with open(os.path.join(git_repo.local_dir, item.path), 'r') as f:
                                included_files[item.path] = f.read()
                            break  # No need to check the remaining patterns
            return included_files

        # Retrieve the code from the repository
        included_files = traverse_directories(repo, git_repo, config)

        logger.debug(f"Type of included_files: {type(included_files)}")
        logger.debug(f"Value of included_files: {included_files}")

        # Save the UML diagram to a file
        logger.info(f"Saving UML diagram to {output_directory}")
        final_output_paths = generate_content(included_files, output_directory)  # This is now a list of file paths
        logger.info(f"
2024-01-23 15:30:24,312 - DEBUG - Request options: {'method': 'post', 'url': '/completions', 'files': None, 'json_data': {'model': 'gpt-3.5-turbo-instruct', 'prompt': 'Create UML diagrams in .puml format for the following code:\n\n         for item in repo.tree().traverse():\n                if item.type == \'blob\':  # This means it\'s a file, not a directory\n                    for pattern in config[\'include\']:\n                        if fnmatch.fnmatch(item.path, pattern):\n                            with open(os.path.join(git_repo.local_dir, item.path), \'r\') as f:\n                                included_files[item.path] = f.read()\n                            break  # No need to check the remaining patterns\n            return included_files\n\n        # Retrieve the code from the repository\n        included_files = traverse_directories(repo, git_repo, config)\n\n        logger.debug(f"Type of included_files: {type(included_files)}")\n        logger.debug(f"Value of included_files: {included_files}")\n\n        # Save the UML diagram to a file\n        logger.info(f"Saving UML diagram to {output_directory}")\n        final_output_paths = generate_content(included_files, output_directory)  # This is now a list of file paths\n        logger.info(f"', 'max_tokens': 1024}}
2024-01-23 15:30:24,314 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2024-01-23 15:30:24,315 - DEBUG - send_request_headers.complete
2024-01-23 15:30:24,315 - DEBUG - send_request_body.started request=<Request [b'POST']>
2024-01-23 15:30:24,315 - DEBUG - send_request_body.complete
2024-01-23 15:30:24,315 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2024-01-23 15:30:28,121 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Tue, 23 Jan 2024 21:30:28 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'Cache-Control', b'no-cache, must-revalidate'), (b'openai-model', b'gpt-3.5-turbo-instruct'), (b'openai-organization', b'user-lqvxa4hv7a8o8eyohtzmcoc6'), (b'openai-processing-ms', b'3603'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=15724800; includeSubDomains'), (b'x-ratelimit-limit-requests', b'3000'), (b'x-ratelimit-limit-tokens', b'250000'), (b'x-ratelimit-remaining-requests', b'2999'), (b'x-ratelimit-remaining-tokens', b'248975'), (b'x-ratelimit-reset-requests', b'20ms'), (b'x-ratelimit-reset-tokens', b'245ms'), (b'x-request-id', b'520dc39c88ef774ffb65aefe12179353'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'84a3233e3cb26740-ATL'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=":443"; ma=86400')])
2024-01-23 15:30:28,122 - INFO - HTTP Request: POST https://api.openai.com/v1/completions "HTTP/1.1 200 OK"
2024-01-23 15:30:28,122 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2024-01-23 15:30:28,122 - DEBUG - receive_response_body.complete
2024-01-23 15:30:28,122 - DEBUG - response_closed.started
2024-01-23 15:30:28,122 - DEBUG - response_closed.complete
2024-01-23 15:30:28,123 - DEBUG - HTTP Request: POST https://api.openai.com/v1/completions "200 OK"
2024-01-23 15:30:28,124 - INFO - Received response from OpenAI: Generated content: {final_output_paths})

@startuml
class Repo
class Tree
class Config
class Include
class Fnmatch
class Path
class GitRepo
class Logger
class OutputDirectory

Repo --> Tree : tree()
Tree --> Traversal : .traverse()
Traversal --> Item : item in repo.tree().traverse()
Item --> Type : item.type 
Type --> If : ==
If --> Blob : 'blob'
Blob --> File : file
If --> Pattern : pattern in config['include']
Pattern --> Fnmatch : fnmatch.fnmatch()
Fnmatch --> Item : item.path
Item --> Open : open()
Open --> Path : os.path.join
Path --> GitRepo : git_repo.local_dir
GitRepo --> Read : read()
Read --> IncludedFiles : included_files[item.path]
IncludedFiles --> Break : break
Break --> Return : return included_files
Return --> IncludedFiles : included_files = traverse_directories(repo, git_repo, config)
IncludedFiles --> Logger : logger.debug()
Logger --> Type : type(included_files)
Logger --> Value : value(included_files)
Logger --> Save : logger.info()
Save --> OutputDirectory : output_directory
OutputDirectory --> Generate : generate_content()
Generate --> OutputPath : final_output_paths
OutputPath --> Return : return final_output_paths
Logger --> Message : logger.info()
Message --> FinalOutputPaths
2024-01-23 15:30:28,125 - INFO - Sending prompt to OpenAI: Create UML diagrams in .puml format for the following code:

Final output paths: {final_output_paths}")

        for path in final_output_paths:
            logger.info(f"UML diagram saved at: {path}")  # Log the path where each UML diagram was saved

        return {
            "message": "UML diagrams generated successfully",
            "details": {
                "Repository": git_repo.repo_url,
                "Output Paths": final_output_paths  # This is now a list of file paths
            }
        }, 200

    except Exception as e:
        logger.error(f"Error during UML generation: {str(e)}")
        return {"error": str(e)}, 500

    finally:
        # Clean up the temporary directory
        if temp_dir is not None:
            logger.info("Cleaning up temporary directory")
            shutil.rmtree(temp_dir)

2024-01-23 15:30:28,126 - DEBUG - Request options: {'method': 'post', 'url': '/completions', 'files': None, 'json_data': {'model': 'gpt-3.5-turbo-instruct', 'prompt': 'Create UML diagrams in .puml format for the following code:\n\nFinal output paths: {final_output_paths}")\n\n        for path in final_output_paths:\n            logger.info(f"UML diagram saved at: {path}")  # Log the path where each UML diagram was saved\n\n        return {\n            "message": "UML diagrams generated successfully",\n            "details": {\n                "Repository": git_repo.repo_url,\n                "Output Paths": final_output_paths  # This is now a list of file paths\n            }\n        }, 200\n\n    except Exception as e:\n        logger.error(f"Error during UML generation: {str(e)}")\n        return {"error": str(e)}, 500\n\n    finally:\n        # Clean up the temporary directory\n        if temp_dir is not None:\n            logger.info("Cleaning up temporary directory")\n            shutil.rmtree(temp_dir)\n', 'max_tokens': 1024}}
2024-01-23 15:30:28,127 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2024-01-23 15:30:28,127 - DEBUG - send_request_headers.complete
2024-01-23 15:30:28,128 - DEBUG - send_request_body.started request=<Request [b'POST']>
2024-01-23 15:30:28,128 - DEBUG - send_request_body.complete
2024-01-23 15:30:28,128 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2024-01-23 15:30:31,706 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Tue, 23 Jan 2024 21:30:31 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'Cache-Control', b'no-cache, must-revalidate'), (b'openai-model', b'gpt-3.5-turbo-instruct'), (b'openai-organization', b'user-lqvxa4hv7a8o8eyohtzmcoc6'), (b'openai-processing-ms', b'3376'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=15724800; includeSubDomains'), (b'x-ratelimit-limit-requests', b'3000'), (b'x-ratelimit-limit-tokens', b'250000'), (b'x-ratelimit-remaining-requests', b'2999'), (b'x-ratelimit-remaining-tokens', b'248975'), (b'x-ratelimit-reset-requests', b'20ms'), (b'x-ratelimit-reset-tokens', b'245ms'), (b'x-request-id', b'c2aa6a32f642a0fb0c551aba0678cce0'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'84a323562efa6740-ATL'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=":443"; ma=86400')])
2024-01-23 15:30:31,707 - INFO - HTTP Request: POST https://api.openai.com/v1/completions "HTTP/1.1 200 OK"
2024-01-23 15:30:31,707 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2024-01-23 15:30:31,708 - DEBUG - receive_response_body.complete
2024-01-23 15:30:31,708 - DEBUG - response_closed.started
2024-01-23 15:30:31,708 - DEBUG - response_closed.complete
2024-01-23 15:30:31,708 - DEBUG - HTTP Request: POST https://api.openai.com/v1/completions "200 OK"
2024-01-23 15:30:31,710 - INFO - Received response from OpenAI:             
####UML Diagram####
```
@startuml
object UMLGenerator {
    __init__(logger=None, config_path=None)
    +finalize()
    -search_git_repo_config(config_path)
    -git_repo
    #BASE_IMAGE_URL = "https://github.com/{git_username}/{repo_name}.git"

    #temp_dir
    +{ git_repo }
    +{ logger }
    +{ config_path }
    +log_source_files()
    +generate_uml()
    +create_puml()
    +search_for_github_repo()
    +get_output_path()
    +save_puml()
    +return_results()
}

class git_repo {
    repo_url
}

class logger {
    log(message)
    info(message)
    error(message)
}

UMLGenerator --> git_repo
UMLGenerator --> logger

git_repo <-- UMLGenerator
logger <-- UMLGenerator
logger --> UMLGenerator.finalize()

UMLGenerator : __init__(config_path)
UMLGenerator : search_git_repo_config(config_path)
UMLGenerator : __init__(logger)

UMLGenerator : log_source_files()
UMLGenerator : generate_uml()
UMLGenerator : create_puml()
UMLGenerator : search_for_github_repo()
UMLGenerator : get_output_path()
UMLGenerator : save_puml()
UMLGenerator : return_results()

@enduml
2024-01-23 15:30:31,712 - DEBUG - Request options: {'method': 'post', 'url': '/completions', 'files': None, 'json_data': {'model': 'gpt-3.5-turbo-instruct', 'prompt': 'Create a .puml file that integrates the following .puml files:\n\nin\'\n    \n    print("Cloning repository: {}".format(git_repo.url))\n    clone_repo(git_repo.url, output_directory, branch_name, github_access_token)  # Clone the repository\n    print("Repository cloned successfully!")\n    \n    print("Retrieving code from repository...")\n    repo_dir, repo_name = retrieve_code(output_directory, branch_name)  # Retrieve the code from the output directory\n    print("Code retrieved successfully!")\n    \n    # Convert code to UML format\n    print("Generating UML diagrams...")\n    uml_data = generate_content(repo_dir, repo_name)  # Use the generate_content function to create UML diagrams\n    print("UML diagrams generated!")\n    \n    # Save UML data to a file\n    print("Saving UML data to file...")\n    with open(os.path.join(output_directory, \'uml_data.json\'), \'w\') as f:\n        json.dump(uml_data, f)  # Save the UML data to a JSON file\n    print("UML data saved to file successfully!")\n\n\n@startuml\n\nclass GitRepo {\n    - url: str\n    ~ local_dir: str\n    + GitRepo(url, local_dir)\n    + get_url(): str\n    + get_local_dir(): str\n}\n\nclass logging {\n    - log_directory: str\n    - log_filename: str\n    - log_handler: handlers.RotatingFileHandler\n    - log_formatter: logging.Formatter\n    ~ logger: logging.Logger\n    + configure(log_directory, log_filename, maxBytes, backupCount): void\n    + process_request(git_repo, github_access_token): void\n}\n\nclass retrieve_code {\n    + clone_repo(url, output_directory, branch_name, github_access_token): void\n    + retrieve_code(output_directory, branch_name): tuple\n}\n\nclass code_to_uml {\n    ~ process(source): void\n    + generate_content(repo_dir, repo_name): object\n}\n\nclass main {\n    - git_repo: GitRepo\n    - github_access_token: str\n    - output_directory: str\n    - branch_name: str\n    + main(): void\n    - process_request(git_repo, github_access_token): void\n}\n\nmain --|> GitRepo\nmain --|> logging \nmain --|> retrieve_code\nmain --|> code_to_uml\nmain .> logging : uses\nmain .> retrieve_code : uses\nmain .> code_to_uml : uses\nmain ..> git_repo : contains\nmain.Behavior : process_request(git_repo, github_access_token)\n\n@enduml\n</nav>\n    \n@startuml\n\nclass GithubRepo { \n    - repo_url: string \n} \nclass Temp { \n    - temp_dir: None \n} \nclass GithubAccess { \n    - github_access_token: string \n} \nclass GitRepo { \n    - repo_url: string \n    - output_directory: string \n    - branch_name: string \n} \nclass Logger { \n    + error(message: string) \n} \nclass UMLGenerator { \n    + generateUML(repo: GitRepo, github_repo: GithubRepo, temp: Temp, github_access: GithubAccess) \n    + traverse_directories(repo: git.Repo, git_repo: GitRepo, config: json) \n} \nclass OutputDirectory { \n    - output_directory: string \n} \nclass Tempfile { \n    + mkdtemp() \n} \nclass Exception { \n    - message: string \n} \nclass JsonFile { \n    - config: json \n} \n\nUMLGenerator *- github_repo: GithubRepo \nUMLGenerator *- temp: Temp \nUMLGenerator *- github_access: GithubAccess \nUMLGenerator *- repo: GitRepo \nUMLGenerator *- logger: Logger \nUMLGenerator *- temp_file: Tempfile \n\nUMLGenerator --> repo: GitRepo \nUMLGenerator ..> logger: Logger \nUMLGenerator ..> temp_file: Tempfile \n\nGithubRepo ..> repo: GitRepo \n\nTemp ..> temp_dir: None \n\nGithubAccess ..> github_access_token: string \n\nLogger ..> error(message) \n\nGitRepo *- repo: git.Repo \nGitRepo *- output_directory: OutputDirectory \n\nOutputDirectory --> output_directory: string \n\nJsonFile --> config: json \n\nOutputDirectory ..> repo: GitRepo \n\nTemp *- temp_file: Tempfile \n\nTempfile --> mkdtemp() \n\nexception Exception \n\nGithubRepo: -repo_url : string\n\nGitRepo : -repo_url: string\nGitRepo : -output_directory: string\nGitRepo : -branch_name: stirng\nGitRepo --> git:Repo\n\nGithubAccess: -github_access_token: string\n\noutputDirectory: -output_directory: string\n\nTempfile: +mkdtemp()\n\nLogger: +error(message: string)\n\nJsonFile: -config: json\nJsonFile ..> GitRepo : obj key\n\nGitRepo ..> exception : Catch\n\n@enduml\nGenerated content: {final_output_paths})\n\n@startuml\nclass Repo\nclass Tree\nclass Config\nclass Include\nclass Fnmatch\nclass Path\nclass GitRepo\nclass Logger\nclass OutputDirectory\n\nRepo --> Tree : tree()\nTree --> Traversal : .traverse()\nTraversal --> Item : item in repo.tree().traverse()\nItem --> Type : item.type \nType --> If : ==\nIf --> Blob : \'blob\'\nBlob --> File : file\nIf --> Pattern : pattern in config[\'include\']\nPattern --> Fnmatch : fnmatch.fnmatch()\nFnmatch --> Item : item.path\nItem --> Open : open()\nOpen --> Path : os.path.join\nPath --> GitRepo : git_repo.local_dir\nGitRepo --> Read : read()\nRead --> IncludedFiles : included_files[item.path]\nIncludedFiles --> Break : break\nBreak --> Return : return included_files\nReturn --> IncludedFiles : included_files = traverse_directories(repo, git_repo, config)\nIncludedFiles --> Logger : logger.debug()\nLogger --> Type : type(included_files)\nLogger --> Value : value(included_files)\nLogger --> Save : logger.info()\nSave --> OutputDirectory : output_directory\nOutputDirectory --> Generate : generate_content()\nGenerate --> OutputPath : final_output_paths\nOutputPath --> Return : return final_output_paths\nLogger --> Message : logger.info()\nMessage --> FinalOutputPaths\n####UML Diagram####\n```\n@startuml\nobject UMLGenerator {\n    __init__(logger=None, config_path=None)\n    +finalize()\n    -search_git_repo_config(config_path)\n    -git_repo\n    #BASE_IMAGE_URL = "https://github.com/{git_username}/{repo_name}.git"\n\n    #temp_dir\n    +{ git_repo }\n    +{ logger }\n    +{ config_path }\n    +log_source_files()\n    +generate_uml()\n    +create_puml()\n    +search_for_github_repo()\n    +get_output_path()\n    +save_puml()\n    +return_results()\n}\n\nclass git_repo {\n    repo_url\n}\n\nclass logger {\n    log(message)\n    info(message)\n    error(message)\n}\n\nUMLGenerator --> git_repo\nUMLGenerator --> logger\n\ngit_repo <-- UMLGenerator\nlogger <-- UMLGenerator\nlogger --> UMLGenerator.finalize()\n\nUMLGenerator : __init__(config_path)\nUMLGenerator : search_git_repo_config(config_path)\nUMLGenerator : __init__(logger)\n\nUMLGenerator : log_source_files()\nUMLGenerator : generate_uml()\nUMLGenerator : create_puml()\nUMLGenerator : search_for_github_repo()\nUMLGenerator : get_output_path()\nUMLGenerator : save_puml()\nUMLGenerator : return_results()\n\n@enduml', 'max_tokens': 1024}}
2024-01-23 15:30:31,714 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2024-01-23 15:30:31,714 - DEBUG - send_request_headers.complete
2024-01-23 15:30:31,714 - DEBUG - send_request_body.started request=<Request [b'POST']>
2024-01-23 15:30:31,715 - DEBUG - send_request_body.complete
2024-01-23 15:30:31,715 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2024-01-23 15:30:31,900 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Tue, 23 Jan 2024 21:30:31 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'Cache-Control', b'no-cache, must-revalidate'), (b'openai-model', b'gpt-3.5-turbo-instruct'), (b'openai-organization', b'user-lqvxa4hv7a8o8eyohtzmcoc6'), (b'openai-processing-ms', b'92'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=15724800; includeSubDomains'), (b'x-ratelimit-limit-requests', b'3000'), (b'x-ratelimit-limit-tokens', b'250000'), (b'x-ratelimit-remaining-requests', b'2999'), (b'x-ratelimit-remaining-tokens', b'248975'), (b'x-ratelimit-reset-requests', b'20ms'), (b'x-ratelimit-reset-tokens', b'245ms'), (b'x-request-id', b'84208a392556f75e15ecfb5103f9e58b'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'84a3236c6de16740-ATL'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=":443"; ma=86400')])
2024-01-23 15:30:31,902 - INFO - HTTP Request: POST https://api.openai.com/v1/completions "HTTP/1.1 200 OK"
2024-01-23 15:30:31,903 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2024-01-23 15:30:31,904 - DEBUG - receive_response_body.complete
2024-01-23 15:30:31,904 - DEBUG - response_closed.started
2024-01-23 15:30:31,904 - DEBUG - response_closed.complete
2024-01-23 15:30:31,905 - DEBUG - HTTP Request: POST https://api.openai.com/v1/completions "200 OK"
2024-01-23 15:30:31,907 - INFO - UML code generated for src/routes/uml_from_repo.py: @startuml
title uml_from_repo.py

@enduml
2024-01-23 15:30:31,907 - INFO - Saving generated output to ../../output/uml_from_repo.py.puml
2024-01-23 15:30:31,907 - INFO - Generated output saved to ../../output/uml_from_repo.py.puml
2024-01-23 15:30:31,908 - INFO - Skipping empty file: src/scripts/__init__.py
2024-01-23 15:30:31,908 - INFO - Processing file: src/scripts/execute_generate_uml.py
2024-01-23 15:30:31,909 - INFO - Sending prompt to OpenAI: Create UML diagrams in .puml format for the following code:

import json
import os
import requests
import logging
from logging import handlers
from dotenv import load_dotenv

# Load environment variables from .env file
load_dotenv()

# Configure logging to write to a file
log_directory = '../../logs'
# Create the directory if it doesn't exist
os.makedirs(log_directory, exist_ok=True)  
log_filename = os.path.join(log_directory, 'execute_generate_uml.log')
log_handler = handlers.RotatingFileHandler(log_filename, maxBytes=1024*1024, backupCount=5)  # Log file with rotation
log_formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')
log_handler.setFormatter(log_formatter)
logger = logging.getLogger()
logger.addHandler(log_handler)
logger.setLevel(logging.DEBUG)


# Configure logging
logging.basicConfig(filename='execute_generate_uml.log', level=logging.DEBUG, format='%(asctime)s - %(levelname)s - %(message)s')

# Current file's directory
current_dir = os.path.dirname(os.path.abspath(__file__))
# Configuration file path
config_file_path = os.path.join(cu
2024-01-23 15:30:31,910 - DEBUG - Request options: {'method': 'post', 'url': '/completions', 'files': None, 'json_data': {'model': 'gpt-3.5-turbo-instruct', 'prompt': "Create UML diagrams in .puml format for the following code:\n\nimport json\nimport os\nimport requests\nimport logging\nfrom logging import handlers\nfrom dotenv import load_dotenv\n\n# Load environment variables from .env file\nload_dotenv()\n\n# Configure logging to write to a file\nlog_directory = '../../logs'\n# Create the directory if it doesn't exist\nos.makedirs(log_directory, exist_ok=True)  \nlog_filename = os.path.join(log_directory, 'execute_generate_uml.log')\nlog_handler = handlers.RotatingFileHandler(log_filename, maxBytes=1024*1024, backupCount=5)  # Log file with rotation\nlog_formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\nlog_handler.setFormatter(log_formatter)\nlogger = logging.getLogger()\nlogger.addHandler(log_handler)\nlogger.setLevel(logging.DEBUG)\n\n\n# Configure logging\nlogging.basicConfig(filename='execute_generate_uml.log', level=logging.DEBUG, format='%(asctime)s - %(levelname)s - %(message)s')\n\n# Current file's directory\ncurrent_dir = os.path.dirname(os.path.abspath(__file__))\n# Configuration file path\nconfig_file_path = os.path.join(cu", 'max_tokens': 1024}}
2024-01-23 15:30:31,912 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2024-01-23 15:30:31,912 - DEBUG - send_request_headers.complete
2024-01-23 15:30:31,912 - DEBUG - send_request_body.started request=<Request [b'POST']>
2024-01-23 15:30:31,912 - DEBUG - send_request_body.complete
2024-01-23 15:30:31,913 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2024-01-23 15:30:35,418 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Tue, 23 Jan 2024 21:30:35 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'Cache-Control', b'no-cache, must-revalidate'), (b'openai-model', b'gpt-3.5-turbo-instruct'), (b'openai-organization', b'user-lqvxa4hv7a8o8eyohtzmcoc6'), (b'openai-processing-ms', b'3081'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=15724800; includeSubDomains'), (b'x-ratelimit-limit-requests', b'3000'), (b'x-ratelimit-limit-tokens', b'250000'), (b'x-ratelimit-remaining-requests', b'2999'), (b'x-ratelimit-remaining-tokens', b'248405'), (b'x-ratelimit-reset-requests', b'20ms'), (b'x-ratelimit-reset-tokens', b'382ms'), (b'x-request-id', b'dcd10993af43648428762d5bee880198'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'84a3236dbfab6740-ATL'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=":443"; ma=86400')])
2024-01-23 15:30:35,420 - INFO - HTTP Request: POST https://api.openai.com/v1/completions "HTTP/1.1 200 OK"
2024-01-23 15:30:35,421 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2024-01-23 15:30:35,422 - DEBUG - receive_response_body.complete
2024-01-23 15:30:35,422 - DEBUG - response_closed.started
2024-01-23 15:30:35,422 - DEBUG - response_closed.complete
2024-01-23 15:30:35,423 - DEBUG - HTTP Request: POST https://api.openai.com/v1/completions "200 OK"
2024-01-23 15:30:35,425 - INFO - Received response from OpenAI: rrent_dir, 'config.json')

@startuml

class json
class os
class requests
class logging
class handlers
class dotenv
class load_dotenv
class Logs {
    - log_directory
    - log_filename
    - log_handler
    - log_formatter
    - logger
}
class Execute_Generate_Uml {
    - current_dir
    - config_file_path
}
class basicConfig
class setLevel
class log
class makedirs
class join
class create_diff
class RotatingFileHandler

json ..> os
os ..> requests
os ..> logging
logging ..> handlers
dotenv ..> load_dotenv
load_dotenv <.. Execute_Generate_Uml
Execute_Generate_Uml -- Logs
Execute_Generate_Uml -- json
Execute_Generate_Uml -- os
Execute_Generate_Uml -- requests
Execute_Generate_Uml -- logging
Execute_Generate_Uml -- basicConfig
Execute_Generate_Uml -- setLevel
Execute_Generate_Uml -- log
Execute_Generate_Uml -- makedirs
Execute_Generate_Uml -- join
Execute_Generate_Uml -- create_diff
handlers ..> RotatingFileHandler

@enduml
2024-01-23 15:30:35,426 - INFO - Sending prompt to OpenAI: Create UML diagrams in .puml format for the following code:

rrent_dir, 'config.json')

# Read configuration from the JSON file
with open(config_file_path, 'r') as config_file:
    config_data = json.load(config_file)

# Retrieve the GitHub access token from environment variables
github_token = os.getenv('GITHUB_PAT')

# Endpoint URL
url = 'http://127.0.0.1:5000/generate-uml'

# Headers
headers = {
    'Content-Type': 'application/json'
}

# Update the GitHub access token, local directory, and configFile in the config data
config_data['gitHubAccessToken'] = github_token
config_data['local_dir'] = os.path.join(current_dir, '../..', 'output')
config_data['configFile'] = config_file_path  

# Log the JSON data being sent in the request
logging.info(f'Sending JSON data to {url}:')
logging.info(json.dumps(config_data, indent=2))

try:
    # Make the POST request
    response = requests.post(url, headers=headers, json=config_data)

    # Log the response from the server
    logging.info(f'Response from server ({url}):')
    logging.info(f'Status Code: {response.status_code}'
2024-01-23 15:30:35,429 - DEBUG - Request options: {'method': 'post', 'url': '/completions', 'files': None, 'json_data': {'model': 'gpt-3.5-turbo-instruct', 'prompt': "Create UML diagrams in .puml format for the following code:\n\nrrent_dir, 'config.json')\n\n# Read configuration from the JSON file\nwith open(config_file_path, 'r') as config_file:\n    config_data = json.load(config_file)\n\n# Retrieve the GitHub access token from environment variables\ngithub_token = os.getenv('GITHUB_PAT')\n\n# Endpoint URL\nurl = 'http://127.0.0.1:5000/generate-uml'\n\n# Headers\nheaders = {\n    'Content-Type': 'application/json'\n}\n\n# Update the GitHub access token, local directory, and configFile in the config data\nconfig_data['gitHubAccessToken'] = github_token\nconfig_data['local_dir'] = os.path.join(current_dir, '../..', 'output')\nconfig_data['configFile'] = config_file_path  \n\n# Log the JSON data being sent in the request\nlogging.info(f'Sending JSON data to {url}:')\nlogging.info(json.dumps(config_data, indent=2))\n\ntry:\n    # Make the POST request\n    response = requests.post(url, headers=headers, json=config_data)\n\n    # Log the response from the server\n    logging.info(f'Response from server ({url}):')\n    logging.info(f'Status Code: {response.status_code}'", 'max_tokens': 1024}}
2024-01-23 15:30:35,430 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2024-01-23 15:30:35,430 - DEBUG - send_request_headers.complete
2024-01-23 15:30:35,431 - DEBUG - send_request_body.started request=<Request [b'POST']>
2024-01-23 15:30:35,431 - DEBUG - send_request_body.complete
2024-01-23 15:30:35,431 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2024-01-23 15:30:38,759 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Tue, 23 Jan 2024 21:30:38 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'Cache-Control', b'no-cache, must-revalidate'), (b'openai-model', b'gpt-3.5-turbo-instruct'), (b'openai-organization', b'user-lqvxa4hv7a8o8eyohtzmcoc6'), (b'openai-processing-ms', b'3238'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=15724800; includeSubDomains'), (b'x-ratelimit-limit-requests', b'3000'), (b'x-ratelimit-limit-tokens', b'250000'), (b'x-ratelimit-remaining-requests', b'2999'), (b'x-ratelimit-remaining-tokens', b'248975'), (b'x-ratelimit-reset-requests', b'20ms'), (b'x-ratelimit-reset-tokens', b'245ms'), (b'x-request-id', b'43807afc45ef48ce39a036e303b6bb45'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'84a32383adf66740-ATL'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=":443"; ma=86400')])
2024-01-23 15:30:38,761 - INFO - HTTP Request: POST https://api.openai.com/v1/completions "HTTP/1.1 200 OK"
2024-01-23 15:30:38,761 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2024-01-23 15:30:38,762 - DEBUG - receive_response_body.complete
2024-01-23 15:30:38,762 - DEBUG - response_closed.started
2024-01-23 15:30:38,763 - DEBUG - response_closed.complete
2024-01-23 15:30:38,763 - DEBUG - HTTP Request: POST https://api.openai.com/v1/completions "200 OK"
2024-01-23 15:30:38,765 - INFO - Received response from OpenAI: )


Component Diagram:

@startuml
rectangle "config.json" as config
database "config_file" as configFile
rectangle "JSON data" as jsonData
cloud "http://127.0.0.1:5000/generate-uml" as server
rectangle "response" as response

component "main.py" as main {
    main -> config
    main -> configFile
    main -> jsonData
    main -> server
    server -> response
}
@enduml


Sequence Diagram:

@startuml
actor User
rectangle "main.py" as main
database "config.json" as config
database "config_file" as configFile
entity "JSON data" as jsonData
database "environment variables" as envVars

User -> main: Run application
activate main

main -> config: Read configuration
activate config
config -> configFile: Open file
activate configFile
configFile --> config: Read data
deactivate configFile
config -> jsonData: Process data
activate jsonData
jsonData --> config: Updated data
deactivate jsonData
config --> main: Return data
deactivate config

main -> envVars: Retrieve access token
activate envVars
envVars --> main: Access token
deactivate envVars

main -> jsonData: Update data with access token
activate jsonData
jsonData --> main: Updated data
deactivate jsonData

main -> server: Make POST request
activate server
server --> main: Response from server (Status Code)
deactivate server
main -> User: Display response
deactivate main

@enduml
2024-01-23 15:30:38,766 - INFO - Sending prompt to OpenAI: Create UML diagrams in .puml format for the following code:

)
    logging.info(f'Response Text: {response.text}')

    # Print the response from the server
    print(response.text)

    # Save the response to a file in the output directory
    output_dir = os.path.join(current_dir, '../..', 'output')
    os.makedirs(output_dir, exist_ok=True)
    with open(os.path.join(output_dir, 'response.json'), 'w') as output_file:
        json.dump(response.json(), output_file, indent=2)

except Exception as e:
    # Log any exceptions that occur
    logging.error(f'Error occurred: {str(e)}')
2024-01-23 15:30:38,768 - DEBUG - Request options: {'method': 'post', 'url': '/completions', 'files': None, 'json_data': {'model': 'gpt-3.5-turbo-instruct', 'prompt': "Create UML diagrams in .puml format for the following code:\n\n)\n    logging.info(f'Response Text: {response.text}')\n\n    # Print the response from the server\n    print(response.text)\n\n    # Save the response to a file in the output directory\n    output_dir = os.path.join(current_dir, '../..', 'output')\n    os.makedirs(output_dir, exist_ok=True)\n    with open(os.path.join(output_dir, 'response.json'), 'w') as output_file:\n        json.dump(response.json(), output_file, indent=2)\n\nexcept Exception as e:\n    # Log any exceptions that occur\n    logging.error(f'Error occurred: {str(e)}')", 'max_tokens': 1024}}
2024-01-23 15:30:38,770 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2024-01-23 15:30:38,770 - DEBUG - send_request_headers.complete
2024-01-23 15:30:38,771 - DEBUG - send_request_body.started request=<Request [b'POST']>
2024-01-23 15:30:38,771 - DEBUG - send_request_body.complete
2024-01-23 15:30:38,771 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2024-01-23 15:30:39,643 - INFO -  * Detected change in '/Users/preston/Documents/gitRepos/diagrammr/src/services/openai_api.py', reloading
2024-01-23 15:30:39,703 - INFO -  * Restarting with stat
2024-01-23 15:30:39,985 - WARNING -  * Debugger is active!
2024-01-23 15:30:39,990 - INFO -  * Debugger PIN: 139-904-016
